[
    {
        "type": "text",
        "text": "基于极性转移和LSTM的树结构网络与句子分类",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "汪冉，金忠 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "(南京理工大学 计算机科学与工程学院，南京 210018)",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "摘要：长短期记忆网络(long short term memory，LSTM)是一种能长久储存序列信息的循环神经网络，在语言模型、语音识别、机器翻译等领域都得到了广泛的应用。先研究了前人如何将LSTM中的记忆模块拓展到语法树得到LSTM树结构网络模型，以获取和储存句子深层次的语义结构信息；然后针对句子词语间的极性转移在LSTM树结构网络模型中添加了极性转移信息提出了极性转移LSTM 树结构网络模型，更好获取情感信息来进行句子分类。实验表明在Stanford sentimenttree-bank 数据集上，提出的极性转移LSTM树结构网络模型的句子分类效果优于LSTM、递归神经网络等模型。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "关键词：神经网络；LSTM网络；树结构网络；极性转移；句子分类中图分类号：TP393 doi: 10.3969/j.issn.1001-3695.2017.07.0669",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Tree-structured networks based on polarity shifting and LSTM for sentences classification ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Wang Ran, Jin Zhong (School ofComputer Science&Engineering,Nanjing UniversityofScience& Technology,Nanjing 210ol8,China) ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract: Long short term memory(LSTM) is arecurrnt neural network (RNN) which has outstanding ability to preserve sequence information foralongtime.Ithasbeenwidelyused in thelanguagemodeling,machine translation,speechrecognition andother fields.Firstly,this paper explored how predecesors extend the memory module inLSTMto the syntax tree toget Tre-Structured LSTMnetworks model which canobtain and store the semantic structure informationofthe sentences;Then, according tothepolarityshifting information betweenthe wordsofsentences,itproposedaPolarityShiftingTree-Structured LSTM networks model tocapture the sentiment information for better sentences clasification.The proposed model show a better performance than LSTM,recursive neural networks andother models forsentencesclasificationon Stanford Sentiment Tree-bank dataset. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Key Words: neural networks; LSTM; tree-structure network; polarity shifting; sentence classification ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "0 引言",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "近年来深度学习被有效地广泛应用于语音识别[1]、机器翻译[2]以及图像转换成文本等领域[3]。在自然语言处理领域，随着学者对神经网络的不断探索，各种神经网络模型在语言建模和文本情感分类等方面都取得了很大的进展。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "句子分类在自然语言处理领域具有很大的研究价值和应用价值[4]，该研究内容引起越来越多的国内外学者的兴趣和重视。使用神经网络句子分类时，首先面对的是句子建模，即用模型训练后的向量来表示句子。目前有三种模型应用比较广泛：词袋模型、序列模型和树结构模型。三种模式各有特点，词袋模型比较简单，它将句子看成无序的单词的集合，有时将所有单词向量的平均向量作为句子的向量表示[5.6]，它完全忽略单词的顺序和语法，很难获取句子有意义的信息。该模型经常被很有效地用在传统的贝叶斯模型、LSA和LDA等模型中。2008 年Pang 和Lee等人使用词袋模型对文本进行情感分析和分类，随后更多人试着设计能获取更好的句子特征的模型或者使用基于句法结构的极性转移特征来提高情感分类的准确率。序列模型则很看重句子中所有单词的排列顺序[7.8]，该模型句子建模时直接将单词向量按排列顺序连接得到句子表示，然而并没有考虑到句子的结构信息。而树结构模型是根据给定的句法树构造递归网络最终获得句子中短语和句子的表示[9,10]。树结构已经用于分析自然的图像场景，还用于从词向量到短语向量的转换，并对句子的情感极性进行分类[II]。2013年Socher等人在影评数据上使用递归神经网络模型进行情感分类展现了比传统模型更好的效果。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "句子分类中，词袋模型并不能完全掌握自然语言句子的全部语义，无法分辨由词序或句法结构造成的语义差别，于是使用序列模型和树结构模型会更有效，而且由于树结构模型与句法结构的相关性更强，树结构模型是最优选择。循环神经网络（RNN）是解决序列化的一种有效的方法，理论上它可以处理任意长度的序列，但在训练网络时会出现梯度消失的问题[12]。LSTM网络是RNN的一种改进，该网络在处理梯度消失问题上有很大突破[13]，使网络可以长久保存序列信息，但LSTM网络还是无法获得句子的句法结构信息。在基于树的递归神经网络（recursive neuralnetwork,RcNN）结构中运用LSTM记忆模块的Tree-StructuredLSTM模型则不仅能够获取句法结构和语义信息还能在递归中长久保存节点信息。原始的LSTM记忆模块输入是当前时刻的外部输入以及上一时刻的隐藏状态量，Tree-StructuredLSTM模型的输入则是当前节点的外部输入和子节点的隐藏状态量。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "本文先介绍了已有的基本网络模型，重点介绍将LSTM记忆模块分别运用到基于依存句法树和短语结构树的树结构网络中得到的tree-structuredLSTM模型；然后将情感极性转移融合到 tree-structured LSTM 模型中提出了两种新的 polarity tree-structuredLSTM(PTree-structuredLSTM)模型，丰富了LSTM记忆模块的信息，不仅获得句子的语义信息和句法结构信息还保存了情感信息，更有效地解决句子分类问题。接着给出了在本文模型下实验的结果以及和其他模型的对比分析。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 相关工作",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1.1长短期记忆网络 (LSTM) ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "循环神经网络（RNN）通过对网络块中隐藏状态向量$h _ { t } \\in \\mathfrak { R } ^ { d }$ 的循环使用，可以处理任意长度的序列。其中， $\\mathbf { \\Phi } _ { t }$ 时刻的隐藏状态向量 $h _ { { t } }$ 是由此刻的输入向量 $x _ { t }$ 和 $t - 1$ 时刻的隐藏状态向量 $h _ { t - 1 }$ 的非线性函数转换得到，如式(1)所示的双曲正切函数：",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nh _ { t } = \\operatorname { t a n h } ( W x _ { t } + U h _ { t - 1 } + b )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "但是，RNN在训练网络参数时会产生梯度向量下降或者增长速度过快的问题[15,16]，特别处理一个较长序列时，RNN 模型会因为存在此问题而很难有很好的效果。",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/a52cca7f4f6fe5b6e2acda100982ad4499cdf6d7c14a4bc1537c267d02e9324a.jpg",
        "img_caption": [
            "图1LSTM网络结构"
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "如图1所示，长短期记忆（LSTM)网络通过利用一个记忆模块来改进RNN，它在此模块中保存若干状态向量来保存序列中的每个时刻信息。这些状态向量包含：输入门i，遗忘门f,，输出门 $o _ { t }$ ，记忆状态 $c _ { t }$ ，以及隐藏状态 $h _ { { } _ { t } }$ 。LSTM记忆模块中的向量维数 $d$ 称为LSTM的内部向量维度。LSTM单元中的状态向量转换公式如下：",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\ni _ { t } = \\sigma ( \\boldsymbol { W } ^ { ( i ) } \\boldsymbol { x } _ { t } + \\boldsymbol { U } ^ { ( i ) } h _ { t - 1 } + \\boldsymbol { b } ^ { ( i ) } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nf _ { t } = \\sigma ( \\boldsymbol { W } ^ { ( f ) } \\boldsymbol { x } _ { t } + \\boldsymbol { U } ^ { ( f ) } h _ { t - 1 } + \\boldsymbol { b } ^ { ( f ) } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\no _ { t } = \\sigma ( { W ^ { ( o ) } } x _ { t } + { { U } ^ { ( o ) } } h _ { t - 1 } + { b } ^ { ( o ) } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nu _ { t } = \\mathrm { t a n h } ( W ^ { ( u ) } x _ { t } + U ^ { ( u ) } h _ { t - 1 } + b ^ { ( u ) } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nc _ { t } = i _ { t } \\odot u _ { t } + f _ { t } \\odot c _ { t - 1 }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nh _ { \\scriptscriptstyle t } = o _ { \\scriptscriptstyle t } \\odot \\mathrm { t a n h } ( c _ { \\scriptscriptstyle t } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "在式(2)\\~(7)中， $x _ { t }$ 是时刻 $\\mathbf { \\Phi } _ { t }$ 的输入，Sigmoid 函数将状态向量的数值控制在0与1之间。该模型通过使用各种门限改变每个向量的状态可以处理各种时间尺度的问题。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1.2基于LSTM的树结构网络",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "句子中单词之间不仅存在顺序关系，基本的LSTM网络局限于序列的有序性不能展示句子的结构信息。而树结构网络可利用句子构造的语法树来建造递归网络，此递归网络可以体现句子结构信息却易造成信息丢失。2015年Socher等人结合两者的特点将LSTM记忆模块拓展运用到树结构网络中，提出两种融合LSTM记忆模块的树结构网络[I7]：融合LSTM的依存句法树结构网络Dependency Tree LSTM（DTree-LSTM）、融合LSTM 的短语结构树网络 Constituency TreeLSTM（CTree-LSTM）。这种模型近期用于句子表示[18]，句子分析[19]都有很好的效果。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Tree-structuredLSTM中节点的LSTM记忆模块中的状态向量均与子节点相关，节点 $j$ 中状态向量包含了输入门 $i _ { j }$ ，输出门 $o _ { j }$ ，记忆状态 $c _ { j }$ 和隐藏状态 $h _ { j }$ ， $x _ { j }$ 表示句子中单词向量，且对每一个子节点均设置一个遗忘门 $f _ { j k }$ （ $K$ 为孩子节点索引)，侧重保存重要的语义信息。",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/97fa313a516dbc38ada01e984e240908f58897210c88791967096d7ee03b1559.jpg",
        "img_caption": [
            "图2依存句法结构树和短语结构树"
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1.2.1融合LSTM的依存句法结构树网络",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "法国的语言学家L.Tesniere最早提出依存句法结构,如图2(a)所示的一棵依存句法树,它描述节点与其子节点之间的依存关系,展示了在句法上词语之间的和语义有关的搭配关系。该树结构有如下特点：子节点是无序的；子节点的数目不定。根据这两个特点，将融合LSTM的依存句法树结构(DTree-LSTM)中的状态向量转换设置如下：",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\widetilde { h _ { j } } = \\sum _ { k \\in C \\left( j \\right) } h _ { k }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\ni _ { j } = \\sigma ( \\boldsymbol { W } ^ { ( i ) } \\boldsymbol { x } _ { j } + \\boldsymbol { U } ^ { ( i ) } \\widetilde { \\boldsymbol { h } _ { j } } + \\boldsymbol { b } ^ { ( i ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nf _ { j k } = \\sigma ( \\boldsymbol { W } ^ { ( f ) } \\boldsymbol { x } _ { j } + \\boldsymbol { U } ^ { ( f ) } h _ { k } + \\boldsymbol { b } ^ { ( f ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\no _ { j } = \\sigma ( { W } ^ { ( o ) } { x } _ { j } + { U } ^ { ( o ) } \\widetilde { h _ { j } } + b ^ { ( o ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nu _ { j } = \\mathrm { t a n h } ( W ^ { ( u ) } x _ { j } + U ^ { ( u ) } \\widetilde { h _ { j } } + b ^ { ( u ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nc _ { j } = i _ { j } \\odot u _ { j } + \\sum _ { k \\in C ( j ) } f _ { j k } \\odot c _ { k }\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nh _ { j } = o _ { j } \\odot \\operatorname { t a n h } ( c _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "式(8)\\~(14)中的 $C ( j )$ 为节点 $j$ 的子节点数目，参数矩阵 $W$ 和 $U$ 将节点的 $x _ { j }$ 和 $h _ { k }$ 转成记忆模块内部向量。DTree-LSTM网络结构如图3(a)所示。",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/e5ab45345a76eca23bef35aef21ba81cb3c9cdd41036388d0ae27501e79d1a16.jpg",
        "img_caption": [
            "图3DTree-LSTM网络结构和CTree-LSTM结构"
        ],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1.2.2融合LSTM的短语结构树网络",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "句子的短语结构句法分析源于传统的句子图解法，即如图2(b)所示把句子分割为短语（动词词组，名词词组等）。这种树的非叶子节点表示子节点之间的关系，叶子节点则代表句子中的单词。短语结构树有如下特点：它是一个二叉树，且节点的子节点是有序的；它只有在叶子节点上有单词输入。由其特点，将LSTM单元融合到短语结构树（CTree-LSTM）中的状态向量转换设置如下：",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\ni _ { j } = \\sigma ( \\boldsymbol { W } ^ { ( i ) } \\boldsymbol { x } _ { j } + \\sum _ { l = 1 } ^ { N } U _ { l } ^ { ( i ) } h _ { j l } + b ^ { ( i ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nf _ { j k } = \\sigma ( \\boldsymbol { W } ^ { ( f ) } \\boldsymbol { x } _ { j } + \\sum _ { l = 1 } ^ { N } { \\boldsymbol { U } } _ { k l } ^ { ( f ) } \\boldsymbol { h } _ { j l } + \\boldsymbol { b } ^ { ( f ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\no _ { j } = \\sigma ( { W ^ { ( 0 ) } x _ { j } } + \\sum _ { l = 1 } ^ { N } U _ { l } ^ { ( o ) } h _ { j l } + b ^ { ( o ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nu _ { j } = \\mathrm { t a n h } ( W ^ { ( u ) } x _ { j } + \\sum _ { l = 1 } ^ { N } U _ { l } ^ { ( u ) } h _ { j l } + b ^ { ( u ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nc _ { j } = i _ { j } \\mathcal { O } u _ { j } + \\sum _ { l = 1 } ^ { N } f _ { j l } \\mathcal { O } c _ { j l }\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nh _ { j } = o _ { j } \\odot \\operatorname { t a n h } ( c _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "式(15)-(20)中的 $N$ 为节点的子节点数目， $k = 1 , 2 , . . . , N$ 。在短语结构树中 $\\scriptstyle \\mathrm { N = } 2$ 或者0。该树网络对子节点的隐藏状态 $h _ { k }$ 赋予了不同的参数矩阵。这种做法对短语组合有不同的侧重度，对于情感强烈的节点参数矩阵需调整到更大值。CTree-LSTM网络如图3(b)所示。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2 融合极性转移和LSTM的树结构网络",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.1情感极性转移 ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "语言理解中，单词的极性转移能够影响整个句子的情感极性，句子中的副词，否定词等都会使整个句子的极性发生偏转，情感极性转移如表1所示一般分为四类：三种句内显式(强调、否定和转折），一种句间极性转折[20]。在 tree-structuredLSTM中，忽略了每个节点的情感信息，所以提出了PTree-LSTM网络，在每个节点利用SoftMax得出节点的情感状态向量，将这些情感状态向量作为情感极性转移特征更好地训练网络。则在计算节点的表示时，不仅拥有子节点由原始记忆模块保存的语义结构相关信息，还获得了子节点的情感信息，具体的做法是为每一个节点的LSTM记忆模块的状态向量中设置添加情感状态向量i和情感极性转移向量 $b$ 。",
        "page_idx": 2
    },
    {
        "type": "table",
        "img_path": "images/762f0d808999361bb746e72f9d9f0eb7142fe9785d30a71175867f1047867150.jpg",
        "table_caption": [
            "表1句子中单词极性转移示例"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>类型</td><td>样例 线索</td></tr><tr><td>强调 他很喜欢这本书</td><td>作用域 显式 句内</td></tr><tr><td>否定 我不喜欢这个地方</td><td>显式 句内</td></tr><tr><td>转折 这个手机还不错，但我不喜欢</td><td></td></tr><tr><td>句间极性转折 这个手机配置不错，但是样式不好看</td><td>显式 句内 隐式 句间</td></tr></table></body></html>",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.2融合极性转移和LSTM的依存句法树网络",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "在依存句法树中，子节点是无序的，所以对于子节点的情感状态向量 $l _ { k }$ 无须侧重考虑，则状态向量转换设置如式(21)\\~(30)所示。",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\stackrel { \\sim } { l } = \\sum _ { k \\in C ( j ) } l _ { k }\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nb _ { j } = \\sigma ( { \\boldsymbol { W } ^ { ( b ) } } \\widetilde { \\boldsymbol { l } } + \\boldsymbol { b } ^ { ( b ) } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\stackrel { \\sim } { h } _ { j } = \\sum _ { k \\in C \\left( j \\right) } h _ { k }\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\ni _ { j } = \\sigma ( \\boldsymbol { W } ^ { ( i ) } x _ { j } + \\boldsymbol { U } ^ { ( i ) } \\widetilde { \\boldsymbol { h } _ { j } } + \\boldsymbol { b } _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nf _ { j k } = \\sigma ( { W ^ { ( f ) } x _ { j } + U ^ { ( f ) } h _ { k } + b _ { j } } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\no _ { j } = \\sigma ( { W } ^ { ( o ) } { x } _ { j } + { U } ^ { ( o ) } \\widetilde { h } _ { j } + b _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nu _ { j } = \\operatorname { t a n h } ( W ^ { ( u ) } x _ { j } + U ^ { ( u ) } \\widetilde { h _ { j } } + b _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nc _ { j } = i _ { j } \\odot u _ { j } + \\sum _ { k \\in C ( j ) } f _ { j k } \\odot c _ { k }\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nh _ { j } = o _ { j } \\odot \\operatorname { t a n h } ( c _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nl _ { j } = S o f t M a x ( \\boldsymbol { W } ^ { ( s ) } \\boldsymbol { h } _ { j } + \\boldsymbol { b } ^ { ( s ) } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "$x _ { j }$ 表示句子中单词向量，节点中门限向量 $i _ { t } \\ , \\ f _ { t }$ 和 $o _ { t }$ 中的向量值均为0到1之间。遗忘门 $f _ { t }$ 控制着子节点的记忆模块中哪些内容将被丢弃，输入门 $i _ { t }$ 控制着更新内容，输出门 $o _ { t }$ 控制着输出内容。若在一棵依存句法树中，某节点语义信息能够强烈表达情感，训练网络过程中则会不断调整 $\\boldsymbol { W } ^ { ( i ) }$ 使 $i _ { j }$ 中的值更接近1来保存这个节点的信息，反之则使其更接近0。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "节点的情感极性转移向量 $b _ { j }$ 由综合考虑子节点的情感状态向量 $l _ { k }$ 所得， $b _ { j }$ 表示语法树中单词之间的极性转移信息。网络训练时通过不断调整参数 $W ^ { ( b ) }$ ，由下往上的递归传递极性转移信息来更准确的获得每个节点的情感标签和整个句子的情感信息。PDTree-LSTM网络图如图4(a)所示。",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/7ca7e79c82ee6f79fe80481a1e48016bf8d183a0e200e251c4e2d9a261d6a52d.jpg",
        "img_caption": [
            "图4PDTree-LSTM网络图和PCTree-LSTM结构"
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2.3融合极性转移和LSTM的短语结构树网络",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "短语结构树中子节点是短语组合，两个节点的侧重性应该",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "不同，所以极性转移在此模型中更有突出体现，则需将子节点的情感状态向量分开考虑，状态向量转换设置如式(31)\\~(38)所示。",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nb _ { j } = \\sigma ( \\sum _ { l = 1 } ^ { N } W _ { l } ^ { ( b ) } l _ { j l } + b ^ { ( b ) } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\ni _ { j } = \\sigma ( { W ^ { ( i ) } x _ { j } } + \\sum _ { l = 1 } ^ { N } U _ { l } ^ { ( i ) } h _ { j l } + b _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nf _ { j k } = \\sigma ( W ^ { ( f ) } x _ { j } + \\sum _ { l = 1 } ^ { N } U _ { k l } ^ { ( f ) } h _ { j l } + b _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\no _ { j } = \\sigma ( W ^ { ( 0 ) } x _ { j } + \\sum _ { l = 1 } ^ { N } U _ { l } ^ { ( o ) } h _ { j l } + b _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nu _ { j } = \\mathrm { t a n h } ( W ^ { ( u ) } x _ { j } + \\sum _ { l = 1 } ^ { N } U _ { l } ^ { ( u ) } h _ { j l } + b _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nc _ { j } = i _ { j } \\odot u _ { j } + \\sum _ { l = 1 } ^ { N } f _ { j l } { \\odot } c _ { j l }\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nh _ { j } = o _ { j } \\odot \\operatorname { t a n h } ( c _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nl _ { j } = S o f t M a x ( { W } ^ { ( s ) } h _ { j } + { b } ^ { ( s ) } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "短语结构树中各个节点表示短语组合，而动词词组的情感表达要远远强烈于名词词组，所以并不能同于依存句法树中将子节点的情感状态向量同等考虑，于是在记忆模块中对 $l _ { j l }$ 和$h _ { j l }$ 均给予不同的参数矩阵，且网络训练中会不断调整 $W _ { l } ^ { ( b ) }$ 使网络更能侧重保存动词词组的结构信息以及情感信息。由于短语结构树网络对子节点更多的限制，网络中的参数要远多于依存句法树网络中的参数，相同地，该模型在训练网络中通过不断调整 $W _ { l } ^ { ( b ) }$ 使节点标签更贴近真实标签。PCTree-LSTM网络图如图4(b)所示。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2.4树结构用于句子分类的误差计算",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "本节主要介绍如何将提出的模型应用到句子分类中。树结构网络中大部分节点都存在真实标签，每个节点表示以这个节点为根节点的子树的短语。对于任意节点 $j$ ，设该节点表示的短语为 $\\{ \\boldsymbol { x } \\} _ { j }$ ，可利用式(39)(40)得到预测标签 $\\hat { \\boldsymbol y } _ { j }$ 。",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\hat { p } _ { \\theta } ( y | \\{ x \\} _ { j } ) = S o f t M a x ( W ^ { ( s ) } h _ { j } + b ^ { ( s ) } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\hat { y } _ { j } = \\arg \\operatorname* { m a x } _ { y } \\hat { p } _ { \\theta } ( y | \\{ x \\} _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "模型中每个节点的预测标签和真实标签的误差和为整个网络的误差，选用负对数似然函数作为误差计算函数。设树结构有 $\\mathrm { ~ m ~ }$ 个节点，每个节点的误差计算公式如式(41)（仅计算有真实标签的节点）所示，其中 $\\lambda$ 和 $\\theta$ 是正则化的超参数。",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nJ ( \\theta ) = - \\frac { 1 } { m } \\sum _ { k = 1 } ^ { m } \\log \\hat { \\rho _ { \\theta } } ( y ^ { ( k ) } \\left| \\left\\{ x \\right\\} ^ { ( k ) } \\right. ) + \\frac { \\lambda } { 2 } \\| \\theta \\| _ { 2 } ^ { 2 }\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3 实验 ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3.1句子分类实验 ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "为了测验模型的有效性，使用斯坦福大学基于影评数据创建的Stanford sentiment tree-bank[21]来进行实验，该数据集有两种类型：用于二分类的句子和用于五分类的句子，五分类为：",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "verynegative,negative,neutral,positive,verypositive.数据集中共有11855个句子，平均句子长度为19，数据集中共有215154个短语，数据集提供了短语和句子标签。如果用其做二分类实验，数据集中6920 句是训练集，872句是验证集，1821句是测试集（原标为中性的句子全部去除)；如果用其做五分类实验，数据集中8544句式训练集，1101句是验证集，2210句是测试集。数据集中已经包含每个句子的短语结构树，本文使用Stanford语义分析包获取句子依存句法结构树。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "实验中，将PDTree-LSTM模型和PCTree-LSTM模型都分别进行了二分类和五分类的实验。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.2超参数调整 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "实验中使用300维的Glovevectors作为单词向量，训练网络过程中词向量的学习率是0.1，模型使用AdaGrad来训练网络，其中学习率是0.05，批量数值为25。在整个模型训练中为避免过拟合使用丢弃率为0.5的Dropout。实验发现记忆模块中内部向量维度 $d$ 的选择会影响实验性能， $d$ 较小则使向量保存信息较少， $d$ 较大则会导致参数过多，训练结果易产生过拟合现象。如图5所示，由实验得出，二分类中 $d$ 设为100 最佳，五分类 $d$ 设为150 最佳。",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/393445a69e3984215562b9c13eb3afc4b952478aedd90d985c6116fb7bc1e098.jpg",
        "img_caption": [
            "图5二分类和五分类内部向量维度选择"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.3实验结果",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "实验中LSTM,CTree-LSTM,PCTree-LSTM,DTree-LSTM,PDTree-LSTM五个模型在验证集上的测试结果如图6所示，本文提出的模型PCTree-LSTM和PDTree-LSTM在训练过程中与基本的网络模型相比都有很好的表现。",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/f4162ca89fc7706ec728394f9fdc6c68e8161b608ac5725016a4cf0236470c03.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "",
        "img_caption": [
            "图6句子二分类和五分类结果"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "最后本文模型与各种网络模型在测试集的测试结果对比如表2所示，由结果可以看出提出的两个模型在二分类和五分类实验中都能达到很好的效果，其中PCTree-LSTM模型在这个数据集上的句子分类效果最好，优于PDTree-LSTM模型。这两种模型的实验结果差距可能是来自两种树结构的节点数目的差别。而且短语结构树是以短语关系为节点的树，拥有31.9万的存在短语标签的节点，在训练中短语结构树可以获取更多的短语信息，而依存句法树是以结构关系为节点的树，某些节点只能表示为单词之间的关系，其子树单词分布并不能构成一个短语，只拥有15万存在短语标签的节点。而且在短语结构树中，参数要远远多于依存句法树，参数越多可以理解为保存的信息越多，这也有影响了分类效果。",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/2e3af0e2879364be67db2912ad82bb5b915cb9e4ce00a7977596f071e7ac80d4.jpg",
        "table_caption": [
            "表2模型分类正确率对比结果"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>模型</td><td>二分类正确率 (%)</td><td>五分类正确率（%)</td></tr><tr><td>DCNN</td><td>86.8</td><td>48.5</td></tr><tr><td>CNN-nonstatic</td><td>87.2</td><td>48.0</td></tr><tr><td>CNN-multichannel</td><td>88.1</td><td>47.4</td></tr><tr><td>LSTM</td><td>84.9</td><td>47.8</td></tr><tr><td>DTree-LSTM</td><td>85.7</td><td>48.4</td></tr><tr><td>PDTree-LSTM</td><td>86.2</td><td>48.7</td></tr><tr><td>CTree-LSTM</td><td>88.0</td><td>50.9</td></tr><tr><td>PCTree-LSTM</td><td>88.6</td><td>51.3</td></tr></table></body></html>",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "4 结束语",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "本文先介绍了几种已有的十分有效的网络，特别是将LSTM记忆模块拓展到树结构网络中，使网络能够有效获取句子的语义结构信息，丰富了网络对句子特征的学习；然后在此树结构网络的LSTM记忆模块中添加了情感极性转移信息提出了新的网络模型，进一步加强了模型对句子情感信息的学习。神经网络模型在句子处理上有很大的优势，下一步的工作展望是如何利用神经网络更加快速学习句子文本的结构特征，从而更准确地获取句子情感信息。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "参考文献：",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "[1]Zhang Y,Chan W, Jaitly N.Very deep convolutional networks for end-toend speech recognition [C]//Proc of IEEE International Conference on Acoustics, Speech and Signal Processing.2017:4845-4849.   \n[2]Gulcehre C,Firat O,Xu K,etal. On integrating a language model into neural machine translation [J]. Computer Speech and Language,2017,45(1):137- 148.   \n[3]Ma L,Lu Z,Li H. Learning to answer questions from image using convolutional neural network [C]// Proc of the 3Oth AAAI Conference on Artificial Intelligence.[S.1.]:AAAI Press,2016: 3567-3573.   \n[4]赵妍妍，秦兵，刘挺．文本情感分析[J].软件学报,2010,21(8):1834- 1848.   \n[5]Landauer TK,Dumais S T.A solution to plato's problem: the latent semantic analysis theory of acquisition,induction,and representation of knowledge [J].Psychological Review,1997,104 (2):211-240.   \n[6]Foltz PW,Kintsch W,Landauer TK.The measurement of textual coherence with latent semantic analysis [J].Discourse Processes,1998,25 (2-3): 285- 307.   \n[7]Elman JL.Finding structure in time [J]. Cognitive Science,199o,14 (2): 179-211.   \n[8]Mikolov T. Statistical Language models based on neural networks [J]. Presentation at Google,Mountain View, 2012.   \n[9]Goller C,KuchlerA.Learning task-dependent distributed representations by backpropagation through structure [C]// Proc of IEEE International Conference on Neural Networks,1996: 347-352.   \n[10] Socher R,Lin C C,Manning C,et al. Parsing natural scenes and natural language with recursive neural networks [C]//Proc of the 28th International Conference on Machine Learning.2011: 129-136.   \n[11] Socher R,Huval B,Manning C D,et al. Semantic compositionality through recursive matrix-vector spaces [Cl//Proc of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. 2012: 1201-1211.   \n[12] Socher R,Pennington J,Huang E H,et al. Semi-supervised recursive autoencoders for predicting sentiment distributions [C]// Proc of Conference on Empirical Methods in Natural Language Processing.2011: 151-161.   \n[13] Hochreiter S,Schmidhuber J.Long short-term memory [J].Neural Computation,1997,9(8): 1735-1780.   \n[14] Jordan MI. Serial order: A paralel distributed processing approach [J]. Advances in Psychology,1997,121 (2): 471-495.   \n[15] Hochreiter S.The vanishing gradient problem during learning recurrent neural nets and problem solutions [J]. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,1998,6(2): 107-116.   \n[16] Bengio Y, Simard P,Frasconi P.Learning long-term dependencies with gradient descentis difficult [J].IEEE Trans on Neural Networks,1994,5(2): 157-166.   \n[17] Tai K S, Socher R, Manning C D.Improved semantic representations from tree-structured long short-term memory networks [C]/ Proc of Association for Computational Linguistics.2015.   \n[18] Rath T. Word and Relation Embedding for Sentence Representation [D]. Phoenix: Arizona State University,2017.   \n[19] Goldberg Y.Neural Network Methods for Natural Language Processing [J]. Synthesis Lectures on Human Language Technologies,2017,10 (1): 1-309.   \n[20]张小倩．情感极性转移现象研究及应用[D].苏州：苏州大学,2012.   \n[21] Socher R,Perelygin A,Wu JY,et al.Recursive deep models for semantic compositionality over a sentiment treebank [Cl// Proc of Conference on Empirical Methods in Natural Language Processing. 2013: 1631-1642. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    }
]