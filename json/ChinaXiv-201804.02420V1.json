[
    {
        "type": "text",
        "text": "基于结构感知深度神经网络的显著性对象检测算法",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "李鑫la，陈雷霆 la,1b,2，蔡洪斌 la, Ib",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "(1．电子科技大学 a.计算机科学与工程学院;b.数字媒体技术四川省重点实验室，成都 611731;2.电子科技大学广东电子信息工程研究院，广东东莞 523000)",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "摘要：由于现有的基于深度神经网络的显著性对象检测算法忽视了对象的结构信息，使得显著性图不能完整地覆盖整个对象区域，导致检测的准确率下降。针对此问题，提出一种结构感知的深度显著性对象检测算法。算法基于一种多流结构的深度神经网络，包括特征提取网络、对象骨架检测子网络、显著性对象检测子网络和跨任务连接部件四个部分。首先，在显著性对象子网络的训练和测试阶段，通过对象骨骼检测子网络学习对象的结构信息，并利用跨任务连接部件使得显著性对象检测子网络能自动编码对象骨骼子网络学习的信息，从而感知对象的整体结构，克服对象区域检测不完整问题；其次，为了进一步提高所提方法的准确率，利用全连接条件随机场对检测结果进行进一步的优化。在三个公共数据集上的实验结果表明，该算法在检测的准确率和运行效率上均优于现有存在的基于深度学习的算法，这也说明了在深度神经网络中考虑对象结构信息的捕获是有意义的，可以有助于提高模型准确率。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "关键词：显著性对象检测；深度学习；显著图；卷积神经网络；对象骨架检测 中图分类号：TP391.41 doi:10.3969/j.issn.1001-3695.2018.01.0064 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Salient object detection algorithm based on structure-sensitive deep neural network ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Li Xinla, Chen Leitingla,1b,2, Cai Hongbinla, 1b (1.a.SchoolofComputerScience&Engineering,b.DigitalMediaTechnologyKeyLaboratoryofSichuanProvince,University ofElectronic Sience&TechnologyofChina,Chengdu 611731,China;2.InstituteofElectronic&InformationEngineringin Guangdong, UniversityofElectronic Science& TechnologyofChina,Dongguan Guangdong 523ooo,China) ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract: Current salient object detectionalgorithms basedondeep neural network (DNN)are usuall notable tobe awareof the structureofinstance,making the generated saliency maps fail tocover theentiresalientobject region,and thus drag down theaccuracy.To solvethis problem,weintroduceda novel multi-streamdeep neural network,inwhich fourcomponents were integrated inasingleframework:featureextractor,objectskeletonsub-network,salientobjectsub-networkandcross-domain conections.Fistlyuringthelearningndtestingprocess,tesalientbjectdetectionsub-networkencodedtheojectstructure which was extractedbyusingobject skeleton detectionsub-network throughthecross-domain connections,soas to make the deep model be aware ofthe informationof object structureandovercome the problemofincomplete detectionof thetargetarea. Then,to further improve theaccuracyweproposed touseadenseconditionalrandom fieldbased algorithm astherefinement post-process,o as to generatea more accurate saliencymapas the finalresults.Experimental evaluations wereconductedon three widely-used benchmarks andtheresults showthat the proposed algorithm outperforms allexisting DNN-baseddetection algorithms inaccuracyand eficiency.Thisalsoindicates that integratingobject structureinformationintodeep neuralnetwork model is meaningful, which can help to improve the overall accuracy. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "KeyWords:salient object detection;deep learning;saliency map；convolutional neural network (CNN);object skeleton detection ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "0 引言",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "显著性对象检测的目的是在某个给定场景中自动地检测出最吸引人注意力的对象区域，从而排除不重要的信息（例如：",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "背景杂质等)。在各种计算机视觉任务中，视觉显著性机制既能提高检测的准确率，也能提高算法的运行效率。显著性对象检测被广泛运用在人工智能相关的任务中，如稠密语义匹配错误！未找到引用源。，人体姿态识别错误！未找到引用源。，对象追踪错误！未找到引用源。等。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "显著性对象检测分为两个主要的研究方向：自顶向下（topdown）的显著性对象检测和自底向上（bottom-up）的显著性对象检测。自顶向下的显著性对象检测与对象检测相关，其目标是自动定位指定类别的对象所在位置。而自底向上的显著性对象检测算法则是自动地识别给定场景中最为显著的一个或者多个对象区域。相比目标驱动的、自顶向下的显著性对象检测算法，自底向上的方法具有下意识、快速以及数据驱动的特征。本文研究自底向上的显著性对象检测算法，即自适应地检测任意场景中最为显著的对象区域。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "自底向上的显著性对象检测可以主要归纳为两类：传统检测模型和基于深度学习的检测模型。传统模型错误!未找到引用源。\\~错误!未找到引用源·利用低级的图像特征（如颜色错误！未找到引用源。、梯度错误！未找到引用源。、深度错误!未找到引用源。等）计算显著性值，并基于简单的先验知识（如中心先验，边缘先验等）和观察进行建模[9，很难用于较复杂的场景。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "近几年，深度学习广泛地运用于计算机视觉的不同任务中。显著性对象检测受益于这种基于深度神经网络、端到端自动学习的高级语义特征。相比传统模型，基于深度模型的显著性检测算法可以利用语义信息极大提升显著性对象检测的准确率。这些基于深度学习的方法可进一步分为基于区域的深度学习检测和基于像素级的深度学习检测。基于区域的显著性检测算法将每个区域当成一个独立的单元进行计算。Li等人错误!未找到引用源。提出一个多尺度显著性对象检测网络去计算每个超像素的显著性值。Wang 等人错误!未找到引用源·把从不同网络提取出来的局部和全局显著性特征融合到统一框架中，然后计算每个区域的显著性值。类似地，Zhao 等人错误!未找到引用源。利用深度神经网络同时提取局部和全局的上下文信息，然后计算每个区域的显著性值。然而，目前的基于区域的深度检测模型都将每个区域看成一个独立的单元，造成计算上的冗余，降低了运行效率。针对这个问题，基于像素级的显著性对象检测通过深度神经网络模型，直接求得每个像素的显著性值。Wang 等人错误!未找到引用源·把先验知识用于循环网络，然后直接预测每个像素的显著性值。Hou等人错误!未找到引用源·提出同时融合多个尺度信息的全卷积网络去检测显著性对象。这个方法基于嵌套网络框架，通过内部的连接去整合多尺度信息。Li等人错误！未找到引用源。提出用级联结构网络同时捕获多尺度上下文信息，以解决对象的尺度问题。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "但由于对象的多样性、训练数据库的样本偏差等，在显著性对象结构复杂的情况下，现有的深度模型，很难检测出完整的对象区域。如何在网络学习和测试的过程中意识和捕获到对象的完整结构信息，是被过去和现有研究所忽略的问题。针对这些问题，本文提出一种基于结构感知的深度神经网络模型，并将之运用于检测场景中的显著性对象。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 本文方法",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "为了在训练和检测中能使得深度神经网络自动捕获和感知对象的整体结构信息，然后利用这些结构信息完整地检测出整个显著性对象区域，本文提出一种基于结构感知的深度神经网络模型。如图1所示，该神经网络模型包括四个部分：特征提取网络，对象骨骼提取子网络，显著性对象检测子网络和跨任务连接部件。本文提出的深度模型贡献在于：a)不同于传统多任务网络错误!未找到引用源。，仅仅通过共享底层表达的方法互相传递信息，本文的网络把跨任务连接部件用在多个尺度上，并深度融合对象骨骼子网络学习的知识和信息，使得显著性检测子网络在特征学习和编码的过程中能主动吸收、整合不同尺度的对象骨骼信息，以便更好地捕捉对象结构特征；b)通过共享的底层表达，对象骨骼检测子网络能共享对象显著性信息，使对象骨骼的检测集中在显著的对象上，从而克服背景杂质的干扰；c)利用全连接条件随机场（denseconditionalrandomfield,DenseCRF）对深度网络的检测结果进行进一步的优化，使得检测结果更准确。",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/bbcbca1252383129be12f70e4ddfb63da920c6024d2a1ef127b0716aa93fb822.jpg",
        "img_caption": [
            "图1基于结构感知的深度神经网络的整体结构"
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "特征提取网络基于 VGG-16 网络结构错误!未找到引用源。。传统的VGG-16网络结构包含5组不同尺度的卷积层以及3个一维向量。由于本文的目标是像素级的检测，因而设计的特征提取网络只取VGG-16的前五组卷积层。另外，为了确保网络深度，模型添加了一个额外的卷积层，维度设为1024维。除此之外，一个额外的反卷积层也被添加在特征提取网络中，用作后续子网络（包括显著性对象检测子网络和对象骨骼检测子网络）的共享底层表达。特征提取网络的前五组卷积层的参数初始化利用在ImageNet中训练的VGG-16对应的网络参数，而新添加的两个卷积层的参数被随机初始化。特征提取网络的输入为任意一张RGB图I，输出为提取的深度特征，标记为 $F$ ，作为后续子网络的底层表达。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1.1对象骨骼检测子网络",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "对象骨骼检测子网络的输入为特征提取网络所提取的深度特征图 $F$ ，输出为对应的对象骨骼图 $S _ { k } ( F ; \\theta _ { s k } )$ ，其中 $\\theta _ { s k }$ 为对象骨骼提取子网络的网络参数。对象骨骼提取网络基于反卷积网络结构，包括5个不同尺度的逆池化层（up-poolinglayer）和反卷积层（De-Convolution layer,Deconv)，每组的尺度与特征提取网络的尺度对应，从而最",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "终获取的对象骨骼图的尺度和输入尺度大小一致。如图2所示，不同于池化层用于整合信息、缩小感受野、排除杂质，逆池化层具有相反的操作，通过重新定位池化区域最大值的位置重建被池化信息。其中逆池化操作和反卷积操作与池化和卷积操作具有完全相反的步骤，用于重建信息。",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/960b350ca3963a60559125c11bd897f913f08d82236ad586a3d81b2b82eb0773.jpg",
        "img_caption": [
            "图2逆池化操作和反卷积操作示意图特征提取网络"
        ],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "由于逆池化操作的输出为一个扩大而稀疏的特征，反卷积操作通过类似于卷积的操作去进一步扩充丢失的信息。另外，每个反卷积层后，额外添加了一个Relu激活函数（Reluactivationfunction）。为了防止过度拟合，对每个Relu层后添加一个Dropout层。对象骨骼检测子网络的配置如表1所示。",
        "page_idx": 2
    },
    {
        "type": "table",
        "img_path": "images/38b7c9c751a4f4d84d1d07af7a7f9099698c74eac298763c7ebdd2bbe414e030.jpg",
        "table_caption": [
            "表1对象骨骼检测子网络配置"
        ],
        "table_footnote": [
            "对象骨骼检测子网络的任务是提取目标图像中对象的骨骼"
        ],
        "table_body": "<html><body><table><tr><td>反卷积层</td><td>设置</td><td>核参数设置</td><td>激活函数</td></tr><tr><td>DeConv1</td><td>DeConv&Unpool</td><td>5×5×512</td><td>Relu</td></tr><tr><td>DeConv2</td><td>DeConv&Unpool</td><td>5×5×256</td><td>Relu</td></tr><tr><td>DeConv3</td><td>DeConv&Unpool</td><td>5×5×128</td><td>Relu</td></tr><tr><td>DeConv4</td><td>DeConv&Unpool</td><td>5×5×64</td><td>Relu</td></tr><tr><td>DeConv5</td><td>DeConv&Unpool</td><td>5×5×32</td><td>Relu</td></tr></table></body></html>",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "信息。网络参数 $\\theta _ { s k }$ 的训练属于像素级的回归问题（pre-pixelregression）。基于此，本文构造了如下目标函数：",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\boldsymbol { L _ { s } } ( \\boldsymbol { \\theta _ { s k } } ) \\mathrm { = } \\operatorname* { m i n } _ { \\boldsymbol { \\theta _ { s } } } \\sum _ { \\mathrm { ~ j ~ } } \\boldsymbol { 1 _ { \\mathrm { s k } } } \\left\\{ \\boldsymbol { S _ { k - g t } } ^ { j } ( \\boldsymbol { p } ) , \\boldsymbol { S _ { k } } ^ { j } ( \\boldsymbol { F } ; \\boldsymbol { \\theta _ { s k } } ) ( \\boldsymbol { p } ) \\right\\}\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "其中： ${ S _ { k - g t } } ^ { j } ( p )$ 表示在第 $j$ 个训练样本，像素坐标 $p$ 处的用户标注（Ground Truth,GT）结果； $S _ { k } { } ^ { j } ( F ; \\theta _ { s k } ) ( p )$ 表示深度模型根据网络参数 $\\theta _ { s k }$ 预测的第 $j$ 个训练样本，坐标 $p$ 处的值。 $1 _ { \\mathrm { s k } } \\{ \\bullet \\}$ 表示损失函数，用于计算深度模型预测结果和用户标注结果的差异。这里，损失函数为典型的交叉熵代价函数，定义如下：",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r l r } {  { 1 _ { \\mathrm { s k } } = - \\sum _ { p \\in Z } S _ { k - g t } ( p ) \\log P ( S _ { k } ( p ) = 1 \\vert F ; \\theta _ { \\mathrm { s k } } ) } } \\\\ & { } & { + ( 1 - S _ { k - g t } ( p ) ) \\log P ( S _ { k } ( p ) = 0 \\vert F ; \\theta _ { \\mathrm { s k } } ) } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "其中： $S _ { k - g t } ( p )$ 表示在坐标 $p$ 处的用户标注， $S _ { k } ( p )$ 表示用户预测的坐标 $p$ 处的结果。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "对象骨骼检测子网络通过大量样本的训练学习到反映对象整体结构的骨骼信息。本文的目标是利用该信息进一步提高显著性对象检测的完整性和准确率，下一节将介绍如何运用学习的对象骨骼信息去帮助学习显著性对象信息。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1.2显著性对象检测子网络",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "显著性对象检测子网络和对象骨骼检测子网络都基于相同的底层表达 $F$ ，输出为一个显著性图 $S _ { s a l } ( F ; \\theta _ { s a l } )$ ，其中 $\\theta _ { s a l }$ 表示对象检测子网络的网络参数。显著性对象子网络的网络参数配置如表2所示。",
        "page_idx": 2
    },
    {
        "type": "table",
        "img_path": "images/d5318d60a1b2d82d1e4f19dd331895f552eb56c093e6c6bc10eaa92339ea79fe.jpg",
        "table_caption": [
            "表2显著性对象检测子网络配置"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>反卷积层</td><td>设置</td><td>核参数设置</td><td>激活函数</td></tr><tr><td>DeConv1</td><td>DeConv&Unpool</td><td>3×3×512</td><td>Relu</td></tr><tr><td>DeConv2</td><td>DeConv&Unpool</td><td>3×3×256</td><td>Relu</td></tr><tr><td>DeConv3</td><td>DeConv&Unpool</td><td>3×3×128</td><td>Relu</td></tr><tr><td>DeConv4</td><td>DeConv&Unpool</td><td>3×3×64</td><td>Relu</td></tr><tr><td>DeConv5</td><td>DeConv&Unpool</td><td>3×3×32</td><td>Relu</td></tr></table></body></html>",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "不同于对象骨骼检测子网络结构，显著性对象检测子网络利用跨任务连接部件同时编码显著性对象信息和对象骨骼信息，帮助学习对象结构感知的显著性特征。具体来说，显著性检测子网络在多尺度的学习过程中与相应尺度的对象骨骼学习得到的特征进行整合：",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nF _ { _ { s _ { i } } } ^ { s a l } = \\sigma \\{ c a t ( f _ { s _ { i } } ^ { s a l } , ~ f _ { _ { s _ { i } } } ^ { s k } ) \\otimes W _ { _ { s _ { i } } } + b _ { _ { s _ { i } } } \\}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "其中： $\\otimes$ 表示卷积操作， $W _ { s _ { i } }$ 表示尺度 $S _ { \\mathrm { i } }$ 的卷积（convolutionalfilters）， $b _ { s _ { i } }$ 表示尺度 $S _ { \\mathrm { i } }$ 的偏置（biases）。本文运用Relu作为非线性激活函数 $\\sigma ^ { \\{ \\cdot \\} }$ 。cat(·)表示一个连接函数，用来连接两个不同的特征图（featuremaps)。如图1所示，两个不同子网络学习的特征通过跨任务连接部件（cross-domainconnections）整合为一个信息更加丰富的特征图，显著性对象检测子网络更高尺度的特征学习基于整合后的特征，从而能编码对象骨骼检测子网络学习的知识。值得注意的是：上述两种信息的整合为非线性的，由深度神经网络通过端到端地训练自动获取。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "类似于对象骨骼检测子网络，显著性对象检测子网络的参数训练也是一个向用户标注数据的回归问题。目标函数构造为：",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nL _ { S A L } ( \\theta _ { s a l } ) { = } \\operatorname* { m i n } _ { \\theta _ { s a l } } \\sum _ { \\mathrm { { j } } } { 1 } _ { { \\mathrm { s a l } } } \\{ S _ { s a l - g t } { } ^ { j } ( q ) , S _ { s a l } { } ^ { j } ( F ; \\theta _ { s a l } ) ( q ) \\}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "其中： ${ S _ { s a l - g t } } ^ { j } ( q )$ 为第 $j$ 个训练样本，像素坐标 $q$ 处的用户标注的显著性值。 ${ S _ { s a l } } ^ { j } ( F ; \\theta _ { s a l } ) ( q )$ 表示显著性对象检测子网络预测的第 $j$ 个训练样本，坐标 $q$ 处的显著性值。损失函数也选用交叉熵代价函数：",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r l r } {  { 1 _ { \\mathrm { s a l } } = - \\sum _ { p \\in Z } S _ { s a l - g t } ( q ) \\log P ( S _ { s a l } ( q ) = 1 | F ; \\theta _ { s a l } ) } } \\\\ & { } & { + ( 1 - S _ { s a l - g t } ( q ) ) \\log P ( S _ { s a l } ( q ) = 0 | F ; \\theta _ { \\mathrm { s a l } } ) } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "其中： $S _ { s a l - g t } ( q )$ 表示图像坐标 $q$ 处用户标注的显著性值，$S _ { s a l } ( q )$ 表示显著性对象子网络的预测值。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "骨骼信息反映了对象的整体结构信息，显著性对象检测子网络在特征学习的过程中，以端到端的方式学习如何整合对象骨骼信息，从而感知对象结构，提高检测结果的完整性。相比单任务反卷积网络，本文在深度神经网络中加入骨骼信息的学习，且将多个尺度的信息融合，从而提高了检测结果的完整性。如图3所示，对于结构复杂的对象，本文模型依然可以准确检测出完整的显著性对象区域。而单任务的反卷积网络，由于不能感知对象结构，则不能很好地保存检测对象区域的完整性。其中GT表示用户标注。",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/319ccb48685544388cc4cb8e9e3412bac3435b86c4651cf65b51ea8b8117a033.jpg",
        "img_caption": [
            "图3单任务反卷积网络（DeconvNet)，本文模型结果对比"
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "由于目前没有可以同时包含对象骨骼和显著性对象区域信息的训练数据集，因此不能够直接训练本文提出的网络模型。为了解决这个问题，本文利用一种交替学习的方式完成网络的训练。主要思想是运用不同的训练数据集交替地训练显著性检测神经网络的两个不同子网络参数，直至目标函数式（1）（4)均收敛。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "1.3显著性图优化方法",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "尽管提出的深度模型已经可以获得准确的检测结果，但是一些极端情形下，深度神经网络模型仍然不能很好地保存对象的边缘信息。为了克服这个问题，本文利用全连接条件随机场（DenseCRF）进一步优化深度模型的检测结果 $S _ { s a l }$ 。类似于语义分割，定义如下能量方程：",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\nE ( L ) = - \\sum _ { i } \\log P ( l _ { i } ) + \\sum _ { i , j } \\omega _ { i , j } ( l _ { i } , l _ { j } )\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "上述能量方程由数据项和平滑项组成。其中 $L$ 表示显著性值的标签。 $L \\in \\{ 0 , 1 \\}$ ，当 $L ( i ) = 1$ 时表示像素 $i$ 属于显著性对象区域，反之当 $L ( i ) = 0$ 表示该对应像素属于非显著性对象区域。$\\log P ( l _ { i } )$ 为数据项，其中 $P ( l _ { i } )$ 的值为深度神经网络预测像素 $i$ 的显著性值，因而 $P ( 1 ) = S _ { s a l } ( i )$ 且 $P ( 0 ) = 1 - S _ { s a l } ( i )$ 。 $\\omega _ { i , j } ( l _ { i } , l _ { j } )$ 为平滑项，用于鼓励颜色相近的相邻像素具有相同的标签。平滑项的定义如下：",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r } { \\omega _ { i , j } ( l _ { i } , l _ { j } ) = \\mu ( l _ { i } , l _ { j } ) \\{ \\gamma _ { 1 } \\exp ( - \\displaystyle \\frac { \\| p _ { i } - p _ { j } \\| ^ { 2 } } { 2 \\nu ^ { 2 } } - \\displaystyle \\frac { \\| I _ { i } - I _ { j } \\| ^ { 2 } } { 2 \\sigma ^ { 2 } } )  } \\\\ {  +  \\gamma _ { 2 } \\exp ( - \\displaystyle \\frac { \\| p _ { i } - p _ { j } \\| ^ { 2 } } { 2 \\kappa ^ { 2 } } ) \\} } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "其中： ${ \\boldsymbol { p } } _ { i }$ 和 ${ { p } _ { j } }$ 用于表示像素 $i$ 和像素 $j$ 所对应的坐标位置， $I _ { i }$ 与 $I _ { \\mathbf { \\Phi } _ { j } }$ 分别表示像素 $i$ 和像素 $j$ 所对应的像素亮度。另外，当$l _ { i } \\neq l _ { j }$ 时， $\\mu ( l _ { i } , l _ { j } ) = 1$ ；反之， $\\mu ( l _ { i } , l _ { j } ) = 0$ 。 $\\nu$ 、 $\\mid o \\mid$ 以及 $\\kappa$ 为能量方程的权重，用于分别控制位置和颜色的相似性等对优化方程的影响。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "求解式（7）为一个NP-hard问题，这里简单采用平均差近似方法求解。优化方程将原本模型输出的显著性图作为输入，并且进行优化，得到一个更加准确的结果作为输出。该优化方法能更很的保存对象的边缘信息以及对象区域连续性。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2 实验和分析",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2.1实验环境 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "本实验的训练和测试环境为：Ubuntu14.04，Caffe框架，并且利用NVIDIAGTXTitanXGPU加速深度神经网络。深度神经网络训练的数据集为：MSRA- $. 1 0 ^ { [ 1 0 ] }$ 作为显著性对象检测子网络的训练集 $\\{ I _ { s { } _ { s l } } , G _ { s { } _ { s l } } \\}$ ；Pascal-SYM[21] 作为对象骨骼检测子网络的训练集 $\\{ I _ { s \\mathrm { k e } } , G _ { s \\mathrm { k e } } \\}$ 。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "测试集采用了三个公共数据集，包括 ECSSD 错误!未找到引用源。DUT-OMRON错误!未找到引用源·以及HKU-IS错误！未找到引用源。其中,ECSSD为目前最为广泛使用的公共数据集。该数据集包括1,000张具有复杂背景的图像。DUT-OMRON为一个大型公共数据库，包含5,168 张非常挑战的图像。这些图像具有一个或者多个显著性对象，这些显著性对象具有各异的姿态和外形。该数据库也被认为是目前最具难度的数据库。HKU-IS 为近期发布的公共数据集。该数据集包含1,447张图像。该数据构建时候特意避免了显著性对象一些简单的先验知识，比如对比度、中心先验等，并且该数据集中的显著性对象一般为多个。因而该数据库也是目前非常有难度的数据库。上述数据库可以用于验证显著性模型的准确性和鲁棒性。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2.2 训练和测试 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "训练和测试的步骤具体如下：",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "a)在训练和测试过程中，先将图像尺度统一调整为 $2 2 4 \\times$ 224，以保证VGG-16网络的输入要求和运行效率；",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "b)在ImageNet上训练VGG-16网络，将得到的网络参数去初始化本文模型中的特征提取网络的前五组卷积层参数，而新添加的卷积层以及反卷积层的网络参数以正态分布来随机初始化。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "c)通过两个数据集：显著性对象检测数据集 $\\{ I _ { s { \\scriptscriptstyle a l } } , G _ { s { \\scriptscriptstyle a l } } \\}$ 和对象骨骼检测子网络的训练集 $\\{ I _ { s \\mathrm { k e } } , G _ { s \\mathrm { k e } } \\}$ 分别训练两个不同的子网络;",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "(a)固定特征提取网络前五组卷积层的网络参数，以及显著性对象检测子网路参数 $\\theta _ { \\mathrm { s a l } }$ ，数据集 $\\{ I _ { s \\mathrm { k e } } , G _ { s \\mathrm { k e } } \\}$ 首先被用来训练对象骨骼检测子网络的参数 $\\theta _ { \\mathrm { s k } }$ ：",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "(b)固定对象骨骼检测子网络参数 $\\theta _ { \\mathrm { s k } }$ ，利用数据集$\\{ I _ { s { \\scriptscriptstyle a l } } , G _ { s { \\scriptscriptstyle a l } } \\}$ 训练显著性对象检测子网络的参数 $\\boldsymbol { \\theta } _ { \\mathrm { s a l } }$ ：",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "(c)上述步骤(a)(b)均采用“poly”衰减算法，根据（204 $( 1 - \\sqrt [ i t e r ] { \\operatorname* { m a x } _ { i t e r } } ) ^ { p o w e r }$ 自动调整学习率。其中Power 被设置为 $1 0 ^ { - 7 }$ ，并且最大训练次数 $\\mathrm { m a x } _ { \\mathrm { i t e r } }$ 设置为60,000；",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "(d)上述步骤交替进行，利用随机梯度下降方法训练参数，直至目标函数收敛。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "d)测试的过程中，将所需要测试的图像作为输入放入训练好的神经网络中，输出初始显著性图。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "e)深度模型输出的初始显著性图运用全连接条件随机场，即式 (6)，进一步优化得出最终检测的显著性图。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2.3验证方法",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "为了验证本文所提方法的有效性和准确性，本文采用四种标准验证方法进行准确性和鲁棒性验证，包括：精确率-召回曲线（PR-curves）、F-measure、weighted F-measure 以及绝对方差错误（MAE）。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "首先，可以根据生成的显著性图，基于不同的阈值将其转换为一系列二值图 $B$ ，并且比较它们和用户标注 $G$ 的差异。因而可以获得给定显著性图的准确率。 $P = \\frac { \\left| B \\cap G \\right| } { \\left| B \\right| }$ ，以及召回率$R = { \\frac { \\left| B \\cap G \\right| } { \\left| G \\right| } }$ 之后，根据不同阈值计算精确率-召回曲线。F-measure（ $F _ { \\beta }$ ）的计算如下：",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\nF _ { \\beta } = ( 1 + \\beta ^ { 2 } ) \\frac { P \\times R } { \\beta ^ { 2 } P + R }\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "根据相关文献错误！未找到引用源。，错误！未找到引用源。的建议，上式中， $\\beta ^ { 2 }$ 设置为0.3。F-measure 指标反映了方法的准确性，除此之外，weighted F-measure（ $F _ { \\beta } ^ { \\omega }$ ）也被用于验证显著性检测算法的准确性，计算如下：",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\nF _ { \\beta } ^ { \\omega } = ( 1 + \\beta ^ { 2 } ) \\frac { P ^ { \\omega } \\times R ^ { \\omega } } { \\beta ^ { 2 } P ^ { \\omega } + R ^ { \\omega } }\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "其中， $P ^ { \\omega }$ 和 $R ^ { \\omega }$ 为加权准确率（weighted precision）和加权召回率（weighted recall）。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "绝对方差错误则用于计算每个像素和用户标注（groundtruth）的平均误差，计算如下：",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\n\\mathit { M A E } = \\frac { 1 } { W \\times H } \\sum _ { \\mathrm { x = 1 } } ^ { W } \\sum _ { \\mathrm { y = 1 } } ^ { H } \\bigl | S ( x , y ) - G ( x , y ) \\bigr | ,\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "其中 $W$ 和 $H$ 分别代表图像的长度和宽度。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2.4对比分析",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "为了验证本文提出算法的有效性和鲁棒性，与5个显著性对象检测算法进行对比，其中包括2个最具代表性的传统显著性对象检测算法：基于鲁棒背景检测的显著性对象检测算法(robust background detection,RBD)错误!未找到引用源。，显著性区域特征融合的显著性对象检测算法(Discriminative RegionalFeatureIntegration,DRFI) ）错误!未找到引用源。；以及3个基于深度学习的显著性对象检测方法，包括：基于距离图的深度显著性对象检测算法(encoded low-level distance,ELD）错误!未找到引用源。，基于深度全卷积网络的显著性对象检测算法(dee saliency，DS)错误！来投到引用器，基于深度对比度的显著性对象检测算法(deep contrast learning,DCL）错误!未找到引用源。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2.4.1定量分析",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "为了验证本文所提算法的准确性和鲁棒性，首先利用F-measure值比较本文所提算法和其他显著性对象检测算法的准确性。如表3所示，本文所提算法在三个公共数据集均获得最高的F-measure值。具体来说，本文所提方法在ECSSD数据库的结果表明本文算法可以在复杂的场景中准确检测出结构复杂的显著性对象。在DUT-OMRON中，本文所提算法也高于所有基于深度学习的显著性对象检测算法，F-measure值达到了0.765，远高于ELD的0.720。该数据库为当前最大数据库，包含了各种不同的场景和对象，因此可以证明本文方法具有一定的鲁棒性，可以用于检测各种复杂场景中的显著性对象。在HKU-IS数据集中，本文算法也获得最高的F-measure值。这证明本文方法可以比当前算法更鲁邦地检测出同一场景中的多个显著性对象。根据F-measure的检测结果，还可以得出如下结论：a)利用深度神经网络提取的语义信息可以极大地提升显著性对象检测算法的准确性；b）本文算法由于整合了对象的结构信息，因而可以更好的检测出复杂场景中的显著性对象，使得方法更具准确性和鲁棒性；c）运用全连接条件随机场优化深度模型输出的显著性图可以进一步提高检测的准确率。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "如表3所示，本文算法在三个公共数据集上都获得了更低的MAE值。具体来说，与对比算法比较，在ECSSD、DUT-OMRON以及HKU-IS，本文所提方法MAE 值分别降低了$19 . 4 \\%$ 、 $2 6 . 3 \\%$ 以及 $2 7 . 1 \\%$ 。该验证结果说明本文算法可以生成和用户标注更为接近的结果，证明了该方法的准确性和鲁棒性。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "主要原因在于：a）整合对象骨骼信息可以使得深度网络感知对象结构，从而能完整的检测出整个显著性对象区域；b）运用全连接条件随机场对显著性图进一步优化可以更好的保存对象的边缘信息，使得结果更加准确。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "最后，根据精确率-召回曲线（PR-curves）和weightedF-measure（图4)，本文算法在三个公共数据集上也有更优结果。再次证明了本文所提算法的准确性和鲁棒性。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "表3不同方法在三个公共数据集的结果比较结果。其中，\"0urs\"表示本文深度神经网络模型，而 $' 0 \\mathrm { u r s } { + } ^ { \\prime \\prime }$ 表示运用额外的DenseCRF进一步优化模型输出的结果。",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/51e6d6cb4192ce15f45a2e63218c615382a2a1f35fe5942473604e5400a070f2.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">方法</td><td colspan=\"2\">ECSSD</td><td colspan=\"2\">DUT-OMRON</td><td colspan=\"2\">HKU-IS</td></tr><tr><td>FB</td><td>MAE</td><td>F</td><td>MAE</td><td>FB</td><td>MAE</td></tr><tr><td>DRFI</td><td>0.786</td><td>0.164</td><td>0.665</td><td>0.155</td><td>0.684</td><td>0.226</td></tr><tr><td>RBD</td><td>0.716</td><td>0.171</td><td>0.630</td><td>0.144</td><td>0.657</td><td>0.120</td></tr><tr><td>DS</td><td>0.882</td><td>0.122</td><td>0.716</td><td>0.120</td><td>0.866</td><td>0.079</td></tr><tr><td>ELD</td><td>0.869</td><td>0.098</td><td>0.720</td><td>0.091</td><td>0.777</td><td>0.121</td></tr><tr><td>DCL</td><td>0.887</td><td>0.072</td><td>0.718</td><td>0.094</td><td>0.879</td><td>0.059</td></tr><tr><td>Ours</td><td>0.900</td><td>0.068</td><td>0.753</td><td>0.077</td><td>0.893</td><td>0.056</td></tr><tr><td>Ours+</td><td>0.907</td><td>0.058</td><td>0.765</td><td>0.067</td><td>0.902</td><td>0.043</td></tr></table></body></html>",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2.4.2定性分析",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "如图5所示，本文算法在检测复杂场景中的显著性对象区域时，能够更好地排除背景杂质、完整提取显著性对象区域。对比现存显著性对象方法，本文方法在应对这些相对少见的场景时，可以更准确地发现显著性对象区域，从而证明该方法具有更好的鲁棒性。也说明了，在深度神经网络中考虑对象骨骼信息的整合有利于深度神经网络更好的感知对象的形状信息，从而帮助克服二义性问题。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "另外，从定性比较中，还可以得出如下结论：a）相比基于人工设计特征的显著性对象检测方法，基于深度神经网络的显著性对象检测方法可以更准确地检测复杂场景中的显著性对象；b)在深度网络模型中考虑对象的结构信息，可以帮助克服复杂场景检测时的二义性问题，提高检测方法的鲁棒性。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "□ □   \n0.8 iDid 0.8 Ed 0.8 □ F 2 RBD RBD RBD DRFI ·DRFI DRFI DS DS C 0.4 DS b ELD ELD ！ ELD   \n0.2 DCL DCL 0.2 DCL Ours Ours Ours 0 0 0 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Recall Recall Recall 1 I 0.8 0.6 0.6 se 0.4 0.4 pegem 0.2 0.2 0 0 0 RBDDRFI DS ELD DCL OurS RBD DRFI DS ELD DCL OurS RBD DRFI DS ELD DCL OurS (a)ECSSD (b)HKU-IS (c) DUT-OMRON ",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/c9eb558c411ad049d339edb3653f01580b43c30e2a26cb6025c81f0bb9f520a0.jpg",
        "img_caption": [
            "图4准确率-召回曲线和weightedF-measure ",
            "图5不同方法定性比较，GT表示用户标注结果。"
        ],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "2.4.3运行效率分析",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "运行效率测试结果如表4所示。传统方法的测试平台为i7$2 . 5 0 ~ \\mathrm { G H z }$ CPU以及8GBRAM；而基于深度学习的显著性对象检测方法的测试平台为NVIDIAGTXTitanXGPU以及12G显存。实验结果表明，传统方法的运行效率低于基于深度学习的显著性对象检测算法。本文的深度模型运行效率为30fps。但是由于本文算法的结果需要额外的后处理，因此处理一张输入图像的整体运行时间为0.51s，依然优于所有对比方法。",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "表4 运行效率比较（单位：s)。其中，\"Ours\"表示本文深度神经网络模型，而 $\" \\mathrm { O u r s } { + } \"$ 表示运用额外的DenseCRF进一步优化模型输出。",
        "page_idx": 6
    },
    {
        "type": "table",
        "img_path": "images/d4e49b3b758efd103e740a0eafb537339a1584cacd72d1d4be5075f2fc9eabf9.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>DRFI</td><td>RBD</td><td>DS</td><td>ELD</td><td>DCL</td><td>Ours</td><td>Ours+</td></tr><tr><td>6.34</td><td>0.52</td><td>0.73</td><td>0.59</td><td>1.17</td><td>0.02</td><td>0.51</td></tr><tr><td>CPU</td><td>CPU</td><td>GPU</td><td>GPU</td><td>GPU</td><td>GPU</td><td>GPU</td></tr></table></body></html>",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "3 结束语",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "本文提出一种基于深度学习的显著性对象检测算法。不同于现存基于深度学习的显著性对象检测算法，本文算法在模型设计中考虑了对象结构信息的感知，从而克服了检测复杂显著性对象时结构保存不完整的问题，获得了更高的准确率。为了获取对象的结构信息，本文设计了一个多流的深度神经网络，并且在网络中多尺度融合学习的骨骼信息。除此之外，为了进一步提高显著性图的准确性，本文运用全连接条件随机场优化深度模型的检测结果。实验结果表明，比较传统显著性对象检测算法和现有基于深度学习的检测算法，本文算法在定量和定性指标上都有改善，也说明了在显著性对象检测中考虑对象结构信息的必要性。如何把基于对象结构信息的深度网络模型运用在更多的像素级预测任务，如语义分割和场景分割，以及如何更好地在深度模型中消除冗余信息错误:未找到引用源。是未来工作值得关注的方向。",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "参考文献：",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "[1]Yang Fan,Li Xin,Cheng Hong et al. Object-aware dense semantic correspondence [C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ: IEEE Press,2017.   \n[2]Chu Xiao,Yang Wei, Ouyang Wanli et al.Multi-Context Attention for Human Pose Estimation [C]/ Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ: IEEE Press,2017   \n[3]BorjiA,Frintrop S,Sihite D.N,et al.Adaptive object tracking by learning background context [C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ: IEEE Press,2012:23-30.   \n[4] 徐新，穆楠，张晓龙．一种鲁棒的夜间图像显著性对象检测模型[J]. 软件学报,2018,29(9):1-16.(Xu Xin,Mu Nan,Zhang Xiaolong.A robust salient object detection model for nightime images [J]. Journal of Software, 2018,29 (9): 1-16)   \n[5] 周洋，何永健，刘晓琪，唐向宏，殷海兵．结合立体视觉舒适度的立体 图像显著性检测[J].软件学报,2017,28(s2):1-10.(Zhou Yang,He Yongjian,Liu Xiaoqi et al. Saliency detection for stereoscopic images by considering stereo visual comfort [J]. Journal of Software,2017,28 (s2): 1- 10)   \n[6]Geng Yujie.leveraging stereopsis for saliency analysis [C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ: IEEE Press,2012: 454-461.   \n[7]Kim Jand Grauman K. Shape sharing for object segmentation [C]/ Proc of European Conference on Computer Vision. [S.1.] $\\because$ Springer,2012: 444- 458.   \n[8]Klein DA,Frintrop S.Center-surround divergence of feature statistics for salient object detection [C]//Proc of International Conference on Computer Vision,NJ: IEEE,2011: 2214-2219.   \n[9]Li Xin,Yang Fan,Chen Leiting et al. Saliency transfer:an example-based   \n[10] Li Guanbin and Yu Yizhou.Visual saliency based on multiscale deep features [C]/ Proc of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway,NJ: IEEE Press,2015   \n[11] Wang Lijun,Lu Huchuan, Xiang Ruan et al.Deep networks for saliency detection via local estimation and global search [C]// Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ: IEEE Press, 2015   \n[12] Zhao Rui, Ouyang Wanli,Li Hongsheng et al. Saliency detection by multicontext deep learning [C]// Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ: IEEE Press, 2015   \n[13]Wang Linzhao,Wang Lijun,Lu Huchuan et al.Saliency detectionwith recurrent fully convolutional networks [C]//Proc of European Conference on Computer Vision. [S.1.]: Springer,2016: 825-841.   \n[14] Hou Qibin, Cheng Ming Ming,Hu Xiaowei et al. Deeply Supervised Salient Object Detection With Short Connections [C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway, NJ: IEEE Press, 2017   \n[15] Yang Fan,LiXin,Cheng Hong etal.Multi-scale cascade network forsalient object detection[C]//Proc ofACM International Conference on Multimedia New York: ACM Press, 2017.   \n[16] Li Xi, Zhao Liming,Wei Lina et al. DeepSaliency: Multi-task deep neural network model for salient object detection.[J].IEEE Transon image processing,2016,25 (8): 3919-3930.   \n[17] Simonyan K and Zisserman A. Very deep convolutional networks for largescale image recognition [Cl//Proc of International Conference on Learning Representations. 2015   \n[18] Xie Yulin,Lu Huchuan and Yang Minghsuan. Bayesian saliency via low and mid level cues [J].IEEE Transon Image Processing.2013,22(5):1689-98.   \n[19] Yang Chuan, Zhang Lihe,Lu Huchuan et al. Saliency detection via graphbased manifold ranking [C]// Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ:IEEE Press,2013:3166-3173   \n[20] Zhu Wangjiang,Liang Shuang,Wei Yichen et al. Saliency optimization from robust background detection [C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ: IEEE Press,2014:2814- 2821.   \n[21] Jiang Huaizu, Wang Jingdong, Yuan Zejian et al. Salient object detection: a discriminative regional feature integrationapproach [C]//Proc of IEEE Conference on Computer Vision and Pattern Recognition.Piscataway,NJ: IEEE Press,2013: 2083-2090.   \n[22] Lee G,Tai YW,Kim J.Deep saliency with encoded low level distance map and high level features [Cl//Proc of IEEE Conference on Computer Vision and Pattern Recognition. Piscataway, NJ: IEEE Pres,2016: 660-668.   \n[23]Li Guanbin,Yu Yizhou.Deep contrast learning for salient object detection ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "[C]//Proc ofIEEE Conference on Computer Vision and Pattern Recognition. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Piscataway,NJ:IEEE Press,2016. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "[24]Dai Jianhua,Wei Bingjie,Shi Hong et al.Uncertainty measurement for incomplete interval-valued information systems based on $\\mathfrak { a }$ -weaksimilarity [J].Knowledge-Based Systems,2017,136:159-171. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    }
]