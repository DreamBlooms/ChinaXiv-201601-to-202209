[
    {
        "type": "text",
        "text": "虚拟软件路由器数据包转发性能实验研究",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "贺鹏 杨建华 张建华 谢高岗",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "摘要：未来网络的体系结构需要根据网络业务自适应进化调整，因此灵活与高性能的路由器是构建未来网络的基础。虚拟软件路由器支持网络虚拟化和可编程的特性，能够在一个物理网络上承载多个不同网络体系结构的逻辑网络，实现业务创新，具有良好的灵活性。但是虚拟化所带来开销会给虚拟路由器的数据包转发性能带来很大影响。如何在保证灵活性的前提下提高转发性能成为迫切需要解决的问题。本文对不同的虚拟技术和可扩展路由软件进行介绍，并搭建实验平台，测试了KVM（全虚拟化）、Xen（半虚拟化)、OpenVZ（操作系统级别虚拟化）等的转发性能，评估了不同I/O加速技术对转发性能的提升。该实验数据对定位虚拟化软件路由器的转发性能瓶颈，提高转发性能具有重要的指导意义。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "关键词：虚拟化 软件路由器 转发性能I/O加速",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1引言",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "近几年来，网络虚拟化成为一项被广泛关注的技术。虚拟路由器作为虚拟网络中的核心网络设备，对比现有网络设备有以下两点优势：1）虚拟路由器能够将一台物理路由设备虚拟成多台路由设备，多个虚拟设备并行处理不同的业务流量，并根据业务的特性提供相应的功能扩展，从而满足物联网、云计算服务等新兴应用对不同业务下的数据包处理策略提出的多样化要求。同时虚拟路由器还能根据虚拟网络的应用规模，给特定的虚拟设备分配特定的资源，尽可能提高设备的资源利用率。如，一个电信运营商可以使用单独一个虚拟网来承载一个大公司的网络业务，使用另一个虚拟网络来承载多个小公司的网络业务。各个虚拟网络完全隔离，提高了网络的鲁棒性。2）在一个由虚拟路由器搭建的虚拟网上，不同的虚拟设备可以支持不同的协议栈。如在一台虚拟设备中采用TCP/IP 协议栈，而在另一台虚拟设备中完全采用新的协议。这样就能保证在同一个物理网络的基础设施上，并行进行不同的实验而不互相干扰，满足对未来互联网研究的需要。同时，相对于物理实验网络，虚拟实验网络能提供更快捷的搭建速度，更低廉的搭建成本。目前许多国家和组织正在积极研究未来互联网的发展，如美国的GENI项目[3]，欧盟的FIRE 项目[4等，提出了许多新的互联网架构。虚拟网能在不影响原有网络正常运行的条件下满足这些研究的实验需求，将这些先进的架构更快地部署到实际网络中。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "现有的虚拟路由器实现平台可以分为两大类：通用硬件平台与专用硬件平台。通用硬件平台使用可扩展的路由器软件（如 Click[1],XORP[2]等)和虚拟化软件（如 Xen[16],OpenVZ[17]等）设计实现虚拟路由器。近年来，基于通用硬件平台的软件路由器在转发性能上取得了较大突破。多布雷斯库（M.Dobrescu）等在充分挖掘单个物理设备的吞吐量潜力的基础上，基于集群方式设计实现了RouteBricks 软件路由器。实验结果表明，四个通用硬件平台的服务器构建的 RouteBricks 集群，可以提供高达 35Gbps 的吞吐率。韩祥进（Sangjin Han）等[6],利用 GPU1的强大并行计算能力，研制出单台吞吐率可达 39Gbps 的通用硬件路由器PacketShader。专用硬件可编程平台的虚拟路由器使用专用硬件重新设计路由器的数据平面。如安瓦尔（M.B.Anwer）等9]首次使用NetFPGA[21实现了多个虚拟路由器的数据平面。每个数据平面采用相同的结构，可以实现查表，转发等功能。吕国翰（音译，GuohanLu）等人[10]提出了一种可配置的数据转发引擎——CAFE。CAFE通过使用不同API2配置底层硬件，得到不同的数据平面，从而避免了重新设计硬件。乌尼克里希南（DeepakUnnikrishnan）等人[提出了硬件可重构和软件硬件迁移的概念，即当某个虚拟路由器需要处理的流量增大时，可以将该虚拟路由器的路由表拷贝到硬件上，实现硬件数据平面迁移，而将原先在硬件的数据平面迁移到软件层面；当需要部署新的数据平面时，则可以将所有的数据平面全部迁移到软件层面，对现场可编程门阵列写入新的逻辑。安瓦尔等人[12提出了一种模块化的流水架构 SwitchBlade，支持硬件可编程，从而在保证硬件快速转发性能的前提下，支持新协议的快速原型设计与部署。使用专用硬件设计的虚拟路由器需要在保证高性能的同时，保证底层数据平面的可扩展性。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "网络带宽的激增对路由器的吞吐率提出了非常高的要求。传统路由器对数据包的处理过程简单，在数据平面和控制平面上做了充分的优化，提供了非常高的吞吐率。虚拟软件路由器由于虚拟化、可扩展等技术特点，需要提供更高的转发性能，以满足各种新兴业务的深度数据包处理。在以上的讨论中，无论采用哪种设计方案，都需要使用到虚拟化技术，而虚拟化技术的引入势必会对路由器I/O效率产生影响。因此，对各种不同虚拟化技术所带来的性能开销进行评估是必要的。本文测量了几款有代表性的用虚拟化平台实现的软件路由器的转发性能，在此基础上评估了几种常见的I/O加速技术，以便定位转发性能瓶颈，优化转发性能。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "本文其他章节安排如下，第二章主要对虚拟路由器的相关技术进行介绍，包括不同的虚拟技术的原理，几种可扩展路由器的架构等。第三章对几种常见的虚拟软件的数据包转发性能进行了测试，并给出实验结果。第四章介绍了一些常见的IVO加速技术，并搭建和评估了这些IO 加速技术。第五章对全文进行总结，并对I/O加速研究的可能途径进行了展望。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2 虚拟化软件路由器原理与实现",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.1虚拟化软件路由器",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "虚拟软件路由器的典型框架如图1所示，虚拟路由器由三大组件构成：虚拟化软件平台，数据转发模块，控制模块。虚拟软件作为中间层（Hypervisor），给不同虚拟机提供虚拟I/O通道。数据转发模块可以由专用硬件或者通用网卡构成。在虚拟机内部则可以运行控制模块计算路由，或者用于承载实验网，进行新协议（如OpenFlow[7]等）的实现。",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/7e9f8161a2d418f9ea5131c7d938837e8a0cc3132b5fd370dda7d0ca222db6cd.jpg",
        "img_caption": [
            "图1.虚拟路由器框架图"
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2.2虚拟化实现 ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "虚拟软件路由器所使用的虚拟化技术按照虚拟层次的不同，大致可以分为三类",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "全虚拟化（FullVirtualization）",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "全虚拟化又称本地虚拟化（NativeVirtualization）。该技术在操作系统和底层硬件之间使用一个中间层Hypervisor，使得底层硬件能够被上层操作系统共享访问。当客户操作系统执行某项特权指令时，中间层将接管该段代码，并执行对应的操作，如给某个硬件的某个寄存器写入一个值等。由于在全虚拟化技术中，客户操作系统不能对底层硬件进行任何直接控制，而必须通过中间层进行代理，使得I/O性能会受到一定影响。相对于硬件虚拟化技术，全虚拟化技术可以支持不同的操作系统，但是不能模拟不同硬件平台。全虚拟化技术的代表软件为VMware,KVM[14]等。近年来一些新的X86系列的CPU在指令级别上支持全虚拟化，使得全虚拟化环境下的操作系统能达到原生级别的运行速度。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "半虚拟化（ParaVirtualization）",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "半虚拟化技术又称准虚拟化技术。该技术同样采用中间层来隔离底层硬件和操作系统。不同的是，它试图修改操作系统代码，让客户操作系统本身知道自己运行在一个虚拟化平台上。该技术能够显著提升虚拟化技术的IO性能。例如，当网卡收到一个数据包时，在采用全虚拟化的情况下，中间层必须要模拟硬件中断，然后将数据包传输到某个操作系统上，而由于在半虚拟化技术中，操作系统本身就知道自己运行在一个虚拟化平台上，底层的中间层可以使用一定方式，批量地将数据包传输给该操作系统，而不需",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "要模拟硬件中断。由于半虚拟化技术需要对操作系统代码进行修改，使得该项技术在非开源的商业操作系统上应用比较困难，如Windows。半虚拟化技术代表软件为Xen[16]，UML[15]等。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "操作系统级别虚拟化（Operating System-level virtualization) ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "操作系统级别虚拟化和以上两种虚拟化都不相同，更类似于UNIX的Jail系统调用。它利用操作系统所提供一些机制（如Linux中命名空间），将一组进程放入到“容器”（container）中，与主操作系统的进程隔离开来，并使用这些机制限制这一组进程的CPU 和内存占用率，从而达到虚拟化技术中资源的隔离与分配的目的。这类虚拟化技术只能运行在特定的操作系统上（几乎都是Linux）。所有的“虚拟机”（在该技术中，称为Server)都共享一个内核。",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/06f61a082391e009535b11a782587046e447bec2ee30cfa30267aa0802fedc65.jpg",
        "img_caption": [
            "图2.Click以太网交换机（Ethernet Switch） 配置图"
        ],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "该虚拟化技术的代表软件为",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "OpenVZ[17]，Linux-VServer[18]，LXC[19]等。操作系统级别虚拟化所带来的开销非常小。  \n但其主要缺点是不能运行多个异构的操作系统。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.3数据转发模块",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "(1).Click ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Click 是一个模块化的软件路由器。它将路由器设计的各个部分模块化（Click称之为“元素”Element)，提供了一种灵活的可扩展的路由器软件套件。Click 抽象出两种操作，将不同元素连接起来。一种称为Push，即数据包由源端处理完毕后，从该连接的源端传递到目的端；一种称为Pull，即连接的目的端发起一个请求，向源端请求一个数据包处理。图2是一个使用Click 组合成的以太网交换机的框架图。不过，Click 虽然能够将数据包转发到指定的端口，但是并没有提供路由计算的能力。Click不能动态地计算路由。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Click 软件套件对Linux 内核进行了修改，提供了轮询驱动方式，能够在普通网卡上获得小包转发性能的很大提升。许多关于软件路由器的研究[5.8使用了Click作为他们的数据转发模块。一些路由器软件，如 XORP 等，也可以将Click 作为其转发引擎。RouteBricks 正是使用了Click的轮询驱动达到了非常可观的吞吐率。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "(2)．硬件数据平面",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "使用专用硬件构建路由器的转发模块，在数据包转发性能上有软件不可比拟的优势，并且这些硬件在设计上还集成了路由表。使用 TCAM等硬件进行快速路由查找，能够大大降低CPU的负担。不足之处在于，硬件的资源十分有限，目前容量最大的TCAM也只能存储几千条路由表，不能满足实际网络中20\\~30万的表项存储需求。使用硬件作为数据转发模块需要设计算法，构建“快表”，将频繁查找的IP表项交由硬件处理，而不频繁的表项交由软件处理。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2.4控制模块",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "(1)．基于路由的控制模块XORP ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "汉德利（M.Handley）等人意识到现有的商业路由器软件的封闭导致新协议的实现和研究缺乏一个可靠的测试平台，因此设计并实现了XORP，旨在提供一个开放的、可扩展的路由器软件平台，便于研究人员部署和实现新路由协议。XORP已经被工业界和教育研究机构广泛采纳。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "XORP采用多进程机制，通过转发引擎抽象层（Forward Engine Abstract）将数据包的交换及路由计算和转发引擎隔离开来，既保证了鲁棒性，也保证了开放性和性能。图3是XORP的结构示意图。图中XORP使用了Click作为转发引擎。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "(2)．基于转发表的控制模块OpenFlow[7] ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/79c0c5df743ad193e0b5fcaf328e793308cbaf9d79b7048ce9603cf5b2208bc6.jpg",
        "img_caption": [],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "+中间系统到中间系统的路由选择协议Intermediate  \nSystemto Intermediate System  \n\\*Protocol IndependentMulticast-SparseMode,  \n协议无关稀疏组播模式",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "OpenFlow是斯坦福大学提出的一种 图3.XORP结构示意图新的网络体系结构。OpenFlow 采用主从结构来进行网络管理。在一个OpenFlow 网络中，主控设备（Controllr）存储所有的转发策略，从设备（OpenFlow Switch，交换单元）则负责数据转发。每个从设备都包含一张转发表，存储转发规则。当数据包匹配其中一项规则，从设备便按照规则将数据包从它的某个端口转发出去，当数据包没有匹配其中任何一项，从设备便咨询主控设备，获得新的转发规则。OpenFlow 使用“十元组”（MAC4对，IP对，端口对，等等）作为转发策略依据，能够实现非常复杂的转发策略。同时，主控设备开放了操作转发表的API，能满足可编程网络的需求。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3 不同虚拟平台的转发性能测试",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "我们根据虚拟化层次的不同，选取了三款主流虚拟化软件，KVM（全虚拟化)，Xen（半虚拟化），OpenVZ（操作系统级别虚拟化)，分别进行了数据包转发测试。实验平台的各项参数如表1所示。",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/c40d4d3007ff2e36234f51bead386aa8beacac7f5e4eaf45dfdd3dd4c5cb3f6f.jpg",
        "table_caption": [
            "表1实验平台参数"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>硬件</td><td>规格说明</td></tr><tr><td>CPU</td><td>IntelXeonL54202路四核，共8核 每核一级缓存32K:16K（指令）+16K （数据）</td></tr><tr><td>内存</td><td>四核两两共享二级缓存6M DDR2667M赫兹16G</td></tr><tr><td>主板</td><td>SuperMicro X7DWU</td></tr><tr><td>网卡</td><td>Intel 82571EB千兆，四口</td></tr></table></body></html>",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.1实验配置 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "我们使用 Spirent Testcenter作为测试仪表，采用RFC 2544 标准测试方式测试了不同的虚拟软件的吞吐率。选取的包大小依次为64字节（最小包）、512字节（网络中的平均包大小）、1518 字节（最大包)。同时，我们根据上述实验结果，又设计了多虚拟机情况下的数据包吞吐率实验。我们将不同的虚拟机运行在不同的网段中测试其总体转发性能，由于Spirent仪表不支持这种测试方式，我们采用了另一种方式表征吞吐率，具体说明见3.2.2节。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "不同的虚拟软件的网络配置方式各有不同，我们只选取桥接（Bridge）模式的网络配置来进行。以下对这些软件做一些简单的介绍，并分别说明不同虚拟软件的网络配置情况。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "(1). KVM ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "KVM，意即内核虚拟机（KernelVirtualMachine)，与主流虚拟化技术Xen的最大区别在于，它并没有单独实现一个中间层，而是向内核添加一两个模块，将内核作为中间层的一种虚拟化技术。KVM 是全虚拟化技术，需要CPU对虚拟化有一定的支持。图4显示了KVM的网络配置。图中eth $\\boldsymbol { x }$ 为以太网接口；tap $\\boldsymbol { x }$ 为客户操作系统的以太网网口在中间层上的一个镜像网口； $\\operatorname { B r } x$ 为桥接模块。",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/870e4ca5f90703beeee27699bf91d6487f5a9b7694f3466ee3d0cf6e3a1dec09.jpg",
        "img_caption": [
            "图4.KVM网络配置"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "(2). Xen ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Xen 是剑桥大学开发的一套半虚拟化软件，它实现了一个精简的中间层Hypervisor。Xen使用术语 Domain（域）来描述虚拟化环境。Xen 具有两种Domain，DomO 和DomU。Dom0又称 DriverDomain（驱动域)，运行在该Domain上的操作系统能够直接访问硬件。运行在",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "DomU（客户域）的操作系统则必须通过Dom0的代理。测试中Xen的网络配置采用桥接模式，分别测试了Dom0 和DomU的转发性能。图5显示了Xen的DomU网络配置情况。图中peth $\\boldsymbol { x }$ 为实际物理网口；vif $\\boldsymbol { x }$ 为客户操作系统的以太网网口在中间层上的一个镜像网□。",
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/a10f3557a1d1efb9e3b6b79b632589fe9ffe3b939a1fc990831c14a8cbf6b7ce.jpg",
        "img_caption": [
            "图5.XenDomU 网络配置"
        ],
        "img_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "(3). OpenVZ ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "OpenVZ是Swsoft公司专有软件Virtuozzo的开源版本。它使用一种虚拟网络接口对的方式实现桥接。图6显示了OpenVZ的网络配置情况。图中veth$\\mathbf { \\sigma } _ { \\mathbf { \\boldsymbol { x } } }$ 为客户操作系统的以太网网口在中间层上的一个镜像网口。",
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/893bcb61d136dd00820b0ac06bc66054cd969c54b7e4f2ca007e6b49e0d066d8.jpg",
        "img_caption": [
            "图6.OpenVZ网络配置"
        ],
        "img_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.2实验结果与分析",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.2.1原生操作系统NativeLinux测试 ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "我们首先对没有采用任何虚拟软件的系统平台（NativeLinux）进行了数据包转发测试，测试结果如下。从测试结果可以看出，普通网卡对最小包的转发性能较差。这主要是由于普通网",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/a20a92a671b47320de0ad4f83e88d42fe64e9abd4743730d9bcc76dbe3599c0a.jpg",
        "table_caption": [
            "表2NativeLinux转发性能测试"
        ],
        "table_footnote": [
            "卡采取的中断方式所带来的系统开销过大。一些研究表明[5.6]，采取轮询方式，对小包转发能够带来很大的性能提升。"
        ],
        "table_body": "<html><body><table><tr><td>帧长度</td><td>速率(%)</td><td>速率(帧/秒)</td><td>理论最高速 率 (帧/秒)</td></tr><tr><td>64</td><td>26.171</td><td>778,898</td><td>2,976,190</td></tr><tr><td>512</td><td>100</td><td>469,924</td><td>469,924</td></tr><tr><td>1518</td><td>100</td><td>162,548</td><td>162,548</td></tr></table></body></html>",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.2.2虚拟化技术性能测试结果与分析",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/a62b983ecc79257fab6c7f9536dda1a432075ee3d7f9b5832a2141b932040183.jpg",
        "table_caption": [
            "表3各种虚拟化技术转发性能测试"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">帧长度</td><td colspan=\"4\">吞吐速率 (帧/秒)</td></tr><tr><td>KVM</td><td>Xen Dom0</td><td>Xen DomU</td><td>OpenVZ</td></tr><tr><td>64</td><td>29,762</td><td>611,488</td><td>113,452</td><td>590,565</td></tr><tr><td>512</td><td>31,132</td><td>469,924</td><td>139,506</td><td>469,924</td></tr><tr><td>1518</td><td>26,541</td><td>162,548</td><td>124,831</td><td>162,548</td></tr></table></body></html>",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "表3是分别对不同虚拟化软件的数据包转发性能测试后得出的结果。从结果中可以看出，全虚拟化带来的IO性能开销非常大。这主要是由于数据包在内核中经过的路径太长，上下文切换过多。而半虚拟化则由于其针对虚拟化环境做了一定的优化，性能上相对于全虚拟化有一定的改进，但是仍然和原生操作系统的性能有较大差距，而操作系统级别的虚拟化则由于开销非常小，除小包转发外，在各项数据上都和原生操作系统非常接近。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/380a92fc5978022ccc1476e7febe0a6f0dab1ee311fef42c93f494b9beb43d2d.jpg",
        "img_caption": [
            "图7.OpenVZ性能测试"
        ],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/2d3dab94a33948cda50854e3374c25f1eed2e02db9c57482e9d932fa472b98d9.jpg",
        "img_caption": [
            "图8.Xen性能测试"
        ],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/69f8b8ed33a57a37a48cc9c1627b8c16447eafbaed590bc81a83fbef44bf9b3d.jpg",
        "img_caption": [
            "图9.KVM 性能测试"
        ],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/51f672cb6413984ba2970e4ef62b61fae6a174f1c6dca86e20d6ad28cfdb7b60.jpg",
        "img_caption": [
            "图10.几种不同技术性能对比"
        ],
        "img_footnote": [],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "根据实验结果，我们认为，全虚拟化技术由于虚拟化开销太大，不适合作为虚拟路由器的虚拟化平台。半虚拟化和操作系统级别虚拟化各有优缺点。半虚拟化技术虽然在转发性能上与原生系统相比有很大差距，但它提供了更好的隔离性和鲁棒性。根据实验结果，我们又对Xen 和OpenVZ进行了多虚拟机情况下的性能测试，结果如表4所示。表4使用实际转发速率（fps）与理论最大转发速率的百分比表示性能。",
        "page_idx": 6
    },
    {
        "type": "table",
        "img_path": "images/d7f3319a7ac66f0a68734a84e66808ef202e5d48ccbc189c0e71ed9cfb82f9f3.jpg",
        "table_caption": [
            "表4不同虚拟机的转发性能"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">虚拟 机</td><td rowspan=\"2\">帧长度 （字节）</td><td colspan=\"4\">虚拟机个数</td><td rowspan=\"2\">Native Linux</td></tr><tr><td>1</td><td>2</td><td>4</td><td>8</td></tr><tr><td rowspan=\"3\">Xen</td><td>64</td><td>1.20%</td><td>6.12%</td><td>7.54%</td><td>8.05%</td><td>26.171%</td></tr><tr><td>512</td><td>14.35%</td><td>22.73%</td><td>29.70%</td><td>35.10%</td><td>100%</td></tr><tr><td>1518</td><td>74.21%</td><td>69.62%</td><td>60.2%</td><td>61.16%</td><td>100%</td></tr><tr><td rowspan=\"3\">Open VZ</td><td>64</td><td>10.48%</td><td>10.39%</td><td>9.53%</td><td>12.66%</td><td>26.171%</td></tr><tr><td>512</td><td>54.50%</td><td>53.84%</td><td>52.50%</td><td>53.00%</td><td>100%</td></tr><tr><td>1518</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td><td>100%</td></tr></table></body></html>",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "从实验结果得出，虚拟机数量的不同对于吞吐量的影响并不大。由此可见，提高吞吐率的途径在于缩短数据包的内核路径，给虚拟机直接访问设备的能力。现有一些方法可以显著",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "提升虚拟I/O的性能，将在下一章主要介绍。",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/8ea3724ed9eaf794720f3423508d12dad9f0f50cfe1b7fc906d88dc30d226391.jpg",
        "img_caption": [
            "图11．Xen多虚拟机转发性能"
        ],
        "img_footnote": [],
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/d9e78d662c0ad6577886f56944ef23b535e6868db4de62bd658b7b0f40350dc6.jpg",
        "img_caption": [
            "图12．OpenVZ多虚拟机转发性能"
        ],
        "img_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4 虚拟I/O加速技术",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4.1I/O性能的加速技术",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "上述测试的实验结果可以看出，全虚拟化技术所带来的开销会对I/O操作的性能带来影响。但是全虚拟化技术可以让操作系统无需修改就可以运行，具有很高的适用性。研究人员对此进行研究，设计出了一些能够显著提升虚拟I/O性能的加速技术。以下分别加以介绍。",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "(1).Virtio(半虚拟化I/O )[20] ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Virtio技术是澳大利亚程序员拉塞尔（R.Russell）设计并实现的，旨在给Linux操作系统中越来越多的虚拟化软件提供一套标准化的半虚拟I/O接口。半虚拟化I/O能够让客户操作系统意识到自身运行在虚拟化环境中，并在这个基础上采用一定的优化措施，提升客户操作系统的IO性能。图13是半虚拟化",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/fab04aa5fbac6ea15047de37481772585e009f6e73a4f4bdb494139448b33d26.jpg",
        "img_caption": [
            "图13．半虚拟化I/O技术原理"
        ],
        "img_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "I/O的原理图。目前，KVM支持该虚拟I/O加速技术。",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "(2).IntelVT-d技术与 SR-IOV技术",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "尽管半虚拟化I/O技术能够提升客户操作系统的IO性能，但仍然存在上下文开销过高等问题。解决问题的关键在于取消中间层的代理,让客户操作系统能够直接访问到物理设备。具体说来，直接访问物理设备可以划分为两个问题：1）对客户操作系统的设备缓冲区内存直接访问（DMA Remapping）。2）中断重映射。Intel VT-d 技术（Intel Virtualization TechnologyforDirectedI/O）解决了以上两个问题。如图14所示：图中左半部表示的是全虚拟化I/O,需要由中间层代理，客户操作系统才能访问底层硬件，而右图中由于DMARemapping 的存在，底层硬件能够直接将数据写到客户操作系统的缓冲区内，使得I/O性能接近原生操作系统的性能。",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "VT-d技术提供了数据由底层硬件到客户操作系统的直接通路，SR-IOV（Single-Root I/OVirtualization，单根读写虚拟化）则提供了将单个设备虚拟成多个设备的方法。SR-IOV为每个虚拟机提供独立内存空间、中断资源。SR-IOV 引入了两个新的功能类型：物理功能（Physical Functions,PFs）和虚拟功能（Virtual Functions，VFs）。",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "虚拟机(0) 虚拟机(n) 虚拟机(0) 虚拟机(n)应用程序 应用程序 应用程序 应用程序 应用程序 应用程序 应用程序 应用程序客户操作系统 客户操作系统 客户操作系统 客户操作系统虚拟设备驱动 虚拟设备驱动 设备A驱动 缓存 设备B驱动 缓存... ↑ ... ·虚拟机监督程序或宿主操作系统★ 虚拟机监督程序或宿主操作系统虚拟设备模拟层= T. ：真实设备驱动A 真实设备驱动B ....... DMA重映射硬件 1↑ ?...... .....? 立 ↓ ↓设备A 设备B 设备A 设备B基于软件的I/O虚拟化样例 直接I/O映射",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "物理功能（PhysicalFunctions，PFs）这是一些支持 SR-IOV扩展功能的PCIe功能，被用于配置和管理SR-IOV功能特性。虚拟功能（VirtualFunctions，VFs）这是一些\"精简\"的PCIe 功能，包括数据迁移必需的资源，以及经过谨慎精简配置的资源集。",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "SR-IOV架构的设计允许一个I/O设备支持多个虚拟功能。从而提供了一种不需要软件模拟就可以共享I/O 设备和I/O 端口的物理功能的方法。图15 是SR-IOV的架构示意图。",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "(3)．PCI Passthrough 技术 ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "image",
        "img_path": "images/fabfad89b64693efaeb1e147f4af9406a3b2fce404f3310ccc3bf995a720d3a7.jpg",
        "img_caption": [
            "图14．对客户操作系统的设备缓冲区内存直接访问",
            "图15．SR-IOV架构示意图"
        ],
        "img_footnote": [],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "PCIPassthrough是Xen开发人员开发的客户虚拟操作系统直接访问设备的技术。主要原理是利用软件模拟IOMMU，使得客户操作系统能越过中间层访问",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "设备。和VT-d 技术相比，PCI Passthrough 的优势在于，前者是一种从硬件上支持IOMMU的技术，尚未广泛普及，而后者则从软件层面支持了IOMMU，可以为不支持VT-d的系统进行I/O 性能优化。",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "(4)．软件路由器的一些设计经验",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "RouteBricks设计经验RouteBricks主要使用三种手段优化了单个路由器的转发性能：第一，使用轮询方式代替传统硬件的中断方式，极大地提高了通用硬件对64字节小包的转发性能；第二，采用批处理的方式，每次总线事务（bus transaction）都传递多个数据包的地址，从而充分利用了PCIe的总线带宽；第三，利用网卡的多队列特性，将不同队列与不同核进行绑定，极大地提升了数据处理的并行性。使用以上三种手段，可以使得单台服务器的64字节小包转发速率达到9.77Gbps。",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "■PacketShader设计经验PacketShader对Linux的内核协议栈进行了优化，将内核中表示数据包的结构体sk_buff 由原来的208字节减少为8字节，采用批处理的方式，极大地提升了数据包处理性能。实验表明，仅采用这种优化手段，可以将数据包吞吐量提升到10.5Gbps，性能提升13.5倍。PacketShader同时也利用了多核多队列的特性，并且考虑到其采用的架构为NUMA(非一致性内存访问)，针对数据包的分发均衡性和提升数据局部性做了精心优化，使其整体转发性能达到40Gbps。",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "4.2虚拟I/O加速技术性能评估 ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "本节对VT-d 技术，半虚拟化I/O 技术，PCI Passthrough 技术所带来的 I/O 性能提升进行评估。使用了RFC2544 标准吞吐率测试方法进行了测试。此外，对半虚拟化I/O 还进行了 IPerfl22带宽测试。由于Xen不支持 Virtio，我们的实验平台上也无法为Xen 打开VT-d 的技术支持，我们最终在 KVM上实验了VT-d 技术和Virtio 的技术，在Xen上测试了PCIPassthrough 技术。",
        "page_idx": 9
    },
    {
        "type": "table",
        "img_path": "images/a6fa530d9351f17377e1109f31fbddc5c4a29ea7ceb2059b0f7f310bc7acb2c8.jpg",
        "table_caption": [
            "表5KVM测试结果"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">帧长度</td><td colspan=\"2\">吞吐率 (帧/秒)</td></tr><tr><td>KVM virtio</td><td>KVMVT-d</td></tr><tr><td>64</td><td>113,452</td><td>841,696</td></tr><tr><td>512</td><td>89,943</td><td>413,750</td></tr><tr><td>1518</td><td>69,970</td><td>162,548</td></tr></table></body></html>",
        "page_idx": 9
    },
    {
        "type": "table",
        "img_path": "images/7acccfd5a455ac69313534f94b35ae7a5c8c42a1b1b9d8975b51af8551285ab2.jpg",
        "table_caption": [
            "表6PCIPassthrough 测试结果"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">帧长度</td><td colspan=\"2\">吞吐率 (帧/秒)</td></tr><tr><td>Xen DomU</td><td>Xen DomU+ PCI passthrough</td></tr><tr><td>64</td><td>113,452</td><td>230,650</td></tr><tr><td>512</td><td>139,506</td><td>222,110</td></tr><tr><td>1518</td><td>124,831</td><td>162,548</td></tr></table></body></html>",
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/379dff1c2d854c37066de943b25a24d706548c557b64b3d1786590a0867d6248.jpg",
        "img_caption": [
            "图16.16KVMI/O 加速"
        ],
        "img_footnote": [],
        "page_idx": 9
    },
    {
        "type": "image",
        "img_path": "images/a851eca0aaa8c107ca5c9869a1f5a26c4e89c2e63b76a690fac37dc2f06e36c2.jpg",
        "img_caption": [
            "图17．PCIPassthrough 软件模拟"
        ],
        "img_footnote": [],
        "page_idx": 9
    },
    {
        "type": "table",
        "img_path": "images/cba793ea6a743b6a8b3eee11d40ae9d6f675ed8d99409335858ca7753fc290c3.jpg",
        "table_caption": [
            "表7KVM半虚拟化I/O的IPerf带宽测试"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\"></td><td colspan=\"2\">KVM</td><td colspan=\"2\">KVM virtio</td></tr><tr><td>单向</td><td>双向</td><td>单向</td><td>双向</td></tr><tr><td>带宽</td><td>210Mbps</td><td>117Mbps</td><td>749Mbps</td><td>55Mbps</td></tr></table></body></html>",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "实验结果表明，半虚拟化I/O 技术对于客户操作系统来说，确实能提高 TCP 传输的带宽，但是对于数据包转发，性能提升则是有限的。VT-d技术则给客户操作系统直接访问设备硬件的能力，能够获得接近原生操作系统的转发性能，将虚拟化所带来的开销降至最低。",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "PCIPassthrough技术由于软件模拟的开销，所提供的性能则在两者之间。",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "5 结束语",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "本文围绕虚拟路由器这一概念分别探讨了几种虚拟化技术、与之相关的I/O 加速技术以及业界广泛认可几种可扩展路由器软件，并在此基础上，测量了几款具有代表意义的虚拟化软件的转发性能，对虚拟路由器的研究和设计具有启发意义。实验结果表明，操作系统级别的虚拟化技术所引入的额外开销最小，然而该项技术由于其虚拟化程度较低，使用起来有一定的局限性。采用VT-d等硬件支持的虚拟化技术能够使运行在全虚拟化环境下的客户操作系统获得原生的I/O效率，同时又保持了虚拟化层次高的优势。虽然现有的虚拟路由器的设计均是采用OpenVZ 等轻量级虚拟化技术，但可以预期，VT-d 和 SR-IOV 技术将会对虚拟路由器的设计带来较大影响。",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "在目前的研究中，并没有使用通用平台搭建的虚拟路由器，大部分已有研究均使用专用硬件来降低虚拟化开销，并采用OpenVZ等轻量级虚拟系统来进一步降低虚拟化开销。然而，由于 SR-IOV 等技术的出现，虚拟化开销已经被降至最低，因此一些软件路由器提高I/O 性能的经验完全可以借鉴，用来设计虚拟路由器。可以预见，未来一到两年内，会出现使用SR-IOV技术的软件虚拟路由器的研究。",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "参考文献：",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "[1] Robert Morris,et al. The Click modular router. SOSP,1999   \n[2] Mark Handley，et al. XORP: An Open Platform for Network Research.ACM SIGCOMM Computer Communication Review, 2003   \n[3] GENI. http://www.geni.net/   \n[4] FIRE. http://cordis.europa.eu/fp7/ict/fire/   \n[5] MiHai Dobrescu, et al. RouteBricks: Exploiting Parallelism To Scale Software Routers. SOSP 2008.Best Paper Awards.   \n[6] Sangin Han, et al. PacketShader: a GPU-Accelerated Software Router. SIGCOMM 2010.   \n[7] Nick McKeown,et al. OpenFlow:Enabling Innovation in College Networks.ACM SIGCOMM Computer Communication Review, 2008   \n[8] Nobert Egi, et al. Towards High Performance Virtual Routers on Commodity Hardware. ACMCoNEXT, 2008.   \n[9] Muhammad Bilal Anwer,et al. Building a Fast，Virtualized Data Plane with Programmable Hardware.VISA 2009.   \n[10] Guohan Lu, et al. CAFE:A Configurable pAcket Forwarding Engine for Data Center Networks.PRESTO'09.   \n[11] Deepak Unnikrishnan, et al. Scalable Network Virtualization Using FPGAs.FPGA'10   \n[12] Muhammad Bilal Anwer, et al. SwitchBlade: A Platform for Rapid Deployment of Network Protocols on Programmable Hardware.SIGCOMM 2010.   \n[13] QEMU. http://wiki.qemu.org/   \n[14] Kernel Virutal Machine.KVM. http://www.linux-kvm.org/ ",
        "page_idx": 10
    }
]