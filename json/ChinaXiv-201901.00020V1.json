[
    {
        "type": "text",
        "text": "基于加权K近邻的改进密度峰值聚类算法",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "杨震，王红军(国防科技大学，合肥230037)",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "摘要：密度峰值聚类算法是一种新颖的密度聚类算法，但是，原算法仅仅考虑了数据的全局结构，在对分布不均匀的数据集进行聚类时效果不理想，并且原算法仅仅依据决策图上各点的分布情况来选取聚类中心，缺乏可靠的选取标准。针对上述问题，提出了一种基于加权K近邻的改进密度峰值聚类算法，将最近邻算法的思想引入密度峰值聚类算法，重新定义并计算了各数据点的局部密度，并通过权值斜率变化趋势来判别聚类中心临界点。通过在人工数据集上与UCI真实数据集上的实验，将该改进算法与原密度峰值聚类算法、K-MEANS 算法及DBSCAN算法进行了对比，证明了改进算法能够在密度不均匀数据集上有效完成聚类，能够发现任意形状簇，且在三个聚类性能指标上普遍高于另外三种算法。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "关键词：数据挖掘；加权K近邻；密度峰值；聚类中图分类号：TP301.6 doi:10.19734/j.issn.1001-3695.2018.08.0656",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Improved density peak clustering algorithm based on weighted K-nearest neighbor ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yang Zhen, Wang Hongjun (National University ofDefense Technology,Hefei23o037,China) ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract: The density peak clustering algorithm wasanew density-based clustering algorithm,the algorithmrequires only oneinput parameter and does not require frequent iterative processes.However,theoriginal algorithmonly considers the global structureof the data,and theefect is notideal whenclustering data sets withuneven distribution.Moreover,the originalalgorithmonlyselects the clustercenter acording tothe distributionof points onthe decision graph,which is not reliable.Aiming atthe above problems，animproved density peak clustering algorithm based on weighted K-nearest neighbor is proposed.The ideaof nearest neighboralgorithm is introduced into the densitypeak clustering algorithm,the localdensityofeachdatapointisredefinedandcalculated,anddeterminethecriticalpointoftheclustercenterbythetrend of the slopeof the weight.The improved algorithm iscomparedwiththe original density peak clusteringalgorithm, K-MEANS algorithmand DBSCANalgorithmby experiments onthe artificialdataset and UCIrealdataset.Itis proved that theimproved algorithmcan deal withthedensityuneven dataset and findclusters of arbitrary shapes.On the three cluster performance indicators,the improved algorithm is generally higher than the other three algorithms. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Key words: data mining; weighted K-nearest neighbor; density peaks; clustering ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "0 引言",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "聚类通常作为一种无监学习方法被用于数据挖掘领域。聚类分析的主要目的是将给定的集群划分为具有共同特征的群组，特征相似的对象被分在一起，而特征差异较大的对象则属于不同的群组。聚类在探索性模式分析，分组决策和机器学习情境中用途广泛，包括文档检索，图像分割和模式分类等[I]。聚类方法按照原理不同一般可分为五类[2]：划分聚类（如K-means $+ + ^ { [ 3 ] }$ ）、层次聚类、密度聚类（如DBSCAN[4.5])、网格聚类以及模型聚类，每种方法都有对应的优点和缺点。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "其中，基于密度的聚类算法假设聚类结构能够通过样本分布的紧密程度确定，其优势在于可发现任意形状的簇。Rodriguez等人提出了一种新颖的密度聚类算法：密度峰值聚类（density peaksclustering,DPC）算法[6]，该算法能够检测非球形簇，并且不需要用户先验指定聚类数量。DPC算法只有截断距离 $d _ { c }$ 这一个输入参数，因此稳定性良好。目前，已经有不少学者将这种方法用于图像处理、模式分类等领域",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "[7-12]。 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "但是，DPC 算法仍然存在一些不足：a)在计算局部密度时，DPC算法没有考虑到局部数据结构；b)DPC算法采用启发式的决策图来选取聚类中心，缺乏可靠的选择标准。例如，当样本集群中具有密度分布不均匀的情况时，DPC算法的聚类结果往往不太理想，而具有不同密度的簇在数据集中是非常常见的。图1(a)\\~(c)为DPC在不同输入参数下的聚类结果，与(d)中本文算法的聚类结果相比，可以看出，DPC算法没有检测出样本数据集中所有簇，它将原本由三个簇组成的样本数据集聚类为两个簇。当遇到类似的密度不均匀数据集时，DPC算法无法给出准确的聚类结果。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "这几年来不断有学者针对DPC算法的不足作出改进，但同时也产生了新的问题。文献[13]基于信息熵理论提出了一种自动确定最佳输入参数的改进算法，但是该算法仍然没有解决对密度不均数据集聚类效果不佳的问题，并且确定参数的时间成本大大增加。文献[14]将DPC算法和Chameleon算法相结合,提出了E_CFSFDP 算法，解决了DPC 算法难以识别低密度簇的问题，但是其模型较为复杂。文献[15]提出了",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "DPC-KNN算法，结合KNN思想重新定义局部密度，一定程度上提升了算法对密度不均匀数据集的聚类效果，但是该算法在聚类中心的选择上与DPC算法一样缺乏明确的标准。",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/70d1adc1377dbb67f11ef79e4b23d7aea405325eb1b3a0b6936473e0339e0d0e.jpg",
        "img_caption": [
            "图1DPC算法与本文提出的ADPC-WKNN算法对样本数据集的聚类结果对比"
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "本文提出了一种基于加权 K 近邻的改进DPC 算法（ADPC-WKNN)，结合加权K近邻的思想重新定义并计算了局部密度，并根据样本点的权值斜率变化趋势找到聚类中心与其余点之间的临界点，解决了DPC算法在选取聚类中心时缺乏明确标准的不足，一定程度上避免了多选或漏选聚类中心所带来的误差。在人工数据集上的实验结果证明了本文算法的可行性。为了评估ADPC-WKNN算法的性能，在UCI数据集上对ADPC-WKNN算法、DPC 算法、DBSCAN算法以及K-MEANS $^ { + + }$ 算法进行了对比实验，结果表明，在绝大多数情况下，ADPC-WKNN算法的聚类性能更好。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 算法简介",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1.1 DPC 算法",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "DPC算法的基本思想如下：聚类中心的特征在于其密度高于其周边样本，并且与具有较高密度的样本的距离相对较大。该算法使用了两个重要的量，一个是各样本点的局部密度 $\\rho _ { i }$ ，另一个是各样本点与具有更高局部密度样本点的最小距离 $\\delta _ { i \\circ }$ 这两个量分别对应于该算法基本思想中的两个假设，即聚类中心的局部密度高于周围点的局部密度，并且与具有较高密度点的距离相对较大。接下来将详细介绍这两个量的计算方法。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "假设存在一个数据集 $S = \\left\\{ x _ { i } \\right\\} _ { i = 1 } ^ { N }$ ， $N$ 为样本点个数。首先要计算出各样本点之间的距离矩阵， $d ( x _ { i } , x _ { j } )$ 表示样本点 $x _ { i }$ 到样本点 $x _ { j }$ 之间的欧氏距离。样本点 $x _ { i }$ 的局部密度表示为 $\\rho _ { i }$ 其计算公式如下：",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\rho _ { i } = \\sum _ { j } \\chi ( d _ { i j } - d _ { c } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\textstyle \\chi ( x ) = { \\left\\{ \\begin{array} { l l } { 1 , x < 0 } \\\\ { 0 , x \\geq 0 } \\end{array} \\right. }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "其中 $: d _ { c }$ 代表截断距离， $\\rho _ { i }$ 表示以点 $x _ { i }$ 为圆心、 $d _ { c }$ 为半径的圆中包含的所有其余样本点的数量。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "但是，对于数据量较小的数据集，式(1)有时会导致某些样本点具有同样的局部密度，从而影响聚类结果的准确性。因此，Rodriguez和Laio 还提供了另一种局部密度计算方法，采用高斯核函数来定义 $\\rho _ { i }$ ，前者可称为硬阈值，后者可称为软阈值。如下所示：",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\rho _ { i } = \\sum _ { j } e ^ { - \\left( \\frac { d _ { i j } } { d _ { c } } \\right) ^ { 2 } }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "$d _ { c }$ 是DPC算法中唯一的输入参数，选择 $d _ { c }$ 的过程实际上是选择数据集中所有点的平均邻居点数量的过程。假设有数据集 $s$ 由 $N$ 个样本点组成，在求出数据集中所有样本点两两之间的距离之后，将距离值按从小到大的顺序排列，得到由 $N ^ { 2 } / 2$ 个距离值组成的向量， $d _ { c }$ 通常由距离总个数与用户输入的百分比 $p$ 的乘积所对应的向量中某个距离值表示。因此，实际上DPC 算法唯一由用户输入的参数为百分比 $p$ 。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "另一个量 $\\delta _ { i }$ 的计算则非常简单，它表示点 $x _ { i }$ 与具有更高局部密度的任何其他样本点之间的最小距离，其定义如下：",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\delta _ { i } = \\operatorname* { m i n } _ { j : \\rho _ { j } > \\rho _ { i } } ( d _ { i j } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "特别地，当样本点 $i$ 的局部密度为所有样本点中最高时，其 $\\delta _ { i }$ 的计算公式如下：",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\delta _ { i } = \\operatorname* { m a x } _ { j } ( d _ { i j } )\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "只有同时具有较高 $\\rho _ { i }$ 和 $\\delta _ { i }$ 的样本点才被考虑为聚类中心，并且，Rodriguez和Laio引入了决策图来帮助确定聚类中心，如图2所示，(b)中决策图的横轴表示 $\\rho _ { i }$ ，纵轴表示 $\\delta _ { i }$ 决策图可以直观地反映出各样本点这两个量的分布情况。",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/5f7a6f86c05b98a49fd1a1eed2ab64f991ad73fe45c1c0251d05741e8434361c.jpg",
        "img_caption": [
            "Fig.1 Comparison of clustering results between DPC algorithm and ADPC-WKNN algorithm on sample dataset ",
            "图2数据集分布与决策图示例",
            "Fig.2Data set distribution and decision diagram DPC 算法的具体流程如下所示： Algorithm DPC "
        ],
        "img_footnote": [],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1:Input:样本点数据集 $\\boldsymbol { S } = \\left\\{ \\boldsymbol { x } _ { i } \\right\\} _ { i = 1 } ^ { N }$ ，百分比 $p$ 6",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2:Output:聚类索引的标量 $y$ 。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3：计算数据集样本点两两之间的距离，得到按距离值升序排列的向量；",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "4:根据式(1)或(3)计算各样本点的 $\\rho _ { i }$ 5:根据式(4)计算各样本点的 $\\delta _ { i }$ 6:根据计算得到的 $\\rho _ { i }$ 与 $\\delta _ { i }$ 绘制决策图，并选取聚类中心；7：将其余样本点按局部密度大小分配至距离最近的更高密度点所在簇；8:得到聚类索引的标量 $y _ { \\ast }$ ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1.2K近邻算法",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "K 近邻算法又称为KNN 算法，已经被广泛用于分类、回归、密度估计及模式识别等领域[16]。顾名思义，这种算法的目的就是在所有样本中找到距离目标样本最近的K个邻居，样本之间的距离通常由欧氏距离表示。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "KNN算法的原理比较简单，仍然假设存在数据集$\\boldsymbol { S } = \\left\\{ \\boldsymbol { x } _ { i } \\right\\} _ { i = 1 } ^ { N }$ ， $N$ 为样本个数。计算出目标样本 $x _ { i }$ 与剩下 $N { - } 1$ 个样本的距离后，将距离值按升序排列，前K个距离值所对应的样本即距离目标样本 $x _ { i }$ 最近的K个邻居，表示为 $\\mathsf { K N N } ( x _ { i } )$ 。如图3所示，KNN算法用于分类时，其基本思路为：如果一个样本在特征空间中的K个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，并且，其所选择的邻居都是已经正确分类的对象。",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/4ec917a2a1e843f5749ec9b954fed89452c34907c25b4df87a303de03c7d2a1f.jpg",
        "img_caption": [
            "图3KNN算法分类原理",
            "Fig.3Classification principle of KNN algorithm "
        ],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2 ADPC-WKNN 算法",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "为了提高DPC算法应用在密度不均匀数据集时的表现，本文基于加权K近邻算法的基本理念，重新定义了局部密度$\\rho _ { i }$ 的计算方法。并且，为了克服DPC算法在选取聚类中心时缺乏标准的不足，本文根据样本点权值趋势给出了判别聚类中心与剩余点的明确标准，实现了聚类中心的自动选择。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "DPC算法所定义的局部密度存在对数据的局部结构不敏感的缺点，特别是数据集中不同簇的密度存在很大差异时，局部密度的变化会导致选取聚类中心时存在很大的差异。本文将加权KNN的概念引入到局部密度的计算中，采用反函数与高斯核函数乘积和的加权形式来表示新的局部密度。$\\mathrm { N N } _ { k } ( x _ { i } )$ 表示所有样本点中与点 $x _ { i }$ 的距离排名(由小到大)为K的点， $\\mathsf { K N N } ( x _ { i } )$ 定义为",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nK N N ( x _ { i } ) = \\{ j \\in S \\mid d ( x _ { i } , x _ { j } ) \\leq d ( x _ { i } , N N _ { k } ( x _ { i } ) ) \\}\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "新的局部密度表示如下：",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\rho _ { i } = \\sum _ { x _ { j } \\in K N N ( x _ { i } ) } \\frac { 1 } { c + d ( x _ { i } , x _ { j } ) } \\ast \\exp ( d ( x _ { i } , x _ { j } ) ^ { \\frac { 3 } { 2 } } )\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "式(7)中， $\\boldsymbol { c }$ 表示数据集中所有样本点两两之间的距离和，唯一的参数为 $\\mathrm { ~ \\bf ~ K ~ }$ ，其确定方法与截断距离 $d _ { c }$ 类似，由用户指定一个百分比 $p$ ， $\\scriptstyle \\mathbf { K } = \\boldsymbol { p } \\times \\boldsymbol { N }$ ， $N$ 为数据集中所有样本点的数量。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "图4为DPC 算法、DPC-KNN 算法以及本文提出的ADPC-WKNN算法对R15数据集聚类时的决策图，从图中可以看出，(a)中 DPC 算法的决策图里聚类中心分布散乱，其中局部密度最低的聚类中心位于横轴中轴附近，其局部密度大小位于所有样本点里的中游水准，作为聚类中心的特性并不明显。(b)(c)中决策图的聚类中心更为紧凑，(c)中聚类中心的局部密度均接近于所有样本点局部密度的最大值，说明ADPC-WKNN算法新定义的局部密度公式能更加突出聚类中心的特性。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/a5afcd2be2efda39607d824fe3777b775e3e927af531e37ba768b021aaf2e8ab.jpg",
        "img_caption": [
            "图4R15数据集聚类决策图",
            "Fig.4Clustering decision diagram ofR15 dataset "
        ],
        "img_footnote": [],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "由于DPC算法在选取聚类中心需要进行人工决策，使得聚类过程带有一定的主观性和随机性，难以从量化的角度确定聚类中心，不利于算法的应用。鉴于此，本文通过分析 $\\rho _ { i }$ 和 $\\delta _ { i }$ 的统计特性，提出了一种基于二者归一化乘积 $\\gamma _ { i }$ 的聚类中心临界点判别法，从而实现自动选择聚类中心的目的。该判别法的基本思想：根据DPC 选取聚类中心的原则，通过 $\\gamma _ { i }$ 的大小差异评测样本点的特征，根据斜率变化趋势判别出聚类中心临界点，从而将临界点之前的样本点自动确定为聚类中心，将临界点及其之后的样本点根据分配原则完成聚类。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "以文献[6]中使用的GDP数据集为例，选择 $p { = } 1 . 5 \\%$ ，对$\\rho _ { i }$ 和 $\\delta _ { i }$ 进行归一化处理后求出各样本点的 $\\gamma _ { i }$ ，将样本点权值按降序排列并取前40个点，如图5所示， $\\gamma _ { i }$ 越大的样本点越有可能是聚类中心，样本点的权值呈先快速下降再稳定的趋势，但是下降的程度不同。因此，其中相对于初始点斜率变化趋势最大的样本点可被看做聚类中心的临界点，定义这个斜率变化趋势为tendi:",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nt e n d _ { i } = ( i - 1 ) \\frac { \\gamma _ { i + 1 } - \\gamma _ { i } } { \\gamma _ { i } - \\gamma _ { 1 } }\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "从式(8)可以看出，斜率变化趋势即为样本点 $i$ 到 $i { + } 1$ 斜率与样本点 $i$ 到初始点斜率的商，临界点被定义为为拥有最大tendi的样本点。如图6所示，第五个样本点拥有最大的斜率变化趋势，被判定为临界点，则将图5排序图中的前五个样本点选作聚类中心，完成聚类。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "图7展示了选择不同个数聚类中心的聚类结果，可以看出，当聚类中心为五个时聚类效果最好，这也与文献[6]的聚类结果一致。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "ADPC-WKNN算法的具体流程如下：",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "AlgorithmADPC-WKNN ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1:Input:样本点数据集S={𝑥}，百分比p。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2:Output:聚类索引的标量 $y$ 0 ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3：计算数据集样本点两两之间的距离，得到按距离值升序排列的向",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "量；",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4:根据公式(6)计算各样本点的 $\\rho _ { i }$ ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "5:根据公式(4)计算各样本点的 $\\delta _ { i }$ ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "6:根据计算得到的 $\\rho _ { i }$ 与 $\\delta _ { i }$ 算出各样本点的 $\\gamma _ { i }$ ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "7：将各样本点的 $\\gamma _ { i }$ 降序排列，并计算斜率变化趋势tendi，找到最大值对应的临界点，将临界点之前的样本点选作聚类中心；",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "8:将其余样本点按局部密度大小分配至距离最近的更高密度点所在簇；",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "9:得到聚类索引的标量 $y _ { s }$ ",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/c208b1792c688947b42472f3e6307988a98b8917eb194d93ed89c87b9367a1a0.jpg",
        "img_caption": [
            "图5样本点权值排序图"
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/0b0708273a87cb81dbc2453b869ff3a4f242841dd71c33407e4cd416ed5c8827.jpg",
        "img_caption": [
            "Fig.5Sample point weight sorting graph "
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/f17d7238a7a8cda9308d72b4094dbeb164b0ffe3e43f3001f0e3c0bac7f2a290.jpg",
        "img_caption": [
            "Fig.6 Critical point discrimination diagram ",
            "图7选择不同个数聚类中心的聚类结果",
            "Fig.7Clustering results for different numbers of cluster centers "
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3 仿真实验 ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "为了测试ADPC-WKNN算法的聚类性能，本文采用5个经典人工数据集以及5个UCI上的真实数据集进行实验，",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "对比算法为划分聚类经典算法K-means $^ { ; + + }$ ，密度聚类经典算法DBSCAN以及DPC算法。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3.1实验环境与数据集",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "本文的实验环境为Windows1064位操作系统，IntelCorei7-6700HQ $\\textcircled { a } 2 . 6 0$ GHzCPU,8GB内存，采用MATLAB2014a进行实验。人工数据集及UCI真实数据集属性分别如表1、2所示。",
        "page_idx": 3
    },
    {
        "type": "table",
        "img_path": "images/48bc75ff0fbc43715cf9fa9737b92882f9debf8ee4dfd75fa5736d0c3f28b175.jpg",
        "table_caption": [
            "Table1 Artificial dataset ",
            "表2UCI数据集"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>数据集</td><td>样本数</td><td>维数</td><td>类别数</td><td>来源</td></tr><tr><td>Spiral</td><td>312</td><td>2</td><td>3</td><td>[17]</td></tr><tr><td>R15</td><td>600</td><td>2</td><td>15</td><td>[18]</td></tr><tr><td>Flame</td><td>240</td><td>2</td><td>2</td><td>[19]</td></tr><tr><td>Jain</td><td>373</td><td>2</td><td>2</td><td>[20]</td></tr><tr><td>Aggregation</td><td>788</td><td>2</td><td>7</td><td>[21]</td></tr></table></body></html>",
        "page_idx": 3
    },
    {
        "type": "table",
        "img_path": "images/972675e8b44dc750b58d6e9b0637f52e9c5296eea55a981d402c16848d5e2376.jpg",
        "table_caption": [
            "表1人工数据集",
            "Table 2 UCI dataset "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>数据集</td><td>样本数</td><td>维数</td><td>类别数</td><td>来源</td></tr><tr><td>Iris</td><td>150</td><td>4</td><td>3</td><td>[22]</td></tr><tr><td>Seeds</td><td>210</td><td>7</td><td>3</td><td>[22]</td></tr><tr><td>Zoo</td><td>101</td><td>18</td><td>7</td><td>[22]</td></tr><tr><td>Waveform</td><td>5000</td><td>21</td><td>3</td><td>[22]</td></tr><tr><td>Wine</td><td>178</td><td>13</td><td>3</td><td>[22]</td></tr></table></body></html>",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3.2二维人工数据集聚类结果图及分析",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "本文实验中使用的人工数据集各样本点分布情况如图 8所示。",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/bb01a546a462c7770399195a31930f941446f568e44e28b399743ec5d0369ad7.jpg",
        "img_caption": [
            "图6临界点判别图",
            "图8人工数据集样本点分布图例",
            "Fig.8Artificial dataset sample point distribution "
        ],
        "img_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "采用本文提出的ADPC-WKNN算法对这5个数据集进行聚类，其结果如图9所示。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "从图中可以看出，ADPC-WKNN在5个人工数据集上均取得了良好的聚类效果，但是输入参数 $p$ 的跨度较大，尤其",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "是Spiral数据集和Jain数据集，在样本数十分接近的情况下，输入参数分别为 $2 . 2 \\%$ 和 $0 . 2 \\%$ 。因此，如何辅助用户决策出最佳输入参数将成为下一步研究的重点。",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/f070ca2ddcd81ce15c525555357931d8871b243e2bf4748e231a76bb0ba212c0.jpg",
        "img_caption": [
            "图9ADPC-WKNN 算法聚类结果图 Fig.9Clustering result graph of ADPC-WKNN algorithm "
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "3.3算法聚类性能评估分析 ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "本文采用准确率(accuracy)、召回率(recall)以及归一化互信息(normalizedmutualinformation，NMI)来评估各算法的聚类性能，其中准确率和召回率是广泛用于信息检索和统计学领域的两个度量指标，通常用来评价聚类结果的好坏，归一化互信息则是变量之间相互依赖性的度量。三种评价指标的范围均在0到1之间，且值与聚类性能好坏成正相关。下面简要介绍这三个评价指标的计算公式。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "假设 $P _ { j }$ 为已知的人工标注过的簇， $C _ { j }$ 为经过聚类后的簇，各指标计算公式如下：",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\nA c c u r a c y ( P _ { j } , C _ { i } ) { = } \\frac { \\mid P _ { j } \\cap C _ { i } \\mid } { \\mid C _ { i } \\mid }\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\n\\mathsf { R e c a l l } ( P _ { j } , C _ { i } ) = \\frac { \\mid P _ { j } \\cap C _ { i } \\mid } { \\mid P _ { j } \\mid }\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\nN M I ( P _ { j } , C _ { i } ) = \\frac { I ( P _ { j } , C _ { i } ) } { \\sqrt { H ( P _ { j } ) H ( C _ { i } ) } }\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "实验结果如图10\\~15、表3\\~5所示，为了更直观地展示各算法在性能指标上的差异，图中纵轴代表的数据均不是从0开始。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "具体的实验结果如表3\\~5所示，表格中加粗的数字代表在此数据集上最好的结果。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "从图10\\~15与表3\\~5可以看出，本文提出的ADPC-WKNN算法在三个聚类性能评估指标上普遍优于DPC 算法、DBSCAN算法与 ${ \\mathrm { K } } { \\mathrm { - M E A N S + + } }$ 算法，对于前五个人工数据集，ADPC-WKNN算法的三项性能指标均为最高，说明本文提出的ADPC-WKNN算法在二维数据集上性能优越。对于UCI数据集，Iris数据集和Seeds数据集等维数不是特别高的数据集，本文算法在三项性能指标上仍然是最高的，但是在Zoo、Waveform 和Wine等高维数据集上，ADPC-WKNN算法的聚类效果并不突出，在Waveform数据集上，本文只有准确率与召回率两项指标是最高的，而在Zoo及Wine数据集上，本文算法的三项性能指标均不是最高的。由此可见，本文提出的ADPC-WKNN算法在中低维数据集上的聚类效果良好，其处理高维数据集的能力还有所欠缺。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/39d22754db975d878c1d4ba25390c77a63e8784f907bf59a4eda81114779243d.jpg",
        "img_caption": [
            "图10人工数据集上各算法准确率对比"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/e22327c3b06cf43dc43558b9d2ef3e83fb09bd3f1cf4010933709664ceca74c9.jpg",
        "img_caption": [
            "Fig.1OAccuracy comparison of algorithms on artificial data sets "
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/62f1ff585e6d4c8f17f6a83ae352cc1597a7c5523e23ba4aabe2b2510008e4a2.jpg",
        "img_caption": [
            "Fig.11Recall comparison of algorithms on artificial data sets ",
            "图12人工数据集上各算法归一化互信息对比"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/d89d9974b68020f81698c665e7c82c1207f478eea7f203394d47d721601f049f.jpg",
        "img_caption": [
            "图11人工数据集上各算法召回率对比",
            "Fig.12Normalized mutual information comparison of algorithms on ",
            "图13UCI数据集上各算法准确率对比",
            "Fig.13Accuracy comparison of algorithms on UCI data sets "
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/c2ba1c797ba01b6fa4ec6409d136c10be0fac5bcfbacb0f88100bb9d404cf1f2.jpg",
        "img_caption": [
            "图14UCI数据集上各算法召回率对比",
            "Fig.14Recall comparison of algorithms on UCI data sets ",
            "图15UCI数据集上各算法归一化互信息对比 Fig.15Normalized mutual information comparison of algorithms on UCI data sets "
        ],
        "img_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "■ADPC-WKNN ■DPC ■DBSCAN ■K-MEANS++10090  \n(%) 8070 m60  \n归 504030Iris Seeds Zoo Waveform Wine数据集名称",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/e55808a8aac2ec9711f5d26a5e1e34975c88f4cb3e88b2fc76f798fdbac8af9a.jpg",
        "table_caption": [
            "表3各算法准确率",
            "Table 3Algorithm accuracy "
        ],
        "table_footnote": [
            "表4各算法召回率"
        ],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">数据集</td><td colspan=\"4\">Accuracy(%)</td></tr><tr><td>ADPC-WKNN</td><td>DPC</td><td>DBSCAN</td><td>K-MEANS++</td></tr><tr><td>Spiral</td><td>100</td><td>100</td><td>98.3</td><td>87.6</td></tr><tr><td>R15</td><td>100</td><td>99.3</td><td>96.4</td><td>74.5</td></tr><tr><td>Flame</td><td>100</td><td>100</td><td>96.7</td><td>85.6</td></tr><tr><td>Jain</td><td>100</td><td>100</td><td>92.8</td><td>79.1</td></tr><tr><td>Aggregation</td><td>100</td><td>98.8</td><td>97.6</td><td>86.2</td></tr><tr><td>Iris</td><td>94.2</td><td>90.6</td><td>67.8</td><td>89.5</td></tr><tr><td>Seeds</td><td>91.7</td><td>90.4</td><td>71.4</td><td>89.1</td></tr><tr><td>Zoo</td><td>78.5</td><td>69.3</td><td>70.1</td><td>82.9</td></tr><tr><td>Waveform</td><td>63.1</td><td>61.8</td><td>45.7</td><td>54.3</td></tr><tr><td>Wine</td><td>91.7</td><td>88.6</td><td>57.3</td><td>95.8</td></tr></table></body></html>",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/da5c820b55c9ec7ce122f26d7526368ea9e987302dcb2fcb6da1bda81557daf4.jpg",
        "table_caption": [
            "Table 4Algorithm recall ",
            "表5各算法归一化互信息"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">数据集</td><td colspan=\"4\">Recall(%)</td></tr><tr><td>ADPC-WKNN</td><td>DPC</td><td>DBSCAN</td><td>K-MEANS++</td></tr><tr><td>Spiral</td><td>100</td><td>100</td><td>99.5</td><td>99.4</td></tr><tr><td>R15</td><td>100</td><td>99.5</td><td>98.4</td><td>87.2</td></tr><tr><td>Flame</td><td>100</td><td>100</td><td>97.3</td><td>90.1</td></tr><tr><td>Jain</td><td>100</td><td>100</td><td>93.2</td><td>83.5</td></tr><tr><td>Aggregation</td><td>100</td><td>99.6</td><td>99.3</td><td>92.7</td></tr><tr><td>Iris</td><td>94.6</td><td>90.3</td><td>86.5</td><td>90.7</td></tr><tr><td>Seeds</td><td>91.7</td><td>90.4</td><td>71.4</td><td>89.1</td></tr><tr><td>Zoo</td><td>79.3</td><td>73.8</td><td>75.2</td><td>87.6</td></tr><tr><td>Waveform</td><td>65.3</td><td>63.7</td><td>56.4</td><td>59.8</td></tr><tr><td>Wine</td><td>93.2</td><td>90.1</td><td>62.3</td><td>96.2</td></tr></table></body></html>",
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/816a95bc9458b18502afe7c199d0ff22e7442df68f505f95defa91136d06bb90.jpg",
        "table_caption": [
            "Table 3Algorithm normalized mutual information "
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td rowspan=\"2\">数据集</td><td colspan=\"4\">NMI(%)</td></tr><tr><td>ADPC-WKNN</td><td>DPC</td><td>DBSCAN</td><td>K-MEANS++</td></tr><tr><td>Spiral</td><td>100</td><td>100</td><td>97.8</td><td>83.4</td></tr><tr><td>R15</td><td>100</td><td>98.9</td><td>95.2</td><td>61.3</td></tr><tr><td>Flame</td><td>100</td><td>100</td><td>95.4</td><td>79.6</td></tr><tr><td>Jain</td><td>100</td><td>100</td><td>90.3</td><td>83.4</td></tr><tr><td>Aggregation</td><td>100</td><td>99.1</td><td>96.8</td><td>83.9</td></tr><tr><td>Iris</td><td>78.8</td><td>75.3</td><td>60.7</td><td>75.8</td></tr><tr><td>Seeds</td><td>73.9</td><td>70.3</td><td>59.9</td><td>67.8</td></tr><tr><td>Z00</td><td>63.5</td><td>59.3</td><td>54.7</td><td>70.4</td></tr><tr><td>Waveform</td><td>54.3</td><td>56.1</td><td>38.2</td><td>48.3</td></tr><tr><td>Wine</td><td>76.1</td><td>71.4</td><td>49.5</td><td>85.7</td></tr></table></body></html>",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "3.4算法复杂度分析 ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "设 $N$ 为数据集点个数，DPC算法的时间复杂度为 $O ( N ^ { 2 } )$ 其复杂度主要来源于计算 $N$ 个数据点两两之间的距离，相比于DPC算法，本文提出的ADPC-WKNN算法由于加入了聚类中心的自动选择，需要多计算一个量 $\\gamma _ { i }$ ，故ADPC-WKNN算法的时间复杂度为 ${ \\cal O } ( N ^ { 2 } ) + { \\cal O } ( N ) { \\sim } { \\cal O } ( N ^ { 2 } ) .$ 。DBSCAN算法与${ \\mathrm { K } } { \\mathrm { - } } { \\mathrm { M E A N S + + } }$ 算法的时间复杂度分别为 $O ( N ^ { 2 } )$ 与 $O ( N )$ ，这是因为K-MEANS $^ { + + }$ 算法作为划分聚类算法，无须考虑数据点两两之间的距离，只需考虑各数据点与指定的数个聚类中心的距离。但是， ${ \\mathrm { K } } { \\mathrm { - M E A N S + + } }$ 算法需要人为指定聚类中心的个数，且对于形状不规则的数据集聚类效果不好。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4 结束语",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "DPC算法在对密度分布不均匀的数据集聚类时效果不理想，且选取聚类中心时缺乏明确的标准。本文针对这两个问题提出了一种基于加权K近邻的改进密度峰值聚类算法：ADPC-WKNN，该算法结合加权K近邻的思想，采用反函数与高斯核函数的乘积形式重新定义了局部密度，并且基于权值斜率变化趋势来确定聚类中心临界点，有效解决了原算法存在的问题。在人工数据集及UCI真实数据集上的实验结果表明，ADPC-WKNN算法能准确识别出二维数据集的所有簇，且在聚类性能指标上也普遍高于原算法以及经典密度聚类算法DBSCAN与经典划分算法 ${ \\mathrm { K M E A N S } } + +$ 。在今后的研究过程中，如何确定算法的最佳输入参数与提高算法在高维数据集上的聚类性能将是下一步的研究重点。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "参考文献：",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "[1]Jain AK,Murty MN,FlynnPJ.Data clustering:a review[J].Acm Computing Surveys,1999,31(3): 264-323.   \n[2]Han Jiawei.Data mining:concepts and techniques [M].San Francisco: Morgan Kaufmann Publishers Inc.,2Oo5:65-67.   \n[3]Arthur D,Vassilvitskii S.K-means $^ +$ ：the advantages of careful seeding [C]//Proc of the 18th ACM-SIAM Symposium on Discrete Algorithms, NewOrleans,Louisiana.Society for Industrial and Applied Mathematics.New York:ACMPress,2007:1027-1035.   \n[4]Ester M,Kriegel H P,Xu Xiaowei.A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters inlarge spatial databases with noise [C]//Proc of International Conference on Knowledge Discovery and Data Mining.Pola Alto:AAAI Press,1996:226-231.   \n[5]Hou Jian,Gao Huijun,Li Xuelong.DSets-DBSCAN:a parameter-free clustering algorithm [J]. IEEE Trans on Image Processing,2016,25(7): 3182-3193.   \n[6]Rodriguez A,Laio A.Machine learning:clustering by fast search and find of density peaks.[J]. Science,2014,344 (6191):1492.   \n[7]Sun Kang,Geng Xiurui, Ji Luyan. Exemplar component analysis:a fast band selection method for hyperspectral imagery [J].IEEE Geoscience & Remote Sensing Letters,2015,12(5): 998-1002.   \n[8]Wang Baoyan,Zhang Jian,Liu Yi,et al.Clustering sentences with densitypeaksformulti-documentsummarization[C]//Procof Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2015: 1262-1267.   \n[9]Xie Ke,Wu Jin,Yang Wankou,et al.K-Means Clustering Based on Density for Scene Image Classification[C]//Proc of Chinese Intelligent Automation Conference.2015:379-386.   \n[10] Chen Yewang,Lai Dehe,Han Qi,et al.A new method to estimate ages offacial image for large database [J].Multimedia Tools & Applications, 2016,75(5): 2877-2895.   \n[11] Chen Guijun,Zhang Xueying,Wang Zizhong，et al.Robust support vector data description for outlier detection with noise or uncertain data [J].Knowledge-Based Systems,2015,90(C):129-137.   \n[12] Shamshirband S,Amini A,AnuarNB,et al.D-FICCA:Adensity-based fuzzy imperialist competitive clustering algorithm for intrusion detection in wireless sensor networks [J].Measurement,2O14,55(9): 212-226.   \n[13]Wang Shuliang，Wang Dakui,Li Caoyuan,et al.Clustering by fast search and find of density peaks with data field [J].Computer Science, 2016,25(3):397-402.   \n[14] Zhang Wenkai,Li Jing.Extended fast search clustering algorithm: widely density clusters,no density peaks [J].Computer Science,2015, 5(7):415-423.   \n[15]Du Mingjing，Ding Shifei，Jia Hongjie.Study on density peaks clustering based on K-nearest neighbors and principal component analysis [J].Knowledge-Based Systems,2016,99:135-145.   \n[16] Chen Huiling，Yang Bo,Wang Gang，et al.A novel bankruptcy prediction model based on an adaptive fuzzy-nearest neighbor method [J].Knowledge-Based Systems,2011,24:1348-1359.   \n[17] Chang Hong,Yeung D Y.Robust path-based spectral clustering [M]. Elsevier Science Inc.2008:17-23.   \n[18] Veenman CJ,Reinders MJT,BackerE.A maximum variance cluster algorithm[J].Pattern Analysis& Machine Intelligence IEEE Trans on, 2002,24(9):1273-1280.   \n[19] Fu Limin,Medico E.FLAME,a novel fuzzy clustering method for the analysis of DNA microarray data[J].BMC Bioinformatics,2Oo7,8(1): 3.   \n[20] Sarat C,Zhu, Yongfang,Jain AK,et al. Statistical models for assessing theindividuality of fingerprints [J].IEEE Trans on Information Forensics & Security,2007,2(3):391-401.   \n[21] Gionis A,Mannila H,Tsaparas P. Clustering aggregation [J].ACM Trans on Knowledge Discovery from Data,2007,1(1): 4.   \n[22] Bache K,Lichman M.UCI machine learning repository [EB/OL]. (2017-11-3O).http://archive.ics.uci.edu/ml. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 6
    }
]