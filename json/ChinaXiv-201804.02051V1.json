[
    {
        "type": "text",
        "text": "Storm下基于最佳并行度的贪心调度算法",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "熊安萍，段杭彪，蒋亚雄(重庆邮电大学 计算机科学与技术学院，重庆 400065)",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "摘要：开源分布式实时计算框架 Storm在互联网、金融、电子商务等领域得到了广泛应用。Storm默认采用轮询的调度策略，且依赖用户对 Topology任务的并行度配置，当配置不合理时依然会造成 Topology 处理时延增大、吞吐量降低等问题。针对该问题，提出了一种 Storm下基于最佳并行度的贪心调度算法，调度时先求解 Topology任务中各组件的最佳并行度，再采用贪心策略进行调度，以最小化节点间的网络通信开销。通过与默认调度算法、线上调度算法和热边调度算法进行实验比较，结果表明算法能够有效降低 Storm处理时延，提高系统吞吐量和资源利用率。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "关键词：实时计算；Storm；最佳并行度；贪心策略；调度算法中图分类号：TP311 doi: 10.3969/j.issn.1001-3695.2017.11.0788",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Greedy scheduling algorithm based on best parallelism in Storm ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Xiong Anping, Duan Hangbiao, Jiang Yaxiong (CollgeofComputerScience&Technology,ChongqingUniversityofPosts&elecommunications,Chongqing4o,Cina) ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract: Open-sourcedistributedreal-timecomputing framework Storminthe Internet,finance,e-commerceandotherfields has ben widelyused.Bydefault,Stormuses the poling sheduling policyand relies onthe user's configuration ofTopology tasks in paralel.When theconfiguration isunreasonable，Storm stillcausesdelays inTopologyprocesinganddecreases throughput.Tosolve this problem,this paperproposesa GreedyScheduling AlgorithmbasedonbestparalelisminStorm.When schedulig,thebestparalelismofeachcomponentinTopologytaskissolvedfirst,andthengreedypolicyisadoptedtomiimize thenetwork communication between nodes.Compared withthe default scheduling algorithm,theonlinescheduling algorithm andthe hot-sideschedulingalgorithm,theresults showthatealgorithmcaneffctivelyreduce the Stormprocesingdelayand improve the system throughput and resource utilization. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Key words: real-time computing; storm; best parallelism; greedy policy; scheduling algorithm ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "0 引言",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "大数据背景下，数据内涵和价值时效性越来越重要，数据的流式特征也越来越显著，而流式计算的重要性也越来越突出[1]。S4、Spark、Storm 等流式计算框架的推出为流式数据实时处理提供了有效途径。Storm 是个实时的、分布式的以及具备高容错的分布式实时系统[2]，以其实时性、高效性[3]4]，被广泛应用于国内外如百度、阿里巴巴、Twitter等互联网企业。但Storm在实际部署应用中[5]，存在诸如Nimbus单节点、Tasks共享Worker相互干扰、反压机制、集群节点间的网络通信开销大等问题。其中，高效的调度策略可以有效缓解延迟、吞吐量、负载均衡等问题，成为提升系统效率的关键。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "现有调度算法如线上调度算法、热边调度算法，关注了Storm中默认调度策略中集群节点间网络通信开销的问题，但在调度中都依赖于用户配置的Topology任务，当配置不当时会严重影响Topology 的执行效率，造成Tuple 的处理时延增加，吞吐量降低和资源的浪费。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "本文的主要贡献如下:a)利用Topology 中各组件间关联性，在满足后一组件恰好能完全处理完前一组件所生成的数据下，求解到Topology中各组件的最佳并行度，来优化用户初始对Topology并行度配置不佳的问题;b）基于Topology各组件最佳并行度，采用贪心策略在保证节点CPU不超载的情况下进行调度，将通信频率高的Executor尽可能的分配到同一节点，从而来最小化节点间的网络通信开销，并提高节点的资源利用率。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1 相关工作",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Storm中用户提交的Topology任务是一个有向无环图，图中节点表示计算任务又称做Component（组件）；边描述节点间的数据流向[6]。Component 之间交换的数据单元称做 Tuple。Storm中有两种类型的组件：",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "a)Spout：数据流的源头，读取待处理数据，然后持续不断地发射Tuple 到 Topology 中。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "b)Bolt:接收Spout或其他Bolt发送的Tuple，并按照用户定义好的方式来处理Tuple，比如将数据持久化到磁盘，执行过滤、聚集等函数操作。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Component的运行时实例，称做Task，数量固定且由用户在 Topology 中配置。在 Storm 集群中，每个 Supervisor 可以启动多个Worker进程，Worker进程运行着Topology的多个线程，称 Topology的线程为Executor。Executor用于运行组件的实例Task，不同组件的 Task 只能运行于不同的 Executor 中[7]，其中运行该组件Task的Executor线程数目，称其为该组件的并行度。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "在Storm调度算法方面，业界已有许多相关研究：Aniello等人[8提出了一种将相互通信频率高的Executor调度到同一个slot上，从而减少网络通信的调度算法。 $\\mathrm { \\Delta X u }$ 等人[9]提出一种基于负载均衡的调度算法，以减少网络通信开销。将Executor按照trafficload降序排列，然后按序将Executor分配到负载最轻的slot上，并保证每个Worker节点的任务量不会过载。Peng等人[10]中提出一种在最大化资源利用率下减少网络通信的调度算法。通过将Task映射到Worker节点，使所有的资源请求都能够得到满足并且各节点不会出现资源过载。Xiong等人[]提出一种基于热边的调度算法，将热边及关联的热边调度到同一节点来降低节点间的网络通信开销。Long等人[12]结合Storm 的不同应用场景，如恢复历史调度任务、单节点任务调度、资源需求调度等，对Storm的资源分配和任务调度做了优化。Chakraborty等人[13]提出了一种基于优先级的调度算法，解决集群资源不足时，可能会导致性能下降、甚至完全饿死具有高业务优先级的拓扑任务的问题。Xin 等人[14提出一种基于分布式QoS（qualityofservice）感知的调度算法，使得Storm适用于地理信息系统的处理。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "以上研究主要从内部通信开销和资源利用率两方面进行优化，均依赖用户配置的Topology的并行度，当用户配置不合理时，现有优化调度方案并不能取得很好的优化效果，并可能对集群处理性能造成更严重的影响，本文提出了一种基最佳并行度的贪心调度算法，在兼顾到网络开销的同时，提升处理效率。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "2 相关描述和相关定义",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "在 Storm 中数据流单位为Tuple。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "定义1一个 Tuple 所需的处理时间 $t _ { t u p l e }$ 定义为",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nt u p l e = t _ { r e c } + t _ { q u e u e } + t _ { p r o c } + t _ { g e n e r a t e }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "其中： $t _ { r e c }$ 表示Tuple由发送节点到处理节点所需的传输时间；tqueue 表示Tuple 在处理节点需要排队等待的时间；tproc 表示Tuple的逻辑处理时间；tgenerate表示Tuple处理完后形成新的Tuple 的时间。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "定义2Tuple 的平均处理时间：",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nt _ { a \\nu g } = \\frac { \\displaystyle \\sum _ { i = 1 } ^ { N _ { t u p l e } } t _ { ( i , t u p l e ) } } { N _ { t u p l e } }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "其中： $N _ { t u p l e }$ 表示一个时间周期内所有组件处理的 Tuple 数量;$N _ { u p l e }$ （204号（20 $\\sum _ { i = 1 } ^ { \\boldsymbol { \\mathbf { v } } _ { t u p l e } } t _ { ( i , t u p l e ) }$ 表示处理 $N _ { t u p l e }$ 个 Tuple 所花费的时间;",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "由式（1）（2）得 ",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nt _ { a v g } = \\frac { \\displaystyle { \\sum _ { i = 1 } ^ { N _ { t u p l e } } } ( { t _ { ( i , r e c ) } + t _ { ( i , q u e u e ) } + t _ { ( i , p r o c ) } + t _ { ( i , g e n e r a t e ) } } ) } { \\displaystyle N _ { t u p l e } }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "由式(3)知，减少节点间的网络通信，能降低 $t _ { r e c }$ ，而当Topology配置不当时，可能会造成前一组件发送的Tuple较快，使得Tuple不断累积，造成tqueue 和 $t _ { P r o c }$ 增大，从而使得 Tuple 平均处理时延增大。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "定义3Topology 中各组件内 Executor 的时效因子TF，表示当前组件中的Executor在时间周期 $T _ { i }$ 内的繁忙程度为",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nT F = \\frac { \\sum _ { j = 1 } ^ { N e x e c u t o r } N _ { ( j , t u p l e ) } * t _ { ( j , t u p l e ) } } { N e x e c u t o r ^ { * } T _ { i } }\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "其中：Nexecutor 表示当前组件中Executor的并行度； $T _ { i }$ 表示第i个时间周期； $N _ { ( j , t u p l e ) }$ 表示在时间周期 $T _ { i }$ 内当前组件中第j个Executor所处理的Tuple 数目； $t ( j , t u p l e )$ 表示在时间周期 $T _ { i }$ 内当前组件中第j个Executor的Tuple平均处理时间。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "TF 值越大，则表明该组件的Executor 越繁忙，即Tuple 流速过快，Executor一直忙于处理 Tuple 流，当 $\\mathrm { T F } { > } 0 . 8$ 时[15]，可能形成数据流处理瓶颈。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3 基于最佳并行度的贪心调度算法",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "当Topology任务提交到Storm到集群中后，Nimbus节点对其进行调度分配，包括将Executor分配到Worker上，再将Worker分配到WorkerNode上，其中Excutor之间的通信分为进程内通信，进程间通信和节点间通信。在首次提交 Topology 时，以 Storm 的默认的调度算法进行调度，同时对该Topoloyg进行监控，并将获取的周期内实时运行参数存入数据库。通过对集群进行实时监控，在首次提交运行周期 $T$ 后或超载时，触发新的调度。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "重新调度时根据获取到Topology的运行数据，求解Topology中各组件的最佳并行度，算法伪代码表2。然后对并行度调整后的Topology任务进行分配，采用贪心策略将通信频率高的Eexecutor尽量分配到集群中的同一节点上，算法伪代码表3。算法用到的相关符号参照表1。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "3.1组件最佳并行度",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Storm中数据流分组包括：",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "ShuffleGrouping:随机分组，尽量均匀分布到下游Bolt中；",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "FieldsGrouping:按字段分组，按数据中Field值进行分组，将相同Field值的Tuple 被发送到同一Task;",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "All Grouping:广播分组，每一个Tuple 都会复制一份给每一个Bolt 处理;",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "GlobalGrouping:全局分组，所有Tuple被分配到一个Bolt中的一个Task 中;",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "None Grouping:不分组，目前等同于 Shuffle Grouping;",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "DirectGrouping:指定分组，由Tuple的发射单元直接决定将Tuple 发射给哪个Bolt;",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "其中以GlobalGrouping为分组方式的组件不需要进行并行度的调整，规定该组件的并行度为1。",
        "page_idx": 2
    },
    {
        "type": "table",
        "img_path": "images/d31e9d058d143a9fe5873e04eb13016d0610620312381123ee0f5c0b6d059143.jpg",
        "table_caption": [
            "表1符号参考表"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>符号</td><td>含意</td></tr><tr><td>ei</td><td>{executori}(i=1,2,3,,n)</td></tr><tr><td>Wi</td><td>{workernodei}(i=1,2,3,n)</td></tr><tr><td>bk</td><td>{boltk}(k=1,2,3,,,n):Topology 中第k个Bolt 组件。</td></tr><tr><td>LEi</td><td>{load-executori}(i=1,2,3,,n):executor的负载</td></tr><tr><td>CEi</td><td>{CPU-executori}(i=1,2,3,,n):executor 的 CPU 使用率</td></tr><tr><td>Rij</td><td>{rateij}(i,j=1,2,3,,n)相互通信的ei与ej发送 tuple速率</td></tr><tr><td>T</td><td>采集周期</td></tr><tr><td>LWNi</td><td>{load-workernodei}(i=1,2,3,,,n):集群中节点workernodei的</td></tr><tr><td>Nexecutor</td><td>负载，即CPU 的使用率</td></tr><tr><td></td><td>{number-executor}:Topology 中 spout 组件的并行度 {number-executork}(k=1,2,3,,n):Topology 中第k 个组件的</td></tr><tr><td rowspan=\"2\"></td><td>N(k,executor）并行度，k=1时，表示 Topology 中 spout 组件，k=2时,</td></tr><tr><td>表示第一个bolt 组件</td></tr><tr><td>AEi</td><td>{assign-executori}(i=1,2,3,,n):对应 ei 的调度方案</td></tr><tr><td>ECi</td><td>{executor-communicationi}(i=1,2,3,,n):对应 ei的通信量， 包括ei接受和发送的Tuple数</td></tr></table></body></html>",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3.1.1 Spout 组件并行度",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "在一个时间周期开始时，设上一周期 Spout未处理完的数据量为Datasurplus，外部数据到达速率为 $V _ { c o m e }$ ，组件 Spout的数据处理速率为 $N _ { e x e c u t o r } * V _ { p r o c }$ ，其中 Nexecutor 是组件 Spout 的Executor 并行度， $V _ { p r o c }$ 是Executor 的数据处理速率。为使得到达Spout组件的数据都能够被及时处理，因此有：",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nD a t a s u r p l u s + { V _ { c o m e } } ^ { * } T _ { i } \\ \\leq \\ N e x e c u t o r ^ { * } { V _ { p r o c } } ^ { * } T _ { i }\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "由式（4）（5）得",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nN e x e c u t o r = \\left\\lceil \\frac { D a t a s u r p l u s + V _ { c o m e } * T _ { i } } { V _ { p r o c } * T _ { i } } \\right\\rceil\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "由式（4）可知增大 $N _ { e x e c u t o r }$ 能使得 TF 的值变小，降低 Spout 组件的繁忙程度，因此对式（6）中求得的Executor并行度向上取整，求得在外部数据到达速率为 $V _ { c o m e }$ 时Spout组件的最佳并行度。式(6)中 $V _ { c o m e }$ 和 $V _ { p r o c }$ 可以通过监控 Spout组件然后取平均值获得，其中周期 $T _ { i }$ 的选择也需要进行考虑，Storm 中进行一次Topology的调整需要一定的时间，因此，这里的时间周期 $T _ { i }$ 不应过短，但也不能太长，周期 $T _ { i }$ 太短，使得调度过于频繁进而增大处理时延，而周期 $T _ { i }$ 太长会导致无法及时对外部变化进行响应，此处 $T _ { i }$ 通过多次实验取得经验值。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3.1.2Bolt组件并行度",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "用户提交的Topology任务中，前一个组件输出的Tuple 数是后一个组件接受的Tuple数。定义第 $\\mathbf { k }$ 个组件中 Executor 的Tuple 处理速度为 $V _ { p r o c }$ ，则组件 $\\mathbf { k }$ 的 Tuple 处理速率为$N _ { ( k , e x e c u t o r ) } * V _ { p r o c }$ ，其中 $N _ { ( k , e x e c u t o r ) }$ 是该组件的Executor 数量。其前一组件，即第 $\\mathbf { k } { - } 1$ 个组件的 Executor 的 Tuple 产生速度为$V _ { g e n e r a t e }$ ，则该组件的 Tuple 产生速度为 $N _ { ( k \\mathrm { ~ - ~ } 1 , e x e c u t o r ) } * V _ { g e n e r a t e }$ ，其中 $N _ { ( k \\mathrm { ~ - ~ } 1 , e x e c u t o r ) }$ 是该组件Executor 的并行度。当后一组件能处理前一组件发送的Tuple 数时，即前一组件产生的Tuple 将不会被排队，如式(7)所示。",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nN ( k ~ { \\textrm { - } } 1 , e x e c u t o r ) ~ { \\textrm { * } } ~ V _ { g e n e r a t e } ~ { \\textrm { * } } ~ T i ~ \\leq ~ N ( k , e x e c u t o r ) ~ { \\textrm { * } } ~ V _ { p r o c } ~ { \\textrm { * } } ~ T i ~ \\left( 7 \\right)\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "由式（4）（7）得",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nN ( k , e x e c u t o r ) = \\left\\lceil \\frac { N ( k - 1 , e x e c u t o r ) ^ { * } V _ { g e n e r a t e } * T _ { i } } { V p r o c ^ { * } T _ { i } } \\right\\rceil \\quad \\mathrm { ( s ) }\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "由式（4)知增大 $N _ { ( k , e x e c u t o r ) }$ 会使TF的值变小，降低Bolt 组件的繁忙程度，因此对式（8）中求得的Executor并行度向上取整，求得在后一组件能处理前一组件发送Tuple速率下第 $\\mathbf { k }$ 个组件的最佳并行度。式（8）中 $V _ { g e n e r a t e }$ 和 $V _ { p r o c }$ 的值可通过实时监控然后取平均值获得。将式（6）代入式（8），求得 Topology中第一个Bolt 组件的并行度 $N _ { ( 2 , e x e c u t o r ) }$ ，然后同理迭代出Topology中其余Bolt组件的最佳并行度。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "算法1求解组件最佳并行度算法",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "输入：Topology 信息 $\\{ \\mathrm { e } _ { 1 } , \\mathrm { e } _ { 2 } , , , \\mathrm { e } _ { \\mathrm { n } } \\}$ 、Nexecutor、 $N _ { ( k , e x e c u t o r ) }$ ，周期T内Topology 的运行数据 Datasurplus 、 $V _ { c o m e }$ 、 $V _ { p r o c }$ 、Vgenerate、 $N _ { ( j , t u p l e ) }$ 、t(j,tuple）。  \n输出：Spout组件的最佳并行度Nexecutor，和各Bolt 组件的最佳并行度$N _ { ( k , e x e c u t o r ) }$ 。  \n1.获取 Spout 组件中 Executor 的数据平均处理速率。  \n2.求解在下一周期T内 Spout 需要处理的数据量 $D a t a s u r p l u s \\textit { + } V _ { c o m e } \\textit { * } T i$ 。  \n3.根据式(6)求得 Topology 中 Spout 组件的最佳并行度Nexecutor 。  \n4.从第一个Bolt 组件开始，即 $\\mathrm { k } { = } 2$ ，依次求解Bolt组件的并行度。5.如果 $b _ { k }$ 的分组方式为Grobal grouping 则跳至(7)。  \n6.根据式(8)求得Topology中第k个Bolt 组件的最佳并行度 $N _ { ( k }$ executor）。7.将 $b _ { k }$ 的最佳并行度设置为： $N _ { \\mathit { ( k , e x e c u t o r ) } } { = } 1$ 。  \n8.选取 $b \\boldsymbol { k } + 1$ ，重复步骤(5)、(6)，直到求得所有Bolt组件的最佳并行度。  \n9.输出求解到的Nexecutor、 $N _ { ( k , e x e c u t o r ) }$ ，算法结束。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3.2基于贪心策略的任务调度",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "采集周期T内，相互通信的Executor之间的Tuple传输速率 $R i , j$ 、Executor 的 CPU 使用率 $C E _ { i }$ 和集群中各节点的负载LWNi，按 $R i , j$ 进行降序排列。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "根据求得的最佳并行度对Topology进行调整，并采用贪心策略进行分配调度，如果某个组件的Executor并行度需要降低，先对该组件中的Executor按 $E C _ { i }$ 进行降序排列，然后去掉 $E C _ { i }$ 低的Executor，同时去掉和该 Executor 通信的 $R _ { i , j }$ ，假如存在$R _ { k , i }$ 和 $R _ { i , j }$ ，当要去掉 $e _ { i }$ 时，则应同时删除 $R _ { k , i }$ 和 $R _ { i , j }$ 。如要增大 Topology中某个组件的并行度，取该组件中 $E C _ { i }$ 低的Executor的通信速率作为该新添加的Executor的通信速率，假设存在 $R _ { k , i }$ 和 $R _ { i , j }$ ，如添加 $e _ { m }$ ，则需添加 $R _ { k , m }$ 和 $R _ { m , j }$ 作为新的关联关系。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "采用贪婪策略对Topology进行分配调度时，尽可能将通信频率高的Executor分配到集群中的同一节点上，最小化节点间的网络通信开销，但太多的Executor分配到同一WorkerNode容易造成该节点的超载，使得该节点处理效率降低，Tuple 处理时延增加。因此，在减少节点间通信量和节点负载之间会有一个权衡。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "思想是：",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "1、采集周期T内运行数据，将相互通信的Executor按 $R _ { i , j }$ 进行降序排列。2、采集周期T内运行数据，对集群中各节点按平均CPU使用率进行降序排列。3、采集周期T内Executor的平均CPU使用率，通过JavaAPI中的getThreadCPUTime(threadID)方法来获取每个线程的CPU使用率。4、循环处理Executor，直到所有的Executor分配完。选择通信频率高的ei、ej，然后对 $\\boldsymbol { e } i \\setminus e _ { i }$ 进行分配。采用贪心策略，当ei， $e _ { j }$ 都未被分配时，选取集群中负载最低的节点 $w i$ ，然后尝试将ei、 $e _ { j }$ 放到 $w i$ 上，如果 $w _ { i }$ 不超载，则 $e _ { i }$ 、 $e _ { j }$ 分配成功。否则从集群中选取新的负载最低的节点 $w _ { j }$ ，然后尝试将 $e _ { i }$ 、ej分配到节点 $w i$ 上，直到 $e _ { i }$ 、 $e _ { j }$ 分配成功。当 $e _ { i }$ 、 $e _ { j }$ 中有一个Executor被分配了，假设 $e _ { i }$ 已被分配到节点 $w i$ 而 $e _ { j }$ 还未被分配，此时尝试将 $e _ { j }$ 分配到 $e _ { i }$ 所在的节点 $w _ { i }$ 上，如果 $w _ { i }$ 不超载，则$e _ { j }$ 分配成功，否则重新从集群中选取新的负载最小的节点 $w _ { j }$ ，然后尝试将e分配到节点 $w j$ ，直到成功。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "5、调度中满足如下约定：",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "在同一个节点上将同一个 Topology 的 Executor 放入到同一个Worker中，且保证节点不会超载。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "本文暂不讨论Executor，Task数量配置优化问题。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "算法2基于最佳并行度的贪心调度算法输入：集群节点运行数据LWNi、 $A E _ { i }$ ，Topology信息 $\\{ \\mathrm { e } _ { 1 } , \\mathrm { e } _ { 2 } , , , \\mathrm { e } _ { \\mathrm { n } } \\}$ 、Nexecutor 、 $N _ { ( k , e x e c u t o r ) }$ ，周期T内 Topology 的运行数据 Datasurplus、Vcome、$\\mathit { \\gamma } _ { { p r o c } \\mathrm { ~ , ~ } } \\ V _ { g e n e r a t e \\mathrm { ~ , ~ } } \\ N _ { ( j , t u p l e ) \\mathrm { ~ , ~ } } \\ t _ { ( j , t u p l e ) \\mathrm { ~ \\ } _ { \\mathrm { ~ o ~ } } }$ （输出： $A E _ { i }$ ， $\\mathrm { i } { = } 1 , 2 , 3 , , , , \\mathrm { n }$ ，即 $\\{ \\mathrm { e } _ { 1 } , \\mathrm { e } _ { 2 } , \\mathrm { e } _ { 3 } , , , \\mathrm { e } _ { \\mathrm { n } } \\}$ 对应的分配方案。1.开始调度 $_ { \\circ } / *$ 在周期T后采用本文调度算法进行重新调度，调度过后，当集群中的节点出现超载或Executor出现超载时，再触发新的调度\\*/2.根据表2 给出的求解组件最佳并行度算法求得 Topology 中 Spout 组件的最佳并行度 $N _ { e x e c u t o r }$ ，Bolt 组件的最佳并行度 $N _ { ( k , e x e c u t o r ) }$ 。3.将 Topology 中对应的组件并行度配置为 Nexecutor 、 $N _ { ( k , e x e c u t o r ) }$ 。4.计算 $R _ { i , j }$ ，将相互通信的 Executor 按 $R _ { i , j }$ 数值大小降序排列。5.计算LWNi，将集群中节点按LWNi数值大小降序排列。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "6.获取集群中负载最小的LWNi节点。  \n7.获取通信频率最高的 $R _ { i , j }$ 。  \n8.如果在 $R _ { i , j }$ 中 $e _ { i }$ 和 $e _ { j }$ 都未被分配，则跳到(9)，否则跳到(12)。9.如果 $e _ { i }$ 和 $e _ { j }$ 都能分配到LWNi节点且不超载，则将 $e _ { i }$ 和 $e _ { j }$ 分配到LWNi节点，并将分配方案记录到 $A E _ { i }$ 和 $A E _ { j }$ ，否则跳到(10)。10.重新获取集群中最新的负载最小的 $L W N _ { i } + 1$ 节点。  \n11.将 $e _ { i }$ 和 $e _ { j }$ 分配到节点 $L W N _ { i } + 1$ ，并将分配方案记录到 $A E _ { i }$ 和 $A E _ { j }$ 。12.如果 $e _ { i }$ 被分配了， $e _ { j }$ 还未被分配，则跳到(13)，否则跳到(15)。13.如果 $e _ { j }$ 能被分配到 $e _ { i }$ 所在的节点且不超载，则将 $e _ { j }$ 分配到 $e _ { i }$ 所在的LWNi节点，并将分配方案记录到AEj，否则跳到(14)。  \n14.重新获取集群中新的负载最小的 $L W N _ { i + 1 }$ 节点，将 $e _ { j }$ 分配到$L W N _ { i + 1 }$ 节点，并记录到 $A E _ { j }$ 中，然后跳到(17)。  \n15.如果ei能被分配到 $e _ { j }$ 所在的节点且不超载，则将 $e _ { i }$ 分配到 $e _ { j }$ 所在的LWNi节点，并将分配方案记录到 $A E _ { i }$ ，否则跳到(16)。  \n16.重新获取集群中新的负载最小的 $L W N _ { i + 1 }$ 节点，将 $e i$ 分配到$L W N _ { i + 1 }$ 节点，并将分配方案记录到 $A E _ { i }$ 中。  \n17.如果 $\\{ \\mathrm { e } _ { 1 } , \\mathrm { e } _ { 2 } , \\mathrm { e } _ { 3 } , , , \\mathrm { e } _ { \\mathrm { n } } \\}$ 中还有未被分配的executor，则跳至(7)，直到所有的executor 被分配。  \n18. $\\{ \\mathrm { e } _ { 1 } , \\mathrm { e } _ { 2 } , \\mathrm { e } _ { 3 } , , , \\mathrm { e } _ { \\mathrm { n } } \\}$ 被分配完,输出分配方案 $A E _ { i }$ ，算法结束。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "4 实验与结果",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "使用Hyperic 的 SIGAR（System Information Gather AndReporter）来读取集群中机器的实时状态，如CPU、内存。结合Storm 提供的ThriftAPI采集 Topology 的运行数据，并将自己实现的监控进程以Deamon进程方式在后台运行。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "实验中利用 Storm 提供的 Pluggable Scheduler[15]来实现本文调度算法。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "实验搭建了三台物理机构成的同构集群，每个节点的具体硬件配置：4核2.8GHzCPU，4GB内存，50GB 硬盘和10Gbit 网卡。每个节点运行Ubuntu12.04 操作系统。其中一个作为主节点运行Nimbus进程，负责资源分配和任务调度；从节点负责接受Nimbus分配的任务，启动和停止属于自己管理的Worker 进程。同时还搭建了Zookeeper[16] 集群，它们始终处于运行状态。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "实验中使用经典的 WordCount 应用，Topology 包括一个Spout和两个Bolt，Spout从Redis中读取数据，形成Tuple发送给后面的Bolt；第一个Bolt对Tuple中的行记录进行分词，然后形成新的Tuple发送给后继Bolt；第二个Bolt用来对词进行统计[17]。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "1)实验1以一定速率向 Redis 中写入数据，然后 Spout从Redis 中读取数据。在将Topology 提交到集群运行后，本文调度算法在周期T后会对 Topology 进行重新调度，统计从第10分钟开始到20分钟的的运行数据，并多次实验取平均值。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "图1显示的是默认(default)调度算法、线上调度(online)算法、热边调度算法和本文调度算法之间处理时延对比。online 和热边调度算法与默认调度算法相比，主要优化了集群节点间的网络通信开销，降低了处理时延。本文调度算法相对online 调度算法和热边调度算法是基于Topology的最佳并行度来进行调度，采用贪心策略进行任务调度，最大化的减少了节点间的网络通信，实验表明本文调度算法比默认调度算法在整体处理时延上降低了约 $2 5 \\%$ ，比online和热边调度算法在整体处理时延上降低了约 $10 \\%$ 。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "图2显示的是4种调度算法之间吐出量的差别，与默认调度算法、线上调度算法和热边调度算法相比本文调度算法吐出量更高。",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/585d5dbf118cd99245053103405a1de899314498b7c71251597826eec9fc426e.jpg",
        "img_caption": [
            "图1处理时延对比"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/b8d444b930dbd73966fb7c0cca2696ace85ceb8ef565519a8b057058d43ab2ba.jpg",
        "img_caption": [
            "图2吞吐量对比"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "将default、online 和热边调度算法中 Topology 的并行度分别配置为本文算法得到的最佳并行度，再分别提交Topology 任务，运行结果图3和图4。由图3知，在Topology配置为最佳并行度下默认调度算法、线上调度算法和热边调度算法的处理时延整体有所下降，更加接近本文调度算法，同时看出基于最佳并行度的贪心调度算法在时效性上最佳。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "图4显示的是Topology任务被分配到的节点的平均CPU使用率。实验结果表明基于最佳并行度的贪心调度比线上调度和热边调度的节点平均CPU 使用率高 $3 6 \\%$ 左右，比默认调度的节点平均CPU使用率高 $45 \\%$ 左右。这是由于在默认调度时采用轮询的方式进行任务调度，会将任务平均地分配到集群中的可用节点上，而线上调度和热边调度是每次从集群中去选取负载最低的节点进行分配，而本文算法采用贪心策略在不超过节点负载的情况下会尽可能的将相互通信频率高的Executor分配到同一节点上，这样能减少Topology任务分配的节点，提高Topolgy任务所分配节点的CPU使用率，从而使得节点的资源利用率更高。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/035fca38f177036a43faeddd290ba2a8b5891cab9f252d348cbbd8a68b744dc9.jpg",
        "img_caption": [
            "图3处理时延对比"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/cfc53a1f748bc250309996208676803ad226f53a30e4ca066dd42fef70b5883b.jpg",
        "img_caption": [
            "图4平均CPU 使用率对比"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "b)实验2不同数据速率下的平均处理时延",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "实验中包括五组对比实验，分别以1000 条/s、2000条/s、3000条/s、4000条/s、5000条/s的速率向Redis中写入数据，然后Spout从Redis中读取数据。图5显示的是默认调度算法和本文调度算法分别在不同数据输入速率下的平均处理时延。",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/a26eb03ad1265485340da88db332164e89e0b130f8e5102ed13308c9f6d84fd5.jpg",
        "img_caption": [
            "图5处理时延对比"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Topology中后端数据处理组件Bolt的处理速度较慢，当写入数据速率较大时，如果Topology并行度配置不当，默认调度算法容易使数据发送端发送的数据产生累积，造成处理时延的增大。实验结果表明本文调度算法比默认调度算法时效性更好，处理时延更低，并且在处理不同速率数据流时更稳定，效果更好。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "5 结束语",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "本文提出的基于最佳并行的贪心调度算法，首先考虑了Topology中组件并行度对调度的影响，然后求解出最佳并行度，并在最佳并行度上采用贪心策略进行调度，和默认调度算法、线上调度算法、热边调度算法相比，本文改进的调度算法，在整体性能上有所提升。在采用贪心策略进行调度时，由于最小化节点间的网络通信，可能会出现负载不均衡的现象。下一步的工作重点将基于本文算法关注调度过程中集群的负载均衡问题。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "参考文献：",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "[1]孙大为，张广艳，郑纬民．大数据流式计算：关键技术及系统实例[J] 软件学报,2014,25(4):839-862.   \n[2]孟小峰，慈祥．大数据管理：概念、技术与挑战[J].计算机研究与发展, 2013,50(1): 146-169.   \n[3]Lim L,Misra A,Mo T.Adaptive data acquisition strategies for energyefficient,smartphone-based, continuous processing of sensor streams [J] Distributed and ParallelDatabases,2013,31(2):321-351.   \n[4]王铭坤，袁少光，朱永利，等．基于 Storm 的海量数据实时聚类[J].计 算机应用,2014,34(11):3078-3081.   \n[5]王润华，母建军，侯佳路.分布式实时计算引擎—Storm研究[J].中 国科技信息,2015(6):68-69   \n[6]Ghaderi J, Shakkottai S,Srikant R. Scheduling storms and streams in the cloud [C]// Proc of ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems.New York: ACM Press, 2015: 439-440.   \n[7]Toshniwal A,Taneja S,Shukla A,et al. Storm@twitter [C]//Proc of ACM SIGMOD international. Conference on Management of Data. New York: ACM Press,2014: 147-156.   \n[8]Aniello L, Baldoni R, QuerzoniL.Adaptive online scheduling in storm [C]// Proc of ACM International Conference on Distributed Event-Based Systems. New York:ACMPress,2013: 207-218.   \n[9]Xu J, Chen Z,Tang J,et al. T-Storm: Traffic-Aware Online Scheduling in Storm [C]// Proc of IEEE International Conference on Distributed Computing Systems.2014: 535-544.   \n[10] Peng B,Hosseini M,Hong Z,et al. R-Storm: resource-aware scheduling in Storm [C]// Proc of MIDDLEWARE Conference. New York: ACM Press, 2015: 149-161.   \n[11]熊安萍，王贤稳，邹洋．基于Storm拓扑结构热边的调度算法[J].计算 机工程,2017,43(1):37-42.   \n[12] Long S,Rao R,Miao W,et al.An improved topology schedule algorithm for storm system [C]// Proc of Asia-Pacific Conference on Computer Science and Applications.[S.1.] : CRC Press,2015: 187.   \n[13] Chakraborty R, Majumdar S.A priority based resource scheduling technique for multitenant storm clusters [C]//Proc of International Symposium on PERFORMANCE Evaluation of Computer and Telecommunication Systems.2016: 1-6.   \n[14] Xin Q,Yao X.Distributed QoS-aware scheduling in cognitive radio cellular networks [C]// Proc of International Conference on Network and Information Systems for Computers.Washington DC:IEEE Computer Society,2015:106-110.   \n[15] Apache Storm [EB/OL].[2017-11-25]. http://storm.apache.org/   \n[16] Apache ZooKeeper [EB/OL]. [2017-11-25]. htps://zookeeper. apache. org/   \n[17] 李川，鄂海红，宋美娜．基于 Storm 的实时计算框架的研究与应用[J]. 软件,2014(10):16-20. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    }
]