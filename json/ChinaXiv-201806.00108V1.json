[
    {
        "type": "text",
        "text": "基于多粒度粗糙集的聚类融合方法",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "于佩秋1,²，李进金1，林国平1,2 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "(1．闽南师范大学 数学与统计学院，福建 漳州 363000;2.福建省粒计算重点实验室，福建 漳州 363000)",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "摘要：现有的聚类融合算法从聚类成员的角度出发，若使用全部聚类成员则融合结果受劣质成员影响，对聚类成员进行选择再进行融合则选择的策略存在主观性。为在一定程度上避免这两种局限性，可以从元素的角度出发，提出一种新的聚类融合方法。通过多粒度决策不一致粗糙集来选择一部分类别确定的元素，再利用这部分元素进行聚类融合生成新的划分；多粒度决策不一致粗糙集模型能够刻画多粒度决策过程中属性一致而决策不一致的现象，提出了一种基于多粒度决策不一致的粗糙集模型，并给出了一种聚类融合方法。具体做法是：首先在数据集上多次使用 K-means聚类算法，生成论域上的多个粒结构；其次对所有粒结构两两之间求粒间包含度，建立包含度矩阵，对矩阵使用Otsu算法计算阈值，得出多组满足阈值条件的信息粒，求解多粒度决策不一致下近似和上近似；最后分别处理下近似与边界域中元素的类别，从而获得了一个经过融合的聚类划分。实验结果表明，该方法能够有效改善聚类的结果，具有较高的时间效率，且算法具有较好的鲁棒性。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "关键词：多粒度粗糙集；聚类融合；大津算法；包含度 中图分类号：TP301.6 doi:10.3969/j.issn.1001-3695.2018.04.0217 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Clustering ensemble algorithm based on multi-granulation rough set ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yu Peiqiu1,², Li Jinjin1†, Lin Guoping1, 2 (1.SchoolofMathematics&Statistics，MinnanNrmalUniversity,ZhangzhouFujian3630o,China;2.Laboratoryof Granular Computing,Zhangzhou Fujian 3630oo,China) ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract:Existingclustering ensemblealgorithmstartsfromthe perspectiveofcluster members,ifalltheclustermembersare used,theensembleresultisectedbytheinferiormembers.Ifthecustermembersareselectedandtenusedinensemble,the selected strategy hassubjectivity.Toavoidthesetwo limitations tosomeextent,fromtheperspectiveof elements,nature proposes anewclustering fusion method:selecting apartofclas-determined elements through multi-granulation rough sets with incongruousdecisions,andthenusingthispartoftheelements to generateanewclustering.Multi-granulationroughet model with incongruous decisionscandescribe thephenomenonofinconsistentdecisions withconsistent tribute set,amodel of multi-granulation rough set with incongruous decisions and aclustering ensemble algorithm based on the model were proposed inthispaper.FirstofallrunaK-Meansclusteringalgorithmseveral timesonthedataset inthecase,multiple granule structures were generated.Next,iclusiondegreesamongallthe granulations werecalculated,andthenthematrixofinclusion degree was obtained.Used Otsu’s method to generatea threshold,then several groupof granulation thatmetthe threshold condition weregot.According to the modelof multi-granulation rough set with incongruous decision,lowerandupper approximations wereobtained.Finallyclassifiedtheelements oflowerapproximationandboundaryseparately,thenaclustering thathasbeen fused wasobtained.Theexperimentsshowed thatthealgorithmhadahightimeeficiencyandrobustness,which improved the result of K-means clustering. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Key words: multi-granulation rough set; clustering ensemble; Otsu's method; inclusion degree ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "0 引言",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "聚类分析[3]是在探索性数据分析领域尤其是在数据挖掘和知识发现方面的一种重要方法，用以揭示数据分布的真实情况。聚类分析目前已被成功应用于工程、生物学、心理学、药学等其他学科中.目前已有的聚类算法还不能够胜任对任意分布情况以及任意形状的数据的聚类,传统的聚类算法都是为特定领域而设计的,在伸缩性和稳定性等方面存在种种不足,因此引入聚类融合[4,5.6,15,17],对聚类结果进行合并,从而得到比单次运行聚类算法更为优越的结果.聚类融合是一个非常强大的工具，可以大大提高非监督分类方法的健壮性以及稳定性。经典的多粒度粗糙集模型[1,2]以属性集上的子集来确定不同的划分，从而形成多个粒度，没有考虑属性集完全相同而决策不同的情况。本文提出了一种刻画属性集相同而决策不同的现象的多粒度决策不一致粗糙集模型,丰富和发展了多粒度粗糙集理论。在使用聚类算法生成划分的过程中,经常存在聚类算法给出的类别标签不一致,这种情况是多粒度决策不一致粗糙集模型的一个特例，可以在聚类融合时使用多粒度决策不一致粗糙集模型。聚类融合是基于聚类分析的结果而产生的一种融合策略。",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "对于聚类融合，目前一部分学者采用对已有的所有聚类结果进行进一步融合的分析逻辑[18]，例如李飞江等人[5]提出了一种粗糙集和证据理论相结合的聚类融合方法，Fred[7利用数据点之间的相似度建立共生矩阵，通过设置阈值来判断矩阵中的两个点是否属于聚类结果中的同一类，此外还有Srehl和Ghosh[提出了三个基于超图的方法MCLA，HGPA和CSPA.这些方法都是对所有的聚类结果进行融合，不能避免劣质聚类成员对聚类融合的质量产生的影响.另一部分学者首先对聚类成员进行评价[16],剔除劣质聚类成员而后再进行聚类融合，例如Faceli等人[9通过遗传算法迭代优化得到最优的融合结果；Hong等人[10]提出通过首先对聚类成员进行选择来提高最终聚类融合结果的质量；阳琳赞等人提出了一种基于粗糙集理论的聚类融合加权迭代模型。这些方法对聚类成员进行了选择,但聚类成员的评价和选择具有较强的主观性,从而使聚类融合结果产生一定程度上偏差。使用多粒度决策不一致粗糙集模型求解真实聚类的下近似，由下近似中元素的类别决定边界域中元素的类别的方法可以在一定程度上减弱这种由于聚类成员的选择而产生的偏差。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "事实上无论聚类成员优劣程度如何,对同一真实聚类而言，劣质聚类成员与优质聚类成员对某些元素的归属可以达成共识。由于多粒度粗糙集具有求同存异的特点[1]，本文首次尝试借鉴多粒度粗糙集理论求多粒度下近似的方法求劣质聚类成员与优质聚类成员的共识元素。将在一个完备的信息系统中多次运行K-means聚类算法生成多个划分，即多个粒度.每个划分中的聚类成员视为等价类,利用多粒度融合的方法,求这些聚类成员的下近似和上近似。通过考量下近似中元素和边界域中元素之间的关系,利用\"类间差异大，类内差异小\"的聚类基本原则,通过求距离所有下近似距离最近的一个元素与每个下近似中部分紧邻元素的平均距离的最小值来决定该元素的分类,从而可以重新建立划分。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "为了进行后续的讨论,首先介绍多粒度决策不一致粗糙集模型。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 多粒度决策不一致粗糙集 ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "在客观世界的决策过程中,由于决策是由专家给出的,存在主观性，即基于同样的条件,不同专家给出的决策可能会不同，这种现象在本文中被称为多粒度决策不一致。首先给出多粒度决策不一致信息系统的概念。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "定义1多粒度决策不一致信息系统。设信息系统 $\\begin{array} { r } { M S = } \\end{array}$ $\\{ I S _ { i } | I S _ { i } = ( U , A T , \\mathrm { f } _ { i } ) \\} ( i \\leq m )$ 为多粒度信息系统，其中 $I S _ { i } =$ $( U , A T , \\mathbf { f } _ { i } )$ 为一个三元信息系统， $U = \\{ x _ { 1 } , x _ { 2 } , \\cdots , x _ { n } \\}$ 为非空有限论域; $A T = \\left\\{ a _ { 1 } , a _ { 2 } , \\cdots , a _ { | A T | } \\right\\}$ 为属性集; $\\mathbf { f } _ { i }$ ： $U \\times A T \\longrightarrow V _ { c }$ 为决策函数， $V _ { c }$ 为决策指标集，即 $\\forall x \\in U$ 有 $ { \\mathrm { f } } ( x , A T ) \\in V _ { c }$ .若 $\\exists x \\in U$ ，当$1 \\leq r \\leq m , 1 \\leq s \\leq m ,$ 使得 $ { \\mathrm { f } _ { r } } ( x ) \\neq  { \\mathrm { f } _ { s } } ( x )$ ，称 $M I D S = \\{ I S _ { i } | I S _ { i } =$ $( U , A T , \\mathbf { f } _ { i } ) \\} ( i \\leq m )$ 为多粒度决策不一致信息系统。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "然后借鉴多粒度粗糙集思想[1],定义多粒度决策不一致粗糙集模型。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "定义2多粒度决策不一致粗糙集。设 $M I D S = \\{ I S _ { i } | I S _ { i } =$ $( U , A T , \\operatorname { f } _ { i } ) , i = 1 , 2 , \\cdots , m \\}$ 是一个多粒度决策不一致信息系 统， $I S _ { i } = ( U , A T , \\mathbf { f } _ { i } ) , \\mathbf { f } _ { i }$ ： $\\mathbb { U } \\times A T \\longrightarrow V _ { c }$ 为决策函数，则多粒度决 策不一致下近似为 ",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } } } ( x ) = \\{ ( y \\in U | \\mathrm { f } _ { 1 } ( x ) = \\mathrm { f } _ { 1 } ( y ) \\Lambda \\mathrm { f } _ { 2 } ( x ) = \\mathrm { f } _ { 2 } ( y ) \\Lambda \\cdots \\Lambda \\mathrm { f } _ { m } ( x ) ,\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "多粒度决策不一致上近似为",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\n\\overline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathfrak { f } _ { i } } } } ( x ) = \\{ y \\in U | \\mathfrak { f } _ { 1 } ( x ) = \\mathfrak { f } _ { 1 } ( y ) \\backslash \\mathfrak { f } _ { 2 } ( x ) = \\mathfrak { f } _ { 2 } ( y ) \\backslash \\cdots \\backslash \\mathfrak { f } _ { m } ( x ) ,\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "多粒度决策不一致边界为",
        "page_idx": 1
    },
    {
        "type": "equation",
        "text": "$$\nB N _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } ( x ) = \\overline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } } } ( x ) - \\underbrace { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } ( x ) } _ { } ;\n$$",
        "text_format": "latex",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "那么称 $\\Big ( \\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\mathbf { f } _ { i } } } } ( \\mathbf { x } ) , ~ \\overline { { I D _ { \\sum _ { i = 1 } ^ { m } \\mathbf { f } _ { i } } } } ( \\mathbf { x } ) \\Big )$ 为多粒度决策不一致粗糙集模型。",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "多粒度决策不一致粗糙集具有如下性质：",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "(1) $\\begin{array} { r l } & { \\frac { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( x ) \\subset \\overline { { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( x ) } } ; } { \\bigcup _ { x \\in U } \\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( x ) } } } ( x ) = U , \\bigcup _ { x \\in U } \\overline { { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } } } ( x ) = U ; } \\\\ & { \\forall u \\in \\frac { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( x ) , I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( u ) = I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( x ) ; } { \\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } } } ( x ) } } \\end{array}$   \n(2)   \n(3)   \n(4) $\\underline { { { I D } _ { \\sum _ { i = 1 } ^ { m } \\bar { f } _ { i } } ( x ) } } = \\bigcap _ { i = 1 } ^ { m } \\underline { { { \\mathrm { f } } } } _ { i } ( x ) , \\overline { { { I D _ { \\sum _ { i = 1 } ^ { m } \\bar { f } _ { i } } } } } ( x ) = \\bigcup _ { i = 1 } ^ { m } \\overline { { { \\mathrm { f } } } } _ { i } ( x ) ;$ ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "证明：",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "$\\begin{array} { r l } & { \\quad ( 1 ) \\ \\forall y \\in I D _ { \\sum _ { i = 1 } ^ { m } \\ f _ { i } } ( x ) \\qquad \\not \\in \\qquad \\ f _ { i } ( y ) = \\ f _ { i } ( x ) ( \\forall i \\leq m ) \\Rightarrow y \\in } \\\\ & { \\frac { ( 1 ) } { I D _ { \\sum _ { i = 1 } ^ { m } \\ f _ { i } } ( x ) , \\Rightarrow \\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\ f _ { i } } ( x ) } } } ( x ) \\subset \\overline { { I D _ { \\sum _ { i = 1 } ^ { m } f _ { i } } } } ( x ) . } \\end{array}$ (2) $\\forall x \\in U$ 有 $\\mathfrak { f } _ { i } ( x ) = \\mathfrak { f } _ { i } ( x ) ( \\forall i \\leq m ) \\Rightarrow U \\subseteq$   \n$\\cup _ { x \\in U } I D _ { \\sum _ { i = 1 } ^ { m } f _ { i } } ( x ) , \\quad \\overleftrightarrow { \\mathcal { H } } \\quad \\longrightarrow \\quad \\overleftrightarrow { \\mathcal { H } } \\quad \\overleftrightarrow { \\mathbb { H } } \\quad \\forall x \\in U , I D _ { \\sum _ { i = 1 } ^ { m } f _ { i } } ( x ) \\subseteq U \\Rightarrow$   \n（204号 $\\cup _ { x \\in U } \\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\ f _ { i } } ( x ) = U _ { \\cdot } } }$ 类似可证 $\\cup _ { x \\in U } \\overline { { I D _ { \\sum _ { i = 1 } ^ { m } \\ f _ { i } } } } ( x ) = U$ (3) $\\begin{array} { r } { \\forall u \\in U , u \\in { \\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( x ) } } } \\Leftrightarrow \\forall i \\leq m , \\ f _ { i } ( u ) = \\ f _ { i } ( x ) \\Leftrightarrow } \\end{array}$   \n$\\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } } } ( u ) = \\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } ( x ) } } .$ (4)只证 $\\underline { { I D _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } } } ( x ) = \\cap _ { i = 1 } ^ { m } \\underline { { \\ : \\mathrm { f } _ { i } } } ( x )$ ，后面的部分类似可证；  \n（204 $\\forall x \\in \\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\Sigma _ { i } } ( x ) } }$ 满足 $\\mathrm { f } _ { 1 } ( x ) = \\mathrm { f } _ { 1 } ( y ) \\Lambda \\mathrm { f } _ { 2 } ( x ) = \\mathrm { f } _ { 2 } ( y ) \\Lambda \\cdots \\Lambda \\mathrm { f } _ { m } ( x )$ ，则  \n$x \\in \\cap _ { i = 1 } ^ { m } \\underline { { \\mathfrak { f } } } _ { i } ( x )$ ，另一方面 $\\forall x \\in \\cap _ { i = 1 } ^ { m } \\underline { { \\mathfrak { f } } } _ { i } ( x )$ 有f1 $( x ) =$   \n$\\mathrm { f } _ { 1 } ( y ) \\Lambda \\mathrm { f } _ { 2 } ( x ) = \\mathrm { f } _ { 2 } ( y ) \\Lambda \\cdots \\Lambda \\mathrm { f } _ { m } ( x ) ,$ 即 $x \\in { \\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } } \\mathrm { f } _ { i } } } } ( x )$ ,综上所述  \n$\\underline { { I } } \\underline { { D } } _ { \\sum _ { i = 1 } ^ { m } \\int _ { i } } ( x ) = \\cap _ { i = 1 } ^ { m } \\underline { { \\{ _ { i } ( x ) } _ { . } } $ ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "例1 多粒度决策不一致信息系统与上下近似的求解",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "设论域 $U = \\{ x _ { 1 } , x _ { 2 } , x _ { 3 } , x _ { 4 } , x _ { 5 } \\}$ ,在两个不同的粒度下分别有如表1所示的决策表。",
        "page_idx": 1
    },
    {
        "type": "table",
        "img_path": "images/d4b02b5301b1a2fa66dedb2307ab6ad8769fc57fe8d3540bf5f9c60e18fdb977.jpg",
        "table_caption": [
            "表1一个多粒度决策不一致信息系统"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>U</td><td>A</td><td>D1</td><td>U</td><td>A</td><td>D2</td></tr><tr><td>X1</td><td>1</td><td>1</td><td>X1</td><td>1</td><td>1</td></tr><tr><td>x2</td><td>1</td><td>1</td><td>x2</td><td>1</td><td>1</td></tr><tr><td>X3</td><td>2</td><td>2</td><td>X3</td><td>2</td><td>2</td></tr><tr><td>X4</td><td>2</td><td>2</td><td>X4</td><td>2</td><td>2</td></tr><tr><td>X5</td><td>3</td><td>2</td><td>X5</td><td>3</td><td>1</td></tr></table></body></html>",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "显然 $M I D S = \\{ I S _ { i } | I S _ { i } = ( U , A , \\mathrm { f } _ { i } ) , i = 1 , 2 \\}$ 是一个多粒度决策不一致信息系统.由定义1.2有： $\\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } } } ( x _ { 1 } ) = \\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } ( x _ { 2 } ) = } }$ （204号 $\\{ x _ { 1 } , x _ { 2 } \\}$ $\\overline { { { I D _ { \\sum _ { i = 1 } ^ { m } f _ { i } } } } } ( x _ { 1 } ) = \\overline { { { I D _ { \\sum _ { i = 1 } ^ { m } f _ { i } } } } } ( x _ { 2 } ) = \\{ x _ { 1 } , x _ { 2 } , x _ { 5 } \\} ~ , ~ B N _ { \\sum _ { i = 1 } ^ { m } f _ { i } } ( x _ { 1 } ) =$ $B N _ { \\sum _ { i = 1 } ^ { m } \\mathbf { f } _ { i } } ( x _ { 2 } ) = \\{ x _ { 5 } \\} .$ （204",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2 基于多粒度粗糙集的聚类融合算法（MGIDA）的设计",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.1噪声的去除和多粒度不一致下近似与边界域的求解 ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "首先介绍一种本文所用的聚类算法：K-means聚类算法：",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "K-means聚类算法是一种至今仍然广泛应用的经典聚类算法，该算法流程如下：设聚类类别数目为 $k$ ，",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "1.选定k个初始聚类中心 $\\mathfrak { s } K = \\{ \\{ x _ { 1 } \\} , \\{ x _ { 2 } \\} , \\cdots , \\{ x _ { k } \\} \\}$ ，分别代表k个类，此时$K _ { 1 } = \\{ x _ { 1 } \\} , K _ { 2 } = \\{ x _ { 2 } \\} , \\cdots , K _ { k } = \\{ x _ { k } \\} ;$ ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.重复以下计算过程，直到所有的聚类中心不再改变：",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "(1)对每个 $x \\in U - K$ ，计算 $\\cdot { } x$ 到 $k$ 个聚类中心的距离，假设 $x$ 到K的聚类中心的的距离是最近的则使 $K _ { * } = K _ { * }$ Ux;(2）重新计算K内样本的各项属性的平均值作为 $\\cdot K _ { * }$ 的新的聚类中心.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3.输出所有的聚类中心和元素类别，算法结束.",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "设 $U$ 为非空有限论域，在该论域上多次运行聚类算法后生成论域多个U的划分，把每次运行聚类算法所形成的划分看做是单个粒度结构,多次运行聚类算法形成多个粒度结构,即多个粒度空间。利用定义2的性质（4）可以方便地求得该粒度空间中多粒度决策不一致下近似。但在求解的过程中，可能存在由一致性噪声产生的下近似，在利用多粒度不一致粗糙集模型进行融合的之前，需要去除这些噪声。方法是首先对论域上的所有粒求粒间包含度。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "定义 $\\pmb { 3 } ^ { [ 1 1 ] }$ 设集合A与 $B$ 是论域 $U$ 的非空子集,定义集合A与集合 $B$ 的包含度为",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\ns i m ( A , B ) = \\frac { | A \\cap B | } { | A \\cup B | } .\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "并将计算得到的包含度存入包含度矩阵 $S ( C )$ ，包含度矩阵定义如下:",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "定义4包含度矩阵 $s ( C )$ 。设有论域 $U$ 上的一个子集族 $C =$ $\\{ C _ { n } \\colon C _ { n } \\subseteq U \\} .$ 由定义2.1计算 $C _ { i } ( i < n )$ 与 $C _ { j } ( j < n )$ 之间的相容度,并将 $C _ { i }$ 与 $C _ { j }$ 的相容度填入矩阵 $S ( C )$ 的第i行第j列所获得的矩阵即为包含度矩阵。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "显然有任意的 $S ( C )$ 中的值大于等于0且小于等于1。得到包含度矩阵后,利用Otsu算法[12]对该矩阵计算包含度阈值。Otsu算法又称大津算法，是由日本学者大津于1979 年提出的一种使前景与背景的类间方差最大化的阈值方法.用以对图像进行阈值分割,该方法又称为最大类间方差法.设 $\\mathsf { S } ( \\mathsf { C } ) _ { \\mathrm { r } \\times \\mathrm { r } }$ 的均值为m，存在阈值t将S(C)中的所有元素分为两类，大于t的类A和小于t的类B,A的均值为 $\\mathrm { { m _ { A } } }$ ，B的均值为 $\\mathbf { m _ { B } }$ ,Nobuyuki Otsu[12]给出的类间方差定义为",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nI _ { A B } = \\frac { | A | } { r ^ { 2 } } ( m _ { A } - m ) ^ { 2 } + \\frac { | B | } { r ^ { 2 } } ( m _ { B } - m ) ^ { 2 } ;\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "该方法寻找一个最佳阈值使得将用灰度值表示的图像分割为两类后错分概率最小,这个最佳的阈值即遍历t的各种取值，取使得 $\\mathrm { \\cdot I _ { A B } }$ 最大的一个t",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "本文将包含度矩阵 $s ( C )$ 作为图像，计算得到阈值后,小于阈值的包含度即由一致性噪声造成的(可视为背景)。对包含度大于阈值的粒求多粒度决策不一致下近似即可。由定义2的性质(4)设满足阈值条件的信息粒为 $\\big \\{ C _ { s _ { 1 } } , C _ { s _ { 2 } } \\big \\}$ ，则 $\\forall x \\in$ $\\cup _ { i = 1 } ^ { 2 } C _ { s _ { i } } , I D _ { \\sum _ { i = 1 } ^ { m } \\Sigma _ { i } } ( x ) = \\cap _ { i = 1 } ^ { 2 } C _ { s _ { i } }$ ．若存在 $I D _ { \\Sigma _ { i = 1 } ^ { m } \\Sigma _ { i } } ( x ) \\cap$ $I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathbf { f } _ { i } } ( y ) \\neq \\emptyset$ ，则合并这两个下近似，使 $\\begin{array} { r } { I D _ { \\sum _ { i = 1 } ^ { m } \\ f _ { i } } ( x ) = } \\end{array}$ $\\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } ( y ) } } = \\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } ( x ) \\cup I D _ { \\Sigma _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } ( y ) } } .$ ，在以后的讨论中，令$B N = U - \\mathsf { U } _ { x \\in U } I D _ { \\Sigma _ { i = 1 } ^ { 2 } \\Sigma _ { i } } ( x )$ 为边界域。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "例2去除噪声和求解多粒度决策不一致下近似与边界域 ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "设论域 $U = \\{ x _ { 1 } , x _ { 2 } , x _ { 3 } , x _ { 4 } , x _ { 5 } \\}$ ,使用聚类算法运行两次生成的划分为$C = \\{ C _ { 1 } , C _ { 2 } \\} ,$ 其中 $^ 1 C _ { 1 } = \\big \\{ \\{ x _ { 1 } , x _ { 2 } , x _ { 5 } \\} , \\{ x _ { 3 } , x _ { 4 } \\} \\big \\} , C _ { 2 } = \\big \\{ \\{ x _ { 1 } , x _ { 2 } \\} , \\{ x _ { 3 } , x _ { 4 } , x _ { 5 } \\} \\big \\} ,$ 计算相容度并填入相容度矩阵，得到相容度矩阵 $s ( C )$ ",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nS ( C ) = \\left[ { \\begin{array} { c c c c } { 1 } & { 0 } & { 2 / 3 } & { 1 / 5 } \\\\ { 0 } & { 1 } & { 0 } & { 2 / 3 } \\\\ { 2 / 3 } & { 0 } & { 1 } & { 0 } \\\\ { 1 / 5 } & { 2 / 3 } & { 0 } & { 1 } \\end{array} } \\right]\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "使用Otsu 算法计算阈值(在MATLAB 数学软件中，Otsu 算法是一个系统函数graythresh),得到阈值为0.4314,则满足阈值条件的信息粒为 ${ ^ { \\mathrm { I } G r } } _ { 1 } = \\big \\{ \\{ x _ { 1 } , x _ { 2 } , x _ { 5 } \\} , \\{ x _ { 1 } , x _ { 2 } \\} \\big \\} , \\ G r _ { 2 } = \\big \\{ \\{ x _ { 3 } , x _ { 4 } \\} , \\{ x _ { 3 } , x _ { 4 } , x _ { 5 } \\} \\big \\} ,$ 不满足阈值条件的包含度即由一致性噪声造成，不作处理。求对满足阈值条件的信息粒解多粒度决策不一致下近似并求边界域,得",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } f _ { i } } ( x _ { 1 } ) = I D _ { \\Sigma _ { i = 1 } ^ { m } f _ { i } } ( x _ { 2 } ) = \\{ x _ { 1 } , x _ { 2 } , x _ { 5 } \\} \\cap \\{ x _ { 1 } , x _ { 2 } \\} = \\{ x _ { 1 } , x _ { 2 } \\} } } ;\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\n\\underline { { I D _ { \\Sigma _ { i = 1 } ^ { m } f _ { i } } ( x _ { 3 } ) = I D _ { \\Sigma _ { i = 1 } ^ { m } f _ { i } } ( x _ { 4 } ) = \\{ x _ { 3 } , x _ { 4 } \\} \\cap \\{ x _ { 3 } , x _ { 4 } , x _ { 5 } \\} = \\{ x _ { 3 } , x _ { 4 } \\} } } ;\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "equation",
        "text": "$$\nB N = U - \\underbrace { I D _ { \\Sigma _ { i = 1 } ^ { m } \\Sigma _ { i } } ( x _ { 1 } ) \\cup I D _ { \\Sigma _ { i = 1 } ^ { m } \\Sigma _ { i } } ( x _ { 3 } ) } _ { \\qquad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad }\n$$",
        "text_format": "latex",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "由定义2，下近似中的元素在每次聚类的过程中都同属于一类，所以下近似中元素是确定同属一类的，而边界中的元素则不一定在每次聚类过程中都同属于一类，所以边界域中元素类别是不定的,所以需要设计算法确定边界域中元素类别。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2.2边界域元素的处理",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "为了方便处理边界域的元素，首先给出聚类的一种定义，该定义是根据\"类间距离大，类内距离小\"得出的。",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "定义5设 $( U , A , \\mathfrak { f } )$ 为完备信息系统，给定距离度量$d ( x , y ) \\colon U \\times U \\to [ 0 , + \\infty )$ ,聚类即在 $U$ 上建立划分 ${ \\mathsf { C } } = \\left\\{ { \\mathsf { C } } _ { k } \\colon k = \\right.$ $1 , 2 , \\cdots , m \\}$ ,使得对于任意的 $x , y \\in \\mathsf C _ { i }$ ，若满足对于任意的 $y ^ { \\prime } \\in$ $\\mathsf C _ { i } , d ( x , y ) \\le d ( x , y ^ { \\prime } )$ ，则对于任意的 $z \\in \\mathsf C _ { j } ( i \\neq j )$ 有 $d ( x , y ) <$ $d ( x , z )$ ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "聚类融合最终是要生成一个聚类.使用多粒度决策不一致粗糙集模型不可避免地产生边界域,如何处理这些边界域中元素将是一个值得讨论的问题;如果能够找到多粒度决策不一致下近似中元素与边界域中元素的某种关系,即可通过下近似中的元素的类别来确定边界域中元素的类别。由定义5给出如下定理:",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "定理1设 $( U , A , F )$ 为完备信息系统, $\\mathsf { C } = \\{ \\mathsf { C } _ { k } \\colon k = 1 , 2 , \\cdots , m \\}$ 是 $U$ 上的一个聚类, $\\underline { { \\mathsf { C } } } = \\Bigl \\{ \\underline { { \\mathsf { C } _ { k } } } \\colon \\underline { { \\mathsf { C } _ { k } } } \\subseteq \\mathsf { C } _ { k } , k = 1 , 2 , \\cdots , m \\Bigr \\} ,$ 其中 $\\underline { { \\mathsf { C } _ { k } } }$ 是 $\\complement _ { k }$ 的任意非空子集， $x \\in U - \\mathsf { U } _ { i = 1 } ^ { m } \\underline { { \\mathsf { C } } } _ { i }$ 且 $d ( x , y ) = m i n \\left\\{ D \\left( x , \\underline { { { \\mathsf { C } } } } _ { k } \\right) : k = \\right.$ $\\left. 1 , 2 , \\cdots , m \\right\\}$ 对任意 $\\boldsymbol { \\mathrm { y } } \\in \\cup _ { k = 1 } ^ { m }$ Ck都成立，则 $x \\in \\mathsf C _ { i }$ 当且仅当存在 $y \\in$ $\\underline { { \\mathsf { C } } } _ { i }$ 使得对于任意的 $\\mathbf { z } \\in \\underline { { \\mathsf { C } _ { j } } } ( i \\neq j )$ 有 $d ( x , y ) < d ( x , z )$ ,其中$D ( x , X ) = m i n \\{ d ( x , t ) \\colon t \\in X \\} _ { \\circ }$ ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "证明：1）充分性",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "若 $d ( x , y ) = m i n \\left\\{ D \\left( x , \\underline { { { \\mathsf { C } } } } _ { k } \\right) : k \\leq m \\right\\}$ 且 $\\mathsf { y } \\in \\mathsf { U } _ { k = 1 } ^ { m } \\underline { { \\mathsf { C } _ { k } } }$ ，则 $\\begin{array} { r l } & { \\exists \\underline { { \\complement } } _ { \\underline { { i } } } , s . t . d ( x , y ) = D \\left( x , \\underline { { \\complement } } _ { \\underline { { i } } } \\right) , \\underline { { \\complement } } _ { \\underline { { i } } } \\subseteq \\complement _ { i } \\Rightarrow \\forall z \\in U - \\underline { { \\complement } } _ { \\underline { { i } } } , d ( x , y ) < } \\\\ & { d ( x , z ) \\Rightarrow \\forall z \\in U - \\complement _ { i } \\subseteq U - \\underline { { \\complement } } _ { \\underline { { i } } } , d ( x , y ) < d ( x , z ) \\Rightarrow x \\in \\complement _ { i } \\circ } \\end{array}$ ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2）必要性",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "若 $\\begin{array} { r } { \\mathrm { ~ : ~ } x \\in \\mathbb { C } _ { i } , x \\in U - \\bigcup _ { k = 1 } ^ { m } \\underline { { \\complement } } _ { k } \\operatorname { E d } ( x , y ) = m i n \\left\\{ D \\left( x , \\underline { { \\complement } } _ { k } \\right) : k = 1 \\right. } \\end{array}$ $\\left. 1 , 2 , \\cdots , m \\right\\}$ 对任意y ${ \\sf r } \\in \\bigcup _ { k = 1 } ^ { m } \\underline { { \\sf C } } _ { k }$ 都成立，则 $y \\in \\underline { { \\mathsf { C } } } _ { i }$ 且 $\\forall z \\in \\underline { { \\mathsf { C } } } _ { j } ( i \\neq j )$ 有 $d ( x , y ) < d ( x , z )$ .否则 $\\mathbf { y } \\in \\underline { { \\mathsf { C } _ { j } } } ( i \\neq j ) , \\exists \\mathbf { t } \\in \\underline { { \\mathsf { C } _ { i } } }$ 且满足 $d ( x , t ) =$ $\\operatorname* { m i n } \\{ d ( x , r ) | \\mathbf { r } \\in \\underline { { \\mathsf { C } } } _ { i } \\}$ 使 $d ( x , y ) < d ( x , t ) \\Rightarrow x \\in { \\mathsf { C } } _ { j } ,$ 矛盾。  \n说明：定理1给出了一种处理边界域元素的方式，即距离所有下近似最近的一个元素（最小的 $D \\left( x , \\underline { { \\mathsf { C } } } _ { k } \\right)$ ， $k \\leq m _ { \\cdot }$ ）一定属于距离这个元素最近的一个下近似。可以通过寻找距离所有下近似最近的一个边界域元素 $x$ 并将该元素并入距离它最近的一个下近似以逐步缩小边界域。由于真实数据集存在复杂性，所以比较可靠的方式是用 $x$ 到下近似中最近的一部分元素（元素个数为 $N _ { 0 }$ ）的平均距离来代替 $x$ 到下近似中最近的一个元素的距离.然后通过比较 $x$ 到所有下近似距离的最小值即可得出边界域中每个元素的归属。在本文中取Nomin[lm +1。2  \n重复这一过程，当边界域中所有的元素都被并入下近似后，论",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "域中不再存在类别不确定的元素，即形成了一个新的划分。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "例3边界域元素的处理",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "续例2, $B N = U - \\underbrace { I D _ { \\sum _ { i = 1 } ^ { m } \\mathsf { f } _ { i } } ( x _ { 1 } ) \\cup I D _ { \\sum _ { i = 1 } ^ { m } \\mathsf { f } _ { i } } ( x _ { 3 } ) } _ { \\qquad } = x _ { 5 , B N } = B N -$ （20 $\\begin{array} { r } { \\{ x _ { 5 } \\} , N _ { 0 } = \\frac { \\operatorname* { m i n } \\{ 2 , 2 \\} } { 2 } + 1 = 2 } \\end{array}$ 设 $\\underline { { G r _ { 1 } } }$ 代表由 $G r _ { 1 }$ 求得的下近似， ${ \\underline { { G r _ { 2 } } } }$ 代表 由 $G r _ { 2 }$ 求得的下近似，若 $\\begin{array} { r } { \\frac { \\cdot d ( x _ { 5 } , x _ { 1 } ) + d ( x _ { 5 } , x _ { 2 } ) } { 2 } < \\frac { d ( x _ { 5 } , x _ { 3 } ) + d ( x _ { 5 } , x _ { 4 } ) } { 2 } } \\end{array}$ ，则 ${ \\underline { { G r _ { 1 } } } } =$ （20 $\\underline { { G r _ { 1 } } } \\cup \\{ x _ { 5 } \\}$ ，否则 $\\underline { { G r _ { 2 } } } = \\underline { { G r _ { 2 } } } \\cup \\{ x _ { 5 } \\} .$ ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "基于以上讨论,给出基于多粒度决策不一致粗糙集的聚类融合算法：",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "算法1基于多粒度决策不一致粗糙集的聚类融合算法(MGIDA) ",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "输入：运行多次或多个聚类算法生成的一族划分。  \n输出：一个经过聚类融合的划分。  \nStepl 计算相容度矩阵;  \nStep2使用Otsu算法计算相容度矩阵的阈值，聚类成员之间相容度大于阈值即一个满足阈值条件的信息粒，求出所有这样的信息粒；  \nStep3 利用定义2求解下近似，求边界域 $B N$ ：  \nStep4取 $. x \\in B N$ 且满足 $d ( x , y ) = m i n \\left\\{ D \\left( x , \\underset { \\overline { { { \\cal { L } } _ { i = 1 } ^ { m } \\mathrm { f } } _ { i } } } { I D _ { \\sum _ { i = 1 } ^ { m } \\mathrm { f } _ { i } } } ( x ) \\right) : k = \\right.$ $1 , 2 , \\cdots , m ; x \\in U \\Biggr \\}$ ，使用定理1的说明的方法对 $x$ 重新归类;  \nStep5 $B N \\gets B N - \\{ x \\}$ ；当 $B N \\neq \\varnothing$ 时转至Step4;  \nStep6，输出所有元素的类别。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "为了验证算法的有效性.下面在10个数据集上进行验证。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "3 实验结果对比与分析",
        "text_level": 1,
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "使用的数据集相关的信息如表3所示。为使同一数据集产生不同的较差的划分,使用算法2对数据集进行处理,算法的具体过程如例4。",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "例4使用同一数据集生成不同的弱划分",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "给定信息系统如表2所示，分别生成2个模为1的一维随机向量：",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r } { r _ { 1 } = \\langle 0 . 5 0 3 0 , 0 . 8 4 0 6 , 0 . 2 0 0 7 \\rangle ^ { T } , } \\\\ { r _ { 2 } = \\langle 0 . 0 9 7 9 , 0 . 6 9 8 5 , 0 . 7 0 8 9 \\rangle ^ { T } ; } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "分别使 $U \\cdot \\mathbf { r } _ { 1 } , U \\cdot \\mathbf { r } _ { 2 }$ 其中·代表矩阵乘法,得到：",
        "page_idx": 3
    },
    {
        "type": "equation",
        "text": "$$\n\\begin{array} { r } { U _ { \\mathrm { r } _ { 1 } } = \\langle 0 . 6 9 6 7 , 0 . 4 5 8 6 , 0 . 8 9 4 6 , 1 . 3 1 0 1 \\rangle ^ { T } , } \\\\ { U _ { \\mathrm { r } _ { 2 } } = \\langle 0 . 6 4 5 4 , 0 . 4 2 5 4 , 0 . 8 7 7 4 , 0 . 9 9 7 4 \\rangle ^ { T } . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 3
    },
    {
        "type": "table",
        "img_path": "images/ddab42590f0af4e540ac338bd9a72d03fe9ef441eddba4d550fad82c39528278.jpg",
        "table_caption": [
            "表2一个不带决策的信息系统"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>U</td><td>a1</td><td>a2</td><td>a3</td></tr><tr><td>X1</td><td>0.4173</td><td>0.4929</td><td>0.3692</td></tr><tr><td>x2</td><td>0.0497</td><td>0.4893</td><td>0.1112</td></tr><tr><td>X3</td><td>0.9027</td><td>0.3377</td><td>0.7803</td></tr><tr><td>X4</td><td>0.9448</td><td>0.9001</td><td>0.3897</td></tr></table></body></html>",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "分别对 $U _ { \\mathrm { r } _ { 1 } }$ 、 $U _ { { \\bf r } _ { 2 } }$ 使用 $\\mathrm { ~ K ~ }$ -means聚类算法进行聚类，得到两个不同的弱划分：",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\nC _ { 1 } = \\bigl \\{ \\{ x _ { 1 } , x _ { 2 } , x _ { 3 } \\} , \\{ x _ { 4 } \\} \\bigr \\} , C _ { 2 } = \\bigl \\{ \\{ x _ { 1 } , x _ { 2 } \\} , \\{ x _ { 3 } , x _ { 4 } \\} \\bigr \\} ,\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "这样就使用同一数据集生成了不同的弱划分。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "算法2[5]弱划分的生成",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "1：生成一个随机的 $d$ 维随机向量 $u ,$ 并使 $| u | = 1$ ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "使用K-means算法对处理后的数据集进行聚类.使用CSPA、HGPA、MCLA（前三者均为基于图的聚类融合算法）、IWCE[13]（基于粗糙集理论的聚类融合加权迭代模型）、DSCE[5]（多粒度信息融合：一种基于证据理论的聚类集成方法）作为对比算法。",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/acff4b20bad86e0839e0d197ebcc9b4cc28c8c624203f3952e384e0689e2a380.jpg",
        "table_caption": [
            "表3实验使用的UCI数据集"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>ID</td><td>数据集</td><td>实例数</td><td>属性</td><td>类数</td></tr><tr><td>1</td><td>Turkey Student Evaluation Generic</td><td>5820</td><td>31</td><td>13</td></tr><tr><td>2</td><td>Epileptic Seizure Recognition Data Set</td><td>11500</td><td>178</td><td>5</td></tr><tr><td>3</td><td>Data User Modeling Dataset</td><td>258</td><td>5</td><td>4</td></tr><tr><td>4</td><td>Synthetic control Data</td><td>600</td><td>60</td><td>6</td></tr><tr><td>5</td><td>Seeds data set</td><td>210</td><td>7</td><td>3</td></tr><tr><td>6</td><td>Wine Recognition data</td><td>178</td><td>13</td><td>3</td></tr><tr><td>7</td><td>Iris</td><td>150</td><td>4</td><td>3</td></tr><tr><td>8</td><td>Mammographic MassData</td><td>830</td><td>5</td><td>2</td></tr><tr><td>9</td><td>PimaIndiansDiabetes Database</td><td>768</td><td>8</td><td>2</td></tr><tr><td>10</td><td>HTRU2</td><td>17898</td><td>8</td><td>2</td></tr></table></body></html>",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "每次生成4个弱划分进行聚类融合，融合结果与真实的聚类对比计算聚类精度[1],聚类精度定义如下：",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\nA C = \\sum _ { i = 1 } ^ { k } { \\frac { m a x _ { j = 1 , 2 , \\cdots , k } n _ { i j } } { n } } ;\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "其中,若真实的聚类划分为 $\\mathsf C _ { \\mathrm R } = \\{ \\mathsf C _ { 1 } , \\mathsf C _ { 2 } , \\cdots , \\mathsf C _ { \\mathrm k } \\}$ ，聚类融合得到的聚类划分为 $\\mathrm { C } _ { \\mathrm { F } } = \\{ \\mathrm { F } _ { 1 } , \\mathrm { F } _ { 2 } , \\cdots , \\mathrm { F } _ { \\mathrm { k } } \\}$ 则",
        "page_idx": 4
    },
    {
        "type": "equation",
        "text": "$$\nn _ { i j } = \\big | C _ { i } \\cap F _ { j } \\big | , i , j \\le k ;\n$$",
        "text_format": "latex",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/274afd78a5e9b285538bd30391b6b544fa1bfa99b0a5ac7a8c2b7be30f77e72b.jpg",
        "table_caption": [
            "表4UCI数据对比实验结果"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>CSPA</td><td>HGPA</td><td>MCLA</td><td>IWCE</td><td>DSCE</td><td>MGIDA</td></tr><tr><td rowspan=\"2\">1</td><td>0.1739±</td><td>0.1742</td><td>0.1713</td><td>0.1742</td><td>0.1688</td><td>0.1683</td></tr><tr><td>0.0000</td><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td></tr><tr><td rowspan=\"3\">2</td><td>0.2729±</td><td>0.2097</td><td>0.2385</td><td>0.1740</td><td>0.1284</td><td>0.2477</td></tr><tr><td>0.0000</td><td>±0.0000</td><td>±0.0001</td><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td></tr><tr><td>0.4744</td><td>0.4074</td><td>0.4589</td><td>0.4780</td><td>0.4613</td><td>0.4992</td></tr><tr><td rowspan=\"3\">3 4</td><td>±.0011</td><td>±.0009</td><td>±0.0021</td><td>±0.0031</td><td>±0.0024</td><td>±0.0152</td></tr><tr><td>0.6370±</td><td>0.5195</td><td>0.6518</td><td>0.6573</td><td>0.5676</td><td>0.6460</td></tr><tr><td>0.0056</td><td>±0.0068</td><td>±0.0002</td><td>±0.0007</td><td>±0.3200</td><td>±0.0009</td></tr><tr><td>5</td><td>0.7652±</td><td>0.5790</td><td>0.7300</td><td>0.7784</td><td>0.6730</td><td>0.7329</td></tr></table></body></html>",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/d11490bcd988c40bf04e8eee36c4496f5c2d590b2baf53627ff2337a8f4eeefc.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td>0.0054</td><td>±0.0097</td><td>±0.0071</td><td>±0.0034</td><td>±0.0049</td><td>±0.0092</td></tr><tr><td>0.7213± 6</td><td>0.6725</td><td>0.7803</td><td>0.725</td><td>0.7743</td><td>0.7989</td></tr><tr><td>0.0005</td><td>±0.0022</td><td>±0.0077</td><td>0±0.0007</td><td>±0.0112</td><td>±0.0152</td></tr><tr><td>0.8810± 7</td><td>0.4673</td><td>0.9200±</td><td>0.8714</td><td>0.8079</td><td>0.8787</td></tr><tr><td>0.0022</td><td>±0.0098</td><td>0.0012</td><td>±0.0015</td><td>±0.0174</td><td>±0.0180</td></tr><tr><td>0.8560 8</td><td>0.6035</td><td>0.9165</td><td>0.8544</td><td>0.8684</td><td>0.8897</td></tr><tr><td>±0.0002</td><td>±0.0000</td><td>±0.014</td><td>±0.0008</td><td>±0.0139</td><td>±0.0081</td></tr><tr><td>0.7852± 9</td><td>0.7852</td><td>0.9018</td><td>0.7852</td><td>0.9192</td><td>0.9053</td></tr><tr><td>0.0000</td><td>±0.0000</td><td>±0.0006</td><td>±0.0000</td><td>±0.0028</td><td>±0.0068</td></tr><tr><td>0.9084 10</td><td>0.9084</td><td>0.9084</td><td>0.9084</td><td>0.9084</td><td>0.9087</td></tr><tr><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td><td>±0.0000</td></tr></table></body></html>",
        "page_idx": 4
    },
    {
        "type": "image",
        "img_path": "images/1273ca8149bc3c2587285e7f47b64cd537cd2eb49376ba4122b036d17d6c5dd7.jpg",
        "img_caption": [
            "图1UCI数据对比实验结果"
        ],
        "img_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "取100 次试验的聚类精度和方差的平均值,与基于多粒度决策不一致粗糙集的聚类融合算法（MGIDA）进行对比,使用聚类精度作为评价聚类效果的指标,得到如表4与图1的结果。由对比实验结果可以看出,MGIDA在第3、6、10数据集上取得了最优的聚类精度,在第2、4、5、7、8、9数据集取得了次优的聚类精度或十分接近（-0.0113,第4个数据集，-0.0323,第5个数据集，-0.0023,第7个数据集，），MGIDA明显地优于HGPA,只在第3个数据集上劣于MCLA,在第2、4、7、8、9数据集上优于MCLA,在其他数据集上劣于MCLA,且与该方法精度差距不大；MGIDA只在第4,5数据集上劣于IWCE，只在第9数据集上劣于DSCE.综上可知上本文的算法在每个数据集上都不是精度最差的算法。算法2使同一数据生成不同的弱划分本质上是在数据集上掺杂了不同程度的噪声，即通过扭曲数据的方式产生噪声,所以在这种含噪声数据的数据集上表现较好的算法具有较好的鲁棒性,本文的算法聚类精度即使不能取得最优,也可以取得或接近次优，说明本文的算法在各数据集上都有较好的表现，具有较好的鲁棒性。",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "算法2将多维数据随机地映射成一维数据，所以各算法在有的数据集上表现一般，对于分布较为复杂的数据集则表现较差，这是因为数据的扭曲使得生成的聚类成员不能很好地反映数据的真实分布所造成的.",
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/f71a68d9b51b3dce8e6298657e8bed8523985d89bc58a5a8174bd3970dcb4f88.jpg",
        "table_caption": [
            "表5时间效率对比表 /s"
        ],
        "table_footnote": [],
        "table_body": "<html><body><table><tr><td></td><td>CSPA</td><td>HGPA</td><td>MCLA</td><td>IWCE</td><td>DSCE</td><td>MGIDA</td></tr><tr><td>1</td><td>9.0171</td><td>0.4925</td><td>0.3501</td><td>396.9662</td><td>46.6548</td><td>1.044</td></tr><tr><td>2</td><td>73</td><td>0.7511</td><td>0.5719</td><td>332.7406</td><td>142.468</td><td>0.6406</td></tr><tr><td>3</td><td>0.5021</td><td>0.4521</td><td>0.4588</td><td>15.569</td><td>0.0818</td><td>0.1131</td></tr><tr><td>4</td><td>0.6138</td><td>0.5267</td><td>0.4719</td><td>18.7822</td><td>0.335</td><td>0.0521</td></tr><tr><td>5</td><td>0.4322</td><td>0.3861</td><td>0.414</td><td>1.8168</td><td>0.0434</td><td>0.0666</td></tr><tr><td>6</td><td>0.4252</td><td>0.4166</td><td>0.4299</td><td>1.7214</td><td>0.0336</td><td>0.0688</td></tr><tr><td>7</td><td>0.4614</td><td>0.4055</td><td>0.4211</td><td>1.8332</td><td>0.0718</td><td>0.0695</td></tr><tr><td>8</td><td>1.0599</td><td>0.4887</td><td>0.4075</td><td>4.3502</td><td>0.4854</td><td>0.0528</td></tr><tr><td>9</td><td>0.9147</td><td>0.4017</td><td>0.4132</td><td>44.731</td><td>0.3006</td><td>0.0515</td></tr><tr><td>10</td><td>25</td><td>0.6045</td><td>0.3639</td><td>2650</td><td>235.8422</td><td>0.4407</td></tr></table></body></html>",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "算法的时间效率如表5与图2所示。设每次运行时生成了h个聚类成员，每个聚类成员有 $K$ 个类，每个类中有 $p$ 个元素，使用多粒度粗糙集计算边界后， $\\begin{array} { r } { | B N | = | U | - n . } \\end{array}$ 则第一步计算相容度矩阵的复杂度为 $( h K p ) ^ { 2 }$ ，第二步Otsu 算法的时间复杂度$| U | \\log | U |$ ，第三步求解下近似和边界的比较次数为 $( \\mathrm { K p } ) ^ { 2 }$ ，第四步对边界域元素重新归类的时间复杂度不大于 $\\lvert U \\rvert ( \\lvert B N \\rvert ! )$ ，所以MGIDA的总的时间复杂度为 $0 \\left( ( \\mathrm { h K p } ) ^ { 2 } \\right) \\ + \\vert U \\vert \\log \\vert U \\vert +$ $( { \\mathrm { K p } } ) ^ { 2 } + | U | ( | B N | ! ) \\Big ) .$ 由时间复杂度可知论域大小和聚类类别数目是影响算法运行时间的最重要因素，且边界域较小时算法的时间复杂度较低。由表5与图2可得,MGIDA在4个数据集上有最好的时间效率，在除第1数据集外的其他数据集上均取得了次优的时间效率，算法的时间复杂度较小.值得注意的是MGIDA在小数据集的时间效率较高，在数据集的数据量增大时表现一般，如图2所示。",
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/d5cb4f143bffcc4dbaa058648dd3ae5e48b191b7d8970214e168822c09bb3d1e.jpg",
        "img_caption": [
            "图2时间效率对比图"
        ],
        "img_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4 结束语",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "本文首先提出了多粒度决策不一致粗糙集模型,进而提出了一种基于多粒度粗糙集的聚类融合算法,在一个新的视角下对聚类融合算法进行了研究,进行了数据实验以与其他聚类融合算法进行对比,使用聚类精度作为指标,验证了算法的有效性。从实验结果可以看出新的聚类融合算法在部分数据集上可以取得最优,不能取得最优时，也可以取得或接近次优。本文的算法具有较好的鲁棒性.在时间效率上,本文的算法在小数据集上也具有较大优势。",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "K-means算法对于非凸形分布的数据聚类效果不好，但基于本文的定理1,对于非凸形分布的数据本文所提出的聚类融合算法受数据分布的影响较小,改进本文的算法可以应用于非凸形分布的数据,将是一个值得研究的问题.",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "参考文献：",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "[1]Qian Yuhua,Liang Jiye,Yao Yiyu, et al.MGRS:a multi-granulation rough set [J].Information Sciences,2010,180(6): 949-970.   \n[2]Lin Guoping,Liang Jiye,Qian Yuhua.An information fusion approach by combining multigranulation rough sets and evidence model [J]. Information sciences,2015,314: 184-199.   \n[3]Antoine C, Cedric W,Pierre G,et al. Collaborative clustering: why, when, what and how [J]. Information Fusion,2018,39:81-95.   \n[4]阳琳资，王文渊．聚类融合方法综述[J].计算机应用研究,2005,22 (12): 8-10.(Yang Linbin,Wang Wenyuan.A survey of clustering fusion methods [J].Application Research of Computers,2005,22 (12): 8-10.)   \n[5]LiFeijiang, Qian Yuhua,Wang Jieting,et al. Multigranulation information fusion:a Dempster-Shafer evidence theory-based clustering ensemble method [J]. Information Sciences,2017,378: 389-409.   \n[6]谢岳山，樊晓平，廖志芳，等．一种基于图论的加权聚类融合算法[J]. 计算机应用研究,2013,30 (4):1015-1016.(Xie Yueshan,Fan Xiaoping, Liao Zhifang,et al.A graph-based weighted clustering ensemble algorithm [i].application research of computers,2013,30 (4): 1015-1016.)   \n[7]Fred A.Finding consistent clusters in data partitions [C]// Proc of International Workshop on Multiple Classifier Systems.Berlin: Springer, 2001: 309-318.   \n[8]Ayad H, Kamel M. Finding natural clusters using multi-cluster combiner based on shared nearest neighbors [C]// Proc of International Workshop on Multiple Classifier Systems.Berlin: Springer,2003: 166-175.   \n[9]Faceli K,De Souto MCP,De Araujo D SA,et al. Multi-objective clustering ensemble for gene expression data analysis [J]. Neurocomputing, 2009,72 (13): 2763-2774.   \n[10] Yi Hong,Kwong S,Wang Hanli，et al. Resampling-based selective clustering ensembles [J].Pattern Recognition Letters,20o9,30(3): 298-305.   \n[11] Nguyen N,Caruana R.Consensus clusterings [C]// Proc of the 7th IEEE International Conference on Data Mining. 20o7: 607-612.   \n[12] Otsu N.A threshold selection method from gray-level histograms [J]. IEEE Trans on Systems,Man,and Cybernetics,1979,9(1): 62-66.   \n[13] JainAK.Data clustering: 5O years beyond K-means [J].Pattern Recognition Letters,2010,31(8):651-666. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "[14]阳琳资，王路，卓晴，等．基于粗糙集理论的聚类融合加权迭代模型 [J].清华大学学报：自然科学版，2009(8):1106-1108.(YangLinbin, WangLu,Zhuo Qing,etal.Weighted iterationmodelofclustering fusion based on rough set theory [J].Journal of Tsinghua University: Natural Science Edition,2009 (8): 1106-1108.) ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "[15]Hu Jie,Li Tianrui,Wang Hongjun,et al.Hierarchical cluster ensemble model based on knowledge granulation [J].Knowledge-Based Systems, 2016,91 (C): 179-188. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "[16]Huang Dong,Wang Changdong,Lai Jianhuang.Locally Weighted Ensemble ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Clustering [J].IEEE Trans on Cybernetics,2018,48 (5):1460-1473. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "[17]KausarN,AbdullahA,SamirBB,et al.Ensemble clustering algorithm with supervised classification of clinical data for early diagnosis of coronary artery disease [J].Journal of Medical Imaging & Health Informatics,2016, In-Press. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "[18]Teng Geer,He Changheng,Xiao Jin,et al. Cluster ensemble framework based on the group method of data handling [J].Applied Soft Computing, 2016,43 (C): 35-46. ",
        "page_idx": 6
    }
]