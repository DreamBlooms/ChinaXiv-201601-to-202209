# 基于CRT机制混合神经网络的特定目标情感分析

孟威1,²，尉永清2,3，刘文锋1,4

(1.山东师范大学 信息科学与工程学院,济南250014;2.山东省分布式计算机软件新技术重点实验室，济南250014;3.山东警察学院 公共基础部,济南 250014;4.菏泽学院计算机学院,山东 菏泽 274015)

摘要：特定目标情感分析的目的是从不同目标词语的角度来预测文本的情感，关键是为给定的目标分配适当的情感词。当句子中出现多个情感词描述多个目标情感的情况时，可能会导致情感词和目标之间的不匹配。由此提出了一个CRT机制混合神经网络来用于特定目标情感分析，模型使用CNN层从经过 BiLSTM变换后的单词表示中提取特征，通过CRT组件生成单词的特定目标表示并保存来自 BiLSTM层的原始上下文信息。在三种公开数据集上进行了实验，实验结果表明，该模型在特定目标情感分析任务中较之前的情感分析模型在准确率和稳定性上有着明显的提升，证明CRT 机制能很好地整合CNN和LSTM的优势，这对于特定目标情感分析任务具有重要的意义。

关键词：特定目标情感分析；自然语言处理；深度学习；卷积神经网络；长短时记忆网络中图分类号：TP391 doi: 10.19734/j.issn.1001-3695.2018.08.0538

# Target-specific sentiment analysis based on CRT mechanism hybrid neural network

Meng Wei1,2, Wei Yongqing2,3†, Liu Wenfeng1,4 (1.School of Information Science& Engineeing，Shandong Normal UniversityJinan 25014,China;2.Shandong Provincial KeyLaboratoryforDistributedComputer SoftwareNovelTechnology,Jinan25o014,China;3.Basic Education Dept，Shandong PoliceColege,Jinan 250014,China; 4.Scholof Computer Science,Heze University,Heze Shandong 274015,China)

Abstract:The purpose of target-specific afective analysis is topredict the sentimentof atext from theperspectiveof diferent target words.The key is to assign appropriate afective words toa given target.When there are more than one afective worddescribing multiple targetsentiments inasentence,itmay lead tothe mismatch between the affective word and the target.In this paper,a hybrid neural network based onCRTmechanism is proposed for target-specific sentiment analysis.The model uses CNN layer to extract features from the word representation after BiLSTM transformation.The specific targetrepresentationof the word isgenerated by CRTcomponent and theoriginal context information from BiLSTMlayer is saved.Experiments on threeopen datasets show that the proposed model can significantly improve the accuracyand stabilityof target-specific affective analysis tasks compared with previous models.It is proved that the CRT mechanism in this papercanintegratetheadvantagesof CNNandLSTM well,which isof great significancetothe task of sentiment analysis for specific targets.

Key words:target-specificsentiment analysis;natural languageprocesing;deep learning;convolutional neuralnetwork; long short-term memory network

# 0 引言

随着互联网社交平台技术的快速发展，人们可以越来越方便使用网络上进行信息交流。用户在网络平台上发表意见，表达观点，这产生了大量带有个人情感色彩的短文本数据，从这些数据中提取有价值的信息成为了一项重要的研究工作。因此，利用自然语言处理来分析互联网文本的情感倾向已经成为研究热点之一[]。

近年来，随着深度学习在自然语言处理领域的应用，越来越多的科研人员尝试使用深度学习的方法来解决情感分析问题[2]。例如，Kim等人[3]使用卷积神经网络来解决文本情感分析问题。梁军等[4]利用深度学习的方法来处理微博情感分析问题，采用长短时记忆网络根据前后词语的关联性进行情感分析。取得了比以往研究更好的效果。

特定目标情感分析(target-level sentiment analysis）是情感分析研究工作中一项重要的子任务，目标是获取更深层次的情感信息。不同于常规的情感分析任务，特定目标情感分析极性的判别需要同时依赖特定目标的特征信息和文本的上下文信息。例如，“great food but the service wasdreadful!”,对于目标“food”是积极情感，对于目标“service”却是消极情感。所以，如果一句话表达了对多个目标的不同情绪，对于同一句话的情感分析可能出现两种截然相反的结果。

由此，研究人员将目光转向图像处理领域的注意力机制。注意力机制（attentionmechanism）源于对人类视觉的研究，人类会选择性地关注重要信息，同时忽略其他可见的信息[5]。在自然语言处理领域，加入注意机制的循环神经网络（RNN)，由Bahdanau等人[首先应用于机器翻译工作中，使注意力机制成功融入自然语言处理领域。Ma 等人[7]通过attention计算每个上下文单词和目标之间的语义相关性，然后使用注意力分数用于预测上下文特征，这类工作证明了注意力机制在自然语言分析领域的有效性。但是，基于注意力权重的词级特征用于分类可能引入噪声并降低预测准确度。例如，在“Thisdish is my favorite andIalways get itandnevergettiredofit."。这些方法往往涉及不相关的词，例如“never”和“tried”，当他们突出意见修饰语“favorite”。在某种程度上来说，这种缺点源于注意机制。

此外，目标的情绪通常由关键短语决定，例如“is myfavorite”。通过这种方式，CNNs-其用于提取信息最丰富的n-gram特征作为句子表示的能力已在Kim等人的工作中得到验证。但是，如果一句话表达了对多个目标的不同情绪，例如“great food but the service was dreadful!”，CNN 可能会束手无策。一个原因是CNN不能像基于RNN 的方法那样充分遍历目标信息，而且CNN很难区分多个目标的情感词。

本文提出了一种加入CRT机制的混合神经网络模型，以解决目标情感分类任务中的上述问题。模型首先将上下文信息编码为单词嵌入，并使用LSTM生成语境化的单词表示。为了将目标信息集成到单词表示中，本文加入了一种特定目标转换（target-level transformation,TLT）组件，用于生成特定目标的单词表示。与基于注意力机制的方法不同，TLT首先使用相同的目标表示来确定单个上下文单词的注意力分数，然后根据每个上下文单词生成不同的目标表示，然后将每个上下文单词与其相应的目标表示进行合并，以于获得变换后的单词表示。由于来自LSTM层的表示所携带的上下文信息将在非线性TLT变换之后丢失，因此本文设计了上下文信息保留机制对所生成的特定目标的单词表示进行上下文化处理。为了使CNN特征提取层能够更准确地定位情感信息，采用接近策略(proximity strategy)对卷积层的输入以及单词和目标之间的位置相关性进行缩放处理。

# 1 相关工作

特定目标情感分析作为情感分析领域一项重要的研究课题，它是通过结合上下文的信息对文本中特定目标进行情感极性判断，属于情感分析工作中的细分任务[8]。特定目标情感分析研究主要分为以下三类：

a)分别提取文本的特定目标和情感，并在之后将它们联系起来。句子的各个目标通常是用语言模型来提取的[9]，有监督的序列标注[10]或分类算法[II]。句子的情感通常分为一般情感分类方法，如基于规则的方法[12]、基于特征的分类器[13]或神经网络[14]。然而，这些方法只为句子分配一个情感极性，因此对于句子在两个目标表达不同意见的情况，不能产生正确的结果。

b)依赖于目标的情感分类，其目的是根据句子中提到的给定目标词推断句子的情感极性，通过添加一些特定目标特征[15]或设计特定的神经网络(Nguyen 等)结构来考虑特定目标词。然而，目标相关情感分类不能处理隐含的方面表达，并且也不能将目标词分组为目标类别。

c)特定目标情感分析最近比较流行的一个趋势，它利用注意力神经网络来预测给定一个目标的句子的情感极性。特别是Wang等人[16]提出了一种基于注意力的LSTM来预测给定方面类别的句子的情感极性，并达到最先进的性能，该模型涵盖了内隐和显式两个方面的表达，并自动将情感分为目标类别，克服了上述两类方法的缺点。然而该模型直接关注具有注意力层的目标的指定情感信息，当不相关的情感词对特定目标在语义上有意义时，这可能导致情感词和特定目标的不匹配。因此，本文提出了一个基于CRT 机制混合神经网络来解决这个问题。

# 2 基于CRT机制混合神经网络模型

基于CRT机制的混合神经网络旨在确定明显出现在句子中的特定目标的情感极性。例如，在"Iampleasedwiththe fast running speed,and the big screen size"这句话中，用户提到两个目标"running speed"和"screen size"，并表达出对它们的积极情绪。该任务通常被当做预测一对（目标，句子）的情感类别。

# 2.1模型概述

给定目标句子对 $( \boldsymbol { w } ^ { \tau } , \boldsymbol { w } )$ ，其中 $\mathbf { W } ^ { \tau } = \{ w _ { 1 } ^ { \tau } , w _ { 2 } ^ { \tau } , \cdots , w _ { m } ^ { \tau } \}$ 是$\mathbf { W } = \{ w _ { 1 } , w _ { 2 } , \cdots , w _ { n } \}$ 的子序列，相应的词嵌入序列是$\mathbf { X } ^ { \tau } { = } \{ \mathbf { x } _ { 1 } ^ { \tau } , \mathbf { x } _ { 2 } ^ { \tau } , \cdots , \mathbf { x } _ { m } ^ { \tau } \}$ 和 ${ \bf X } { = } \{ \mathbf { x } _ { 1 } , \mathbf { x } _ { 2 } , \cdots , \mathbf { x } _ { m } \}$ ，本文研究的目的是预测句子 $W$ 在目标 $\boldsymbol { W } ^ { \tau }$ 上的的情感极性 $y \in \{ P , N , O \}$ ，其中P、N和O分别表示“积极”、“消极”和“中性”的情感极性。

本文提出的基于CRT机制混合神经网络（CRThybridneural network，CRT-HNN)模型如图1所示。底层是word2vec层，它将输入句子转换为词向量的形式。中间层的底部是双向长短时记忆网络(BiLSTM)，它将输入$X = \{ x _ { 1 } , x _ { 2 } , \cdots , x _ { n } \} \in \mathbb { R } ^ { n \times \mathrm { d i m } _ { w } }$ 转换为上下文语境化的词语表示$\boldsymbol { u } ^ { ( 0 ) } = \{ u _ { 1 } ^ { 0 } , u _ { 2 } ^ { 0 } , \cdots , u _ { n } ^ { 0 } \} \in \mathbb { R } ^ { n \times { 2 } \mathrm { d i m } _ { u } }$ （即 BiLSTM 的隐藏表征)，其中$d i m _ { w }$ 和 $d i m _ { u }$ 分别表示词嵌入和隐藏表征的向量维数。中间层是CRT-HNN 模型的核心部分，长度为L的上下文保存转换(context-retention transformation,CRT)层。CRT 层包含了一种上下文保存机制，使用深层网络保留上下文信息并学习更抽象的单词级特征。最顶层的部分是卷积层特征提取层，它首先将单词和目标之间的位置相关性进行编码，然后提取用于分类的信息特征。

![](images/88aa66044242101d2ffe3145fb89706a592813e552ba8637a1748b43bbfe0857.jpg)  
图1模型图  
Fig.1Model diagram

# 2.2 Word2vec层

Word2vec[17]是Google 在 2013 年提出的开源的一款将词表示为实数值向量的高效工具。通过训练可以把对文本内容的处理简化为 $K$ 维向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度，它能很好的保存文本词语的关联信息。它的任务是将输入的句子转换为词嵌入形式。

# 2.3 双向长短时记忆网络层(BiLSTM)

将上下文信息与词嵌入相结合是在基于卷积的结构中表示单词的有效方式[18]。CRT-HNN 还使用 BiLSTM来累积输入句子的每个单词的上下文信息，即图1中中间层的底层部分。为了方便，本文将LSTM单元处理 $x _ { i }$ 的操作记为$\mathrm { L S T M } ( x _ { i } )$ 。因此，获取上下文语境化的词语表示$\boldsymbol { u } ^ { ( 0 ) } = \{ u _ { 1 } ^ { 0 } , u _ { 2 } ^ { 0 } , \cdots , u _ { n } ^ { 0 } \} \in \mathbb { R } ^ { n \times { 2 } \mathrm { d i m } _ { w } }$ 如下：

$$
\boldsymbol { u } _ { i } ^ { ( 0 ) } = [ L \stackrel { \right. } { S T M } ( x _ { i } ) ; L \stackrel { \left. } { S T M } ( x _ { i } ) ] , i \in [ 1 , n ] .
$$

# 2.4保存上下文信息的转换(CRT)

传统的基于注意力机制的方法保存的是静态词级特征，并将其与权重结合作为最终句子表示。相反，如图1中间部分所示，本文引入了多个CRT层，CRT的细节如图2所示。在每个CRT层中，加入一个特定目标变换(Target-SpecificTransformation,TST)组件，旨在更好地巩固目标词语表示,本文设计的保存上下文信息的转换机制，可以在深层神经网络结构中获取目标词语表示。

# 2.4.1特定目标变换单元

特定目标变换单元(TST)如图2所示，它用来生成目标词语的表示。之前的方法将目标词语的词嵌入平均化为目标表示[19]。不过这种方法在一些情况下可能不够合适，因为不同的目标词一般不会对整个句子产生相同的情感贡献。例如，在目标句子“NVIDIATitanicGraphics”中，词语“Graphics"比"NVIDIA"和"Titanic"更为重要，因为情感通常在句子头上传递（即，“Graphics")，一般不会超过修饰词（例如品牌名称“NVIDIA")。Ma等人尝试通过计算每个目标词表示与平均句子向量之间的重要性分数来解决这个问题。但是，这对于表达多个情感的句子来说，它可能无效（例如，“Iphone8 runs fast，but the battery life is tooshort.”)，因为取平均值通常会抵消掉不同倾向的情感。本文设想根据每个句子的单词而不是整个句子来动态地计算目标词的重要性。首先使用一个BiLSTM获得目标词表示$\boldsymbol { u } ^ { \tau } \in \mathbb { R } ^ { n \times 2 \mathrm { d i m } u }$ ：

$$
\begin{array} { r } { u _ { j } ^ { \tau } = [ L S \vec { T } M ( x _ { j } ^ { \tau } ) ; L S \overleftarrow { T } M ( x _ { j } ^ { \tau } ) ] , j \in [ 1 , m ] . } \end{array}
$$

之后，动态地将它们与句子中的每个单词 $w _ { i }$ 相关联，以便在时间步长 $i$ 中制定目标词表示 $r _ { i } ^ { \tau }$ ：

$$
r _ { i } ^ { \tau } = \sum _ { j = 1 } ^ { m } u _ { j } ^ { \tau } * \Gamma ( u _ { i } ^ { ( l ) } , u _ { j } ^ { \tau } ) .
$$

方程 $\Gamma$ 用来度量第 $j$ 个目标词表示 $\boldsymbol { u } _ { j } ^ { \tau }$ 和第 $i$ 个词级表示$u _ { i } ^ { ( l ) }$ 之间的关联性：

$$
\Gamma ( u _ { i } ^ { ( l ) } , u _ { j } ^ { \tau } ) = \frac { \exp ( u _ { i } ^ { ( l ) \mathrm { T } } u _ { j } ^ { \tau } ) } { \sum _ { k = 1 } ^ { m } \exp ( u _ { i } ^ { ( l ) \mathrm { T } } u _ { k } ^ { \tau } ) } .
$$

最后， $\boldsymbol { u } _ { i } ^ { \tau }$ 和 $u _ { i } ^ { ( l ) }$ 以串联的形式送入全连接层来获得第 $i$ 个特定目标单词的表示 $\tilde { u } _ { i } ^ { ( l ) }$ ：

$$
\tilde { u } _ { i } ^ { ( l ) } = h ( W ^ { \tau } [ u _ { i } ^ { ( l ) } : r _ { i } ^ { \tau } ] + b ^ { \tau } ) ,
$$

其中： $h ( * )$ 是非线性激活函数，“："代表向量拼接操作。  
$\boldsymbol { W } ^ { \tau }$ 和 $b ^ { \tau }$ 代表全连接层的权重矩阵。

![](images/f190c2b1972b86b8374d95c676b6d7da98ec85b42b6f026194e55f3062c810cf.jpg)  
图2 CRT细节图Fig. 2 CRTdetails

# 2.4.2上下文信息保留机制

在非线性变换单元处理之后，由BiLSTM层所捕获的上下文信息将丢失，因为特征向量内的特征均值和方差将被改变。为了利用上下文的信息，本文研究了两种策略：无损传输(lossless conveying,LC)和自适应缩放(adaptive zooming,AZ)，可以将上下文信息传递到每个后续层，如图2中无损转发/自适应缩放单元所示。因此，本文模型被命名为TLH-Net-LC和 TLH-Net-AZ。

a)无损传输。该策略通过在转换之前将特征直接馈送到下一层来保存上下文信息。具体来说，就是将针对特定目标转换层中第 $l { + } l$ 层的输入 $u _ { i } ^ { l + 1 }$ 公式为

$$
u _ { i } ^ { l + 1 } = u _ { i } ^ { ( l ) } + \tilde { u } _ { i } ^ { ( l ) } , i \in [ 1 , n ] , l \in [ 0 , L ] ,
$$

其中： $u _ { i } ^ { ( l ) }$ 是第 $\mathbf { \xi } _ { l }$ 层的输入， $\tilde { u } _ { i } ^ { ( l ) }$ 是针对非线性转换(NT)层的输出。将式（6）以递归形式展开如下：

$$
u _ { i } ^ { l + 1 } = u _ { i } ^ { 0 } + N T ( u _ { i } ^ { ( 0 ) } ) + \cdots + N T ( u _ { i } ^ { ( l ) } ) .
$$

这里，将 $\tilde { u } _ { i } ^ { ( l ) }$ 记为 $N T ( u _ { i } ^ { ( l ) } )$ 。从式（7）可以看到，每个层的输出将包含词语的上下文信息（ $u _ { i } ^ { ( 0 ) }$ )，因此上下文信息被编码到变换的特征中。称这种策略为“无损转发”，因为在特征组合期间，上下文信息表示和针对特定目标转换表示（ $N T ( u _ { i } ^ { ( l ) } )$ )保持不变。

b)自适应缩放。无损转发可以通过直接将上下文信息特征添加到转换特征来引入上下文信息，为了使输入和转换特征的权重可以动态地调整，本文提出了另一种策略，称为“自适应缩放”。类似于长短时记忆网络中的门结构，自适应缩放引入了门控函数来控制输入特征和转换特征的通过比例。门控函数 $g ^ { ( l ) }$ 为

$$
g _ { i } ^ { ( l ) } = \sigma ( W _ { t r a n s } u _ { i } ^ { ( l ) } + b _ { t r a n s } ) ,
$$

其中： $g ^ { ( l ) }$ 是第 $\mathbf { \xi } _ { l }$ 个CRT层第 $i$ 个输入的门， $\sigma$ 是sigmoid激活函数。然后，基于门控函数对 $g ^ { ( l ) }$ 和 $\tilde { u } _ { i } ^ { ( l ) }$ 进行凸线性组合：

$$
u _ { i } ^ { ( l + 1 ) } = g _ { i } ^ { ( l ) } \odot \tilde { u } _ { i } ^ { ( l ) } + ( 1 - g _ { i } ^ { ( l ) } ) \odot u _ { i } ^ { ( l ) } .
$$

这里， $\odot$ 记为元素乘法。该等式的非递归形式如下：

$$
u _ { i } ^ { ( l + 1 ) } = [ \prod _ { k = 0 } ^ { l } ( 1 - g _ { i } ^ { ( k ) } ) ] \odot u _ { i } ^ { ( 0 ) } + [ g _ { i } ^ { ( 0 ) } \prod _ { k = 1 } ^ { l } ( 1 - g _ { i } ^ { ( k ) } ) ] \odot N T ( u _ { i } ^ { ( 0 ) } ) +
$$

$$
\cdots + g _ { i } ^ { l - 1 } ( 1 - g _ { i } ^ { ( l ) } ) \odot N T ( u _ { i } ^ { ( l - 1 ) } ) + g _ { i } ^ { ( l ) } \odot N T ( u _ { i } ^ { ( l ) } ) .
$$

因此，上下文信息被集成在每个上层中，并且上下文特征和变换特征的比例由不同变换层中的计算门控制。

# 2.5卷积特征提取层

影响CNN处理效果的一个原因是可以将目标词语与不相关的一般意见词相关联，这些意见词通常被用做跨领域不同目标词语的修饰词。例如，在句子“Greatfoodbut theserviceisdreadful”中，词语“service”可能会同时与“Great"和“dreadful"联系起来。为了解决这个问题，本文引入了一种邻近策略，这个想法是一个更接近目标词语的意见词更可能是目标词语的实际修饰词。

具体来说，本文首先计算第 $i$ 个词和目标句子之间的位置相关性 $\mathbf { \Phi } _ { \nu _ { i } }$ (实际操作中，索引 $i$ 的长度可能会大于句子的实际长度 $n _ { - }$ ）：

$$
\nu _ { i } = \left\{ \begin{array} { c c } { { 1 - \displaystyle \frac { ( k + m - i ) } { C } } } & { { i < k + m } } \\ { { 1 - \displaystyle \frac { i - k } { C } } } & { { k + m \leq i \leq n } } \\ { { 0 } } & { { i > n } } \end{array} \right.
$$

其中： $k$ 是第一个目标词的索引， $\mathbf { \Psi } _ { c }$ 是预先设定好的常量。$m$ 是目标句子的表示矩阵 $\boldsymbol { w } ^ { \tau }$ 的长度，可以利用 $\mathbf { \sigma } _ { \nu }$ 来辅助CNN确定给定目标的正确情感：

$$
\hat { u } _ { i } ^ { ( l ) } = u _ { i } ^ { ( l ) } * \nu _ { i } , i \in [ 1 , n ] , l \in [ 1 , L ] .
$$

靠近目标的词语将被突出，远离目标的词重视程度将会降低。将 $\nu$ 应用在中间层的输出上，以便将位置信息引入每个CRT层；然后将加权的 $u ^ { ( L ) }$ 输入到卷积层，生成如下特征映射 $c \in \mathbb { R } ^ { n - s + 1 }$ ：

$$
\mathbf { c } _ { i } = \mathbf { R e } L U ( w _ { c o n \nu } ^ { \mathrm { T } } u _ { i : i + s - 1 } ^ { ( L ) } + b _ { c o n \nu } ) ,
$$

其中： $u _ { i : i + s - 1 } ^ { ( L ) } \in \mathbb { R } ^ { s \bullet \dim _ { u } }$ 是 $\hat { u } _ { i } ^ { ( L ) } , \cdots , \hat { u } _ { i + s - 1 } ^ { ( L ) }$ 的连接向量， $s$ 是卷积核的大小。 $w _ { c o n \nu } \in \mathbb { R } ^ { s \bullet \mathrm { d i m } _ { h } }$ 和 $b _ { c o n \nu } \in \mathbb { R }$ 是卷积核可学习权值。为了捕捉最翔实的特征，本文采用maxpooling并通过使用 $n _ { k }$ 内核来获得句子表示 $z \in \mathbb { R } ^ { n _ { k } }$ ：

$$
\boldsymbol { z } = [ \operatorname* { m a x } ( \boldsymbol { c } _ { 1 } ) , \cdots , \operatorname* { m a x } ( \boldsymbol { c } _ { n _ { k } } ) ] ^ { \mathrm { T } } .
$$

最终，将 $\textbf { z }$ 输入到全连接层进行情感分析：

$$
p ( y \vert w ^ { \mathrm { T } } , w ) = S o f t \operatorname* { m a x } ( W _ { f } z + b _ { f } ) .
$$

其中： $W _ { f }$ 和 $b _ { f }$ 是可学习参数。

# 3 实验及结果分析

# 3.1数据集

本文选择三种公开数据集进行实验，其中两个数据集来自SemEval2016任务5数据集中的laptop 和 restaurant 领域，它主要用于细粒度情感分析，每个领域的数据集都分为训练数据和测试数据，本文删除了数据集中的冲突类别。本文对restaurant 数据集设置了五个目标类别{food，service,price，environment,atmosphere}，对loptop 数据集设置了五个目标类别{prince，reliability，runningspeed，batterylife,screen}。第三个是Dong 等人[20]收集的 Twitter数据集。所有数据都是在不删除停用词、符号或数字的情况下进行的，因为停用词中也可能含有某些情感情绪，并且句子使用零填充的方式以达到数据集中的最长句子的长度。实验使用数据统计如表1所示，评价指标是分类准确率（accuracy）和召回率（recall）。

表1实验数据统计  
Table 1 Statistics of experimental data   

<html><body><table><tr><td>数据集</td><td></td><td>积极</td><td>消极</td><td>中性</td></tr><tr><td rowspan="2">Restaurant</td><td>训练集</td><td>2159</td><td>805</td><td>643</td></tr><tr><td>测试集</td><td>726</td><td>196</td><td>196</td></tr><tr><td rowspan="2">Laptop</td><td>训练集</td><td>980</td><td>870</td><td>464</td></tr><tr><td>测试集</td><td>352</td><td>128</td><td>169</td></tr><tr><td rowspan="2">Twitter</td><td>训练集</td><td>1560</td><td>1560</td><td>3125</td></tr><tr><td>测试集</td><td>173</td><td>173</td><td>345</td></tr></table></body></html>

# 表2实验数据样例

Table 2Sample experimental data   

<html><body><table><tr><td colspan="2">数据集</td></tr><tr><td rowspan="3">semeval</td><td>Great food but the service was dreadful!</td></tr><tr><td><targetTerms> <targetTerm ="food"polarity="positive"></td></tr><tr><td><targetTerm="service"polarity="neutral"></td></tr><tr><td>twitter</td><td></targetTerms> windows is K than ios.</td></tr></table></body></html>

# 3.2超参数设置

本文使用预先训练好的word2vec词向量来初始化词嵌入模型，向量维度设置为300维。为了减轻过拟合，对LSTM的输入字嵌入和最终句子表示Z应用了Dropout。所有权重矩阵用均匀分布U（-0.01,-0.01）初始化，偏差初始化为零。训练目标是交叉熵损失函数，并且以Adam[21作为优化工具，使用原始文献中的学习率和衰减率。模型超参数设置如表3所示。

表3实验参数 Table 3Experimental parameters   

<html><body><table><tr><td colspan="2">超参数设置</td></tr><tr><td>dimw</td><td>300</td></tr><tr><td>dimu</td><td>50</td></tr><tr><td>Dropout rates (Ptstm Psen)</td><td>(0.3,0.3)</td></tr><tr><td>L</td><td>3</td></tr><tr><td>Batch size</td><td>64</td></tr><tr><td>S</td><td>3</td></tr><tr><td>n</td><td>100</td></tr><tr><td>C</td><td>30</td></tr></table></body></html>

# 3.3模型对比实验

在词向量300 维的基础上，本文选择与基于传统机器学习的 SVM 模型和基于深度学习的模型进行对比。模型具体方法设计如下：

a $\mathrm { | { S V M ^ { [ 2 2 ] } } }$ 。Kiritchenko 等人提出的传统基于特征的SVM分类模型，它采用了一系列人工标注的数据对模型进行训练，该模型取得了比以往研究更好的分类效果。

b)CNN。基于Kim提出的卷积神经网络模型，该模型使用预训练的词向量作为输入，输入可被视为静态或非静态，通过卷积操作提取文本的情感特征，然后使用分类器进行分类。由于该模型是最基本的CNN模型，它没有结合特定目标的注意力机制，无法在训练过程中高度关注特定目标情感信息。例如，模型将“Great foodbut the servicewasdreadful!”中的“food”和“service”都判断为积极的情感倾向。

c)ATT-CNN。基于Wang等人[23]提出的多层注意力机制卷积神经网络，可以使模型在训练过程中高度关注特定目标的情感信息，它将词向量作为注意力机制构建网络的输入矩阵，形成单注意力机制。

d)ATT-LSTM。Wang等人提出的基于注意力机制的LSTM网络，该模型加入了特定目标的注意力信息，使用预训练的word2vec词向量作为输入来训练模型，该模型在五种特定目标的情感分类中取得了比传统LSTM网络更好的分类效果。

e)TD-LSTM。文献[24]中使用两个LSTM分别对目标的左右两侧文本进行建模，并将来自两个方向的最后隐藏状态连接为用于情感分类的情感特征，然后根据级联的上下文

表示进行预测。

f)BILSTM-ATT-G。文献[25]中使用两个基于注意力机制的LSTM对目标左右两侧的上下文信息进行建模，以在句子上提取单词的分布式表示，然后将注意力应用于隐藏节点以估计每个单词的重要性，并引入门函数来计算左、右两侧上下文和整个句子的重要性以用于预测。

如表4所示，CRT-HNN-LC和CRT-HNN-AZ 始终在所有数据集上达到最佳性能，这验证了整个CRT-HNN模型的有效性。

表4实验结果  
Table 4Experimental results   

<html><body><table><tr><td rowspan="2">模型</td><td>laptop</td><td>restaurant</td><td>twitter</td></tr><tr><td>准确率/召回率</td><td>准确率/召回率</td><td>准确率/召回率</td></tr><tr><td>SVM</td><td>71.33/70.89</td><td>75.76/73.12</td><td>*/*</td></tr><tr><td>CNN</td><td>65.67/64.78</td><td>69.90/70.24</td><td>74.64/75.11</td></tr><tr><td>ATT-CNN</td><td>72.67/71.41</td><td>78.23/76.43</td><td>75.67/75.33</td></tr><tr><td>LSTM</td><td>73.32/74.13</td><td>76.34/76.44</td><td>68.13/67.89</td></tr><tr><td>TD-LSTM</td><td>76.25/75.68</td><td>77.88/78.03</td><td>69.10/68.67</td></tr><tr><td>BILSTM-ATT-G</td><td>77.34/77.65</td><td>79.54/78.65</td><td>70.12/69.43</td></tr><tr><td>HNN-LC</td><td>81.67/82.13</td><td>85.46/85.78</td><td>76.46/76.89</td></tr><tr><td>HNN-AZ</td><td>82.13/81.72</td><td>85.50/85.61</td><td>77.28/76.65</td></tr></table></body></html>

对CNN和ATT-CNN进行深入分析可以发现，没有加入任何注意力机制的CNN模型将大量句子中的不同目标词语判别为相同的情感极性，它不能针对特定目标提取出更详细的的特征信息，所以无法准确判断一个句子中不同目标的情感极性。ATT-CNN在三个数据集上的分类效果优于传统的 SVM模型，原因是它加入了特定目标注意力机制，可以使模型在训练的过程中通过注意力机制对特定目标的情感信息进行关注和学习，可以针对特定的目标词语作出相应的情感极性判断，这证明了特定目标注意力机制对于CNN模型的有效性。此外，基于CNN的模型在语法结构不够正式的Twitter数据集中取得了不错的效果，原因是CNN模型在特征提取方面存在着优势，可以在相对混乱的句子中提取出重要的情感特征。

对比SVM、LSTM、TD-LSTM和BILSTM-ATT-G四种模型可以发现，LSTM的模型与传统的SVM模型相比在Restaurant 和Loptop上也取得了更好的结果，原因是基于序列信息的LSTM模型通过捕获更有用的上下文特征能够很好地处理相对正式的句子。并且，使用两个LSTM分别对目标的左右两侧文本进行建模的TD-LSTM和BILSTM-ATT-G模型也取得更加优秀的结果，原因是它可以将来自两个方向的最后隐藏状态连接为用于情感分类的情感特征，然后根据级联的上下文表示进行预测。可以观察到，加入了注意力机制的BILSTM-ATT-G对比TD-LSTM性能提升有限，证明对于LSTM 模型增加注意力机制并不是一个最合适的方法。此外，在面对带有更多不合语法的句子的Twitter数据集，基于LSTM的模型表现不如它在Restaurant和Loptop 数据集上令人满意，究其原因是基于LSTM的模型依赖序列信息，语法不规范的文本限制了这类模型获取上下文特征的能力。由此可以看出，依赖顺序信息的LSTM模型在标准数据上能够捕获上下文信息，获得较好的表现。

对比基于 CNN 的模型和基于LSTM 的模型在 Twitter数据集上的表现可以发现，对于不合语法的文本，基于CNN的模型可能具有一些优势，因为CNN旨在提取信息最丰富的n-gram 特征，因此对顺序信息较弱的非正式文本不太敏感。而基于序列信息的LSTM模型过度依赖序列信

息，所以表现不如CNN模型。

将基于CNN的模型和基于LSTM的模型与本文提出的HNN-LC和HNN-AZ可以看出，本文的模型通过CRT机制将CNN在特征提取和LSTM在上下文信息提取上的优势进行了整合，弥补了CNN和LSTM存在的缺陷，在语法较为标准的数据集上和语法相对混乱的数据集上都取得了令人满意的效果。

# 3.4CRT层数对模型的影响

由于本文的模型引入了多个CRT层，下面将研究CRT层数对于模型预测精度的影响。实验在SemEval数据集上进行， $L \in \{ 1 , 1 0 \}$ 。实验结果如图3所示，可以看出，在CRT层数为3的时候，模型性能最佳。

![](images/63fcf84a7e2d6fd83abd6a19c2e678156e9b4a8c9c2ce237552a907925640cda.jpg)  
图3CRT层数对模型的影响  
Fig.3The impact of the number of CRT layers on the model

# 4 结束语

本文深入研究了目标词语情感分析中注意力机制的缺点，探讨了LSTM 和CNN 模型在这类任务中的优势和不足。本文的CRT-HNN模型很好的将LSTM保存上下文信息的优势和CNN在特征提取中的优势相结合，在解决不同数据类型的问题上取得了比以往的模型更加稳定的表现。实验结果证明本文模型在多种数据集上取得了比先前模型更为优秀的结果。针对CRT机制有效性的实验也证明了CRT-HNN模型架构的合理性。

# 参考文献：

[1]王仲远,程健鹏，王海勋,等.短文本理解研究[J].计算机研究与发展， 2016,53(2):262-269.(Wang Zhongyuan，Cheng Jianpeng，Wang Haixun,et al. Short text understanding:a survey [J].Journal of Computer Research and Development,2016,53(2):262-269.)   
[2]余凯，贾磊,陈雨强，等.深度学习的昨天、今天和明天[J].计算机研 究与发展,2013,50(9):1799-1804.(Yu Kai,Jia Lei,Chen Yuqiang, et al.Deep learning:yesterday, today,and tomorrow [J].Journal of Computer Research and Development,2013,50(9):1799-1804.)   
[3]Kim Y.Convolutional neural networks for sentence classification [J]. Eprint Arxiv, 2014.   
[4]梁军，柴玉梅，原慧斌，等.基于极性转移和LSTM递归网络的情感 分析[J].中文信息学报．2015,29(5):152-159.(Liang Jun,Chai Yumei，Yuan Huibin，et al.Polarity shifting and LSTM based recursive networks for sentiment analysis [J].Journal of Chinese Information Processing,2015,29(5):152-159.)   
[5]KosiorekA.神经网络中的注意力机制[J].机器人产业，2017(6): 12-17.(Adam Kosiorek.Attention mechanism in neural networks [J]. Robot Industry,2017(6):12-17.)   
[6]Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate [J].Computer Science,2014.   
[7]Ma Dehong,Li Sujian,Zhang Xiaodong,et al.Interactive attention networks for aspect-level sentiment classification [C]//Proc of the 26th International Joint Conference on Artificial Intelligence.2017: 4068-4074.   
[8]Pontiki M,Galanis D,Pavlopoulos J,et al.SemEval-2O14 task 4: aspect based sentiment analysis [C]//Proc of International Workshop on Semantic Evaluation.2014: 27-35.   
[9]Qiu Guang,Liu Bing,Bu Jiajun,et al. Opinion word expansion and target extraction through double propagation [J].Computational Linguistics,201,37 (1): 9-27.   
[10] Cheng Jiajun, Zhang Xin,Li Pei,et al. Exploring sentiment parsing of microblogging texts for opinion polling on chinese public figures [J].Applied Intelligence,2016,45(2): 429-442.   
[11] Wu Haibing,Gu Yiwei,Sun Shangdi,et al.Aspect-based opinion summarization with convolutional neural networks [C]//Proc of International Joint Conference on Neural Networks.IEEE,2016.   
[12] Bollegala D,Weir D,Carroll J.Cross-domain sentiment classification using a sentiment sensitive thesaurus [J]. IEEE Trans on Knowledge & Data Engineering,2013,25 (8): 1719-1731.   
[13] Kiritchenko Svetlana,Zhu Xiaodan,Mohammad S M.Sentiment analysis of short informal texts [M].AI Access Foundation,2O14,50: 723-762.   
[14]Nguyen TH,Shirai K.PhraseRNN:phrase recursive neural network for aspect-based sentiment analysis [C]//Proc of Conference on Empirical Methods in Natural Language Processing.2015:2509-2514.   
[15] Zhou Ming. Target-dependent Twitter sentiment classification [C]/Proc of Annual Meeting of the Association for Computational Linguistics Human Language Technologies.2O11:151-160.   
[16]Wang Yequan,Huang Minlie,Zhu Xiaoyan,et al.Attention-based LSTMforaspect-level sentimentclassification[C]//Procof Conference on Empirical Methods in Natural Language Processing. 2017:606-615.   
[17]唐晓丽，白宇,张桂平,等.一种面向聚类的文本建模方法[J].山西大 学学报：自 然科学版,2014，37(4):595-600.(Tang Xiaoli,Bai Yu, Zhang Guiping,et al.A Text Modeling Method for Clustering [J]. Journal of Shanxi University,2014,37(4): 595-600.)   
[18] Lai Siwei,Xu Liheng,Liu Kang，et al．Recurrent convolutional neural networks for text classification[C]//Proc of the 29th AAAI Conference on Artificial Intelligence.AAAI Press,2015.2267-2273.   
[19] Chen Peng,Sun Zhongqian,Bing Lidong,et al.Recurrent Attention Network on Memory for Aspect Sentiment Analysis [C]//Proc of Conference on Empirical Methods in Natural Language Processing. 2017: 452-461.   
[20] Dong Li,Wei Furu,Tan Chuanqi,et al.Adaptive recursive neural network fortarget-dependent twittersentiment classification [C]//Proc of Meeting of theAssociation for Computational Linguistics.2014: 49-54.   
[21] Kingma D P,Ba Lei J.Adam:a method for stochastic optimization [J]. Computer Science,2014.   
[22] Kiritchenko S,Zhu Xiaodan,Cherry C,et al.NRC-Canada-2014: detecting aspects and sentiment in customer reviews [C]//Proc of International Workshop on Semantic Evaluation.2014:437-442.   
[23] Wang Linlin,Cao Zhu,de Melo Gerard,et al.Relation classification viamulti-level attention CNNs [C]//Proc of Meeting of the Association for Computational Linguistics.2016:1298-1307.   
[24] Tang Duyu,Qin Bing,Feng Xiaocheng,et al.Effective LSTMs for Target-Dependent Sentiment Classification [J]. Computer Science, 2016.   
[25] Liu Jiangming,Zhang Yue,et al. Attention modeling for targeted sentiment [C]//Proc of Conference of the European Chapter of the Association for Computational Linguistics:Volume 2,Short Papers. 2017: 572-577.