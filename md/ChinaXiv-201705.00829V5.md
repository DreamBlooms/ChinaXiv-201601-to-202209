# Entropy-partition of Complex Systems and Emergence of Human Brain's Consciousness

Guangcheng Xi

(State KeyLaboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190,P.R.China)

Abstract.The three frameworksfor theories of consciousness taken most seriously by neuroscientists are that consciousness is a biological state of the brain,the global workspace perspective,and the perspective of higher state. Consciousness is discussed from viewpoint of theory of Entropy—partition of complex system in present article.Human brain's system self-organizably and adaptivelyimplementspartition 、aggregationandintegration， and consciousness emerges.The Gibss representation of consciousnessis proved and That consciousness originates from quantum mechanical processes of brain activity is explained by means of SW entropy.

Key words: Human brain'ssystem; Entropy—partition (aggregation);   
Integration; consciousness; emerges； Gibbs ; SW entropy.

# 1.Intrduction

The three frameworksfor theories of consciousness taken most seriously by neuroscientists are that consciousness is a biological state of the brain ,the global workspace perspective,and the perspective of higher state.In particular,in the article of Giulio Tononi and G.M. Edelman[1]， studyingof consciousness is concentrated on description of kinds of neural processes which can account for key properties of conscious experience. Theyhaveemphasizedtwokeypropertiesof consciousness: conscious experience is integrated (each conscious scene is unified) and at the same time it is highly differentiated (within a short time,one can experience any of a huge number of different conscious states)，and have proposed notions so-called functional clustering，neural complexity and dynamical core hypothesis. Those researching methods and train of thought to problem of consciousness agree to a certain extent, with theory and method of Entropy-partition of Complex System I proposed 31 years ago [2]. At that time,related specialists (appraising group of achievements in scientific research) ever have pointed out: theory of Entropy-partition (Aggregation） of complex system not only has been applied to Ecologico-Economical regionalization [3] of Ecologico-Economico-social complex system but also will certainly be applied to researching of human Brain's neural system. Therefore having read the article of G. Tononi and G.M. Edelman,Ihave much sensation seem to have met before,and excited feeling arises spontaneously. Though such,we strongly have felt it still necessary to discuss problem of consciousness from viewpoint of theory of entropy-partition (aggregation) of complex system.

# 2. Entropy-partition (aggregation) of neural system $X$

Suppose neural system $\boldsymbol { X } = ( X _ { 1 } , X _ { 2 } , \cdots , X _ { a } , \cdots , X _ { p } ) ^ { T }$ is consisted of $p$ neuron, $p \in N$ （ $N$ set of natural number)，where $X _ { a } = ( X _ { a _ { i } } )$ ， （204号 $a = 1 , 2 , ^ { , . . . , p } \quad ; \quad i = 1 , 2 , ^ { . . . , q } \quad \cdot \quad \mathrm { L e t } \quad C _ { _ a } ( a = 1 , ^ { . . . , p ) }$ beset of classification of $X _ { a }$ ， $C _ { a _ { i } } = i$ be $i$ -th element of $C _ { a }$ ，then we have $C _ { a } = \{ 1 , 2 , \cdots , i ^ { \ldots } , k \}$ ， $k \leq q$ ,and let $n _ { i }$ be quantity for $X _ { a }$ belong to $i$ 1 th class, then entropy of $X _ { a }$ is defined as

$$
H ( X _ { _ a } ) = - \sum _ { i = 1 } ^ { k } { n _ { i } } / q \log { n _ { i } } / q
$$

joint entropy of $X _ { a } , X _ { b }$ is similarly defined as

$$
H ( X _ { a } \bigcup X _ { b } ) = - { \sum _ { i } \sum _ { j } } n _ { i j } / q \log n _ { i j } / q
$$

where $n _ { i j }$ is quantity for $X _ { a }$ belong to $i$ -th class of $C _ { a }$ simultaneously $X _ { b }$ belong to $j$ -th class of $C _ { b }$ . For the convenience of application, expressions (1) and (2) can respectively be represented as

$$
H ( X _ { a } ) = \log q - { \frac { 1 } { q } } { \sum _ { i = 1 } ^ { k } n _ { i } \log n _ { i } }
$$

$$
H ( X _ { _ a } \bigcup X _ { _ b } ) = \log q - { \frac { 1 } { q } } { \sum _ { i } \sum _ { j } n _ { i j } \log n _ { i j } }
$$

Having had above-mentioned definition of entropy, in what follows, correlative measure by which statistical dependence between the $X _ { a }$ and the $X _ { b }$ is denoted is defined by their mutual information.

Definition 1. Suppose $X _ { a } \cap X _ { b } = \phi$ , then entropy

$$
H ( X _ { a } , X _ { b } ) = H ( X _ { a } ) + H ( X _ { b } ) - H ( X _ { a } \bigcup X _ { b } )
$$

is called correlative measure $\mu ( X _ { a } , X _ { b } )$ between the $X _ { a }$ and the Xb

Definition 2. Suppose $X _ { a } \cap X _ { b } = \phi$ for arbitrary $a , b ( a \neq b )$ ,then

$$
\mu ( X _ { 1 } , X _ { 2 } , \cdots , X _ { p } ) { = } \sum _ { a = 1 } ^ { \overset { \Delta } { \sum } } H ( X _ { a } ) - H { \left( \sum _ { a = 1 } ^ { p } X _ { a } \right) }
$$

is called correlative measure among $X _ { _ 1 } , \quad X _ { _ 2 } , \quad \ldots ,$ and $X _ { _ p }$

Definition 2: Suppose system $X$ be partitioned into $m$ subsystems （204号 $s _ { 1 } , s _ { 2 } , \cdots , s _ { m }$ , for arbitrary $i , j \ ( i \neq j ) , s _ { i } \cap s _ { j } = \phi , X = \sum _ { i = 1 } ^ { m } s _ { i }$ ，then

$$
\mu ( s _ { 1 } , s _ { 2 } , \cdots , s _ { m } ) { \stackrel { \Delta } { = } } \sum _ { i = 1 } ^ { m } H ( s _ { i } ) - H \left( \sum _ { i = 1 } ^ { m } s _ { i } \right)
$$

is called correlative measure among S1,S2,··,Sm

Let us consider nonempty finite set $X$ and set-family $E \left( \boldsymbol { X } \right)$ consisted of its subset, $P _ { e }$ a set-function defined on $E \left( \boldsymbol { X } \right)$ with properties

(i) $P _ { e } ( A ) \geq 0 , \forall A \in { \cal E } ( X ) ,$ （204号 (ii) $P _ { e } ( \phi ) = 0$

Definition 3. If for arbitrary nonempty finite set $S _ { i } \in E \left( X \right)$ $S _ { j } \in E ( X ) , i \neq j , S _ { i } \cap S _ { j } = \phi$ ，have

$$
P _ { e } ( S _ { i } \bigcup S _ { j } ) \geq P _ { e } ( S _ { i } ) + P _ { e } ( S _ { j } )
$$

then,set-function $P _ { e }$ satisfied conditions (i),(ii) is called superadditive.

Although finite superadditivities of mutual information or the socalled measure of cohesion of the components of the set of entities is well known result, it is not still unnecessary to write down our proof about finite,in particular, countably superadditivity of the correlative measure among some subsystems of the system $X$ which is given.

Theorem[2]. Correlativemeasure $\mu ( s _ { 1 } , s _ { 2 } , \cdots , s _ { m } )$ is finitely superadditive, countably superadditive and unique.

Proof.Finite superadditivity.

Suppose system $X$ is partitioned into m subsystems S1, S2,. , $\mathbf { S } _ { \mathrm { m } } .$ and that $s _ { i } \in R , s _ { i } \neq \emptyset ; s _ { j } \in R , s _ { j } \neq \emptyset ; s _ { i } \bigcap s _ { j } = \emptyset \ : \ : , \ : \ : \ : X = \sum _ { j = 1 } ^ { m } s _ { j } \in R$ ，for any $i , j ( i \neq j )$ ，where $\mathtt { R }$ is a algebra on set $X$ which is given.By definition of the correlative measure, we have

$$
\mu ( S ) = \mu ( \sum _ { j = 1 } ^ { m } s _ { j } ) = \mu ( s _ { 1 } , s _ { 2 } , . . . . . . , s _ { \mathrm { m } } ) = \mu ( X _ { 1 } , X _ { 2 } , . . . . . . X _ { p } ) = \sum _ { i = 1 } ^ { p } H ( X _ { i } ) - H ( \sum _ { i = 1 } ^ { p } X _ { i } ) 4
$$

$$
\begin{array} { l } { { \displaystyle \sum _ { s _ { j } \in S } \mu ( s _ { j } ) = \sum _ { s _ { j } \in S } ( \sum _ { X _ { j } \in s _ { j } } H ( X _ { j } ) - H ( \sum _ { X _ { j } \in s _ { j } } X _ { j } ) ) } } \\ { { \displaystyle \qquad = \sum _ { i = 1 } ^ { p } H ( X _ { i } ) - \sum _ { s _ { j } \in S } H ( s _ { j } ) } } \\ { { \displaystyle \qquad = \sum _ { i = 1 } ^ { p } H ( X _ { i } ) - \sum _ { j = 1 } ^ { m } H ( s _ { j } ) } } \end{array}
$$

Subtracting (8) from(7) leads to

$$
\begin{array} { c } { \displaystyle { \mu ( \sum _ { j = 1 } ^ { m } s _ { j } ) - \sum _ { j = 1 } ^ { m } \mu ( s _ { j } ) = \sum _ { j = 1 } ^ { m } H ( s _ { j } ) - H ( \sum _ { i = 1 } ^ { p } X _ { i } ) } } \\ { \displaystyle { = \sum _ { j = 1 } ^ { m } H ( s _ { j } ) - H ( \sum _ { j = 1 } ^ { m } s _ { j } ) \geq 0 } } \end{array}
$$

# Countable superadditivity and uniqueness:

Suppose system $X$ is partitioned into a sequence of subsystems $s _ { 1 } , s _ { 2 } , . . . , s _ { n } . . .$ ， S∈R algebra on （20 $X$ （ ， $s _ { n } \in R .$ $s _ { n } \not = \emptyset ; s _ { m } \in R , s _ { m } \not = \emptyset ; s _ { n } \bigcap s _ { m } = \emptyset , X = \sum _ { n = 1 } ^ { \infty } s _ { n } \in R$ for any $n , m ( n \not = m )$ ： Let $E _ { k } = \sum _ { n = 1 } ^ { k } s _ { n }$ , obviously, we have

$$
E _ { 1 } \subset E _ { 2 } \subset \cdots \subset E _ { k } \cdots , E _ { k } \in R , \operatorname* { l i m } _ { k  \infty } E _ { k } = \sum _ { n = 1 } ^ { \infty } s _ { n } \in R
$$

From nonnegativity and monotonicity of $\mu$ ,we have

$$
\mu ( E _ { 1 } ) \leq \mu ( E _ { 2 } ) \leq \cdots \leq \mu ( E _ { k } ) \leq \cdots \leq \mu ( \sum _ { n = 1 } ^ { \infty } s _ { n } )
$$

If $\operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) \to \infty$ , from monotonicity of $\mu$ ,then inevitably we have $\mu ( \sum _ { n = 1 } ^ { \infty } s _ { n } ) = \infty$ . Hence by using finite superadditivity, we have

$$
\mu ( \sum _ { n = 1 } ^ { \infty } s _ { n } ) = \infty = \operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) = \operatorname* { l i m } _ { k \to \infty } \mu ( \sum _ { n = 1 } ^ { k } s _ { n } ) \geq \operatorname* { l i m } _ { k \to \infty } \sum _ { n = 1 } ^ { k } \mu ( s _ { n } ) = \sum _ { n = 1 } ^ { \infty } \mu ( s _ { n } )
$$

If $\operatorname* { l i m } _ { k  \infty } \mu ( E _ { k } ) = c < \infty$ ，above-mentioned process of proof is understood easily. And eitherof $\operatorname* { l i m } _ { k  \infty } \mu ( E _ { k } ) = c < \infty$ （204 and （204号 $\operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) = \infty$ makes uniqueness of the limit to hold still, i.e.

$\operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) = \operatorname* { l i m } _ { k \to \infty } \mu ( \sum _ { n = 1 } ^ { k } s _ { n } )$ is unique.Hence $\mu ( \mathrm { s } _ { 1 } , \mathrm { s } _ { 2 } , . . . . . , \mathrm { s } _ { \mathrm { m } } )$ is unique. Up to this point, proof of the theorem is completed.

From above-mentioned property of correlative measure,criterion by which lowest requirement of ideal partition is presented should be (i) $\mu ( s _ { i } ) > \mu ( s _ { i } , s _ { j } ) , \mu ( s _ { j } ) > \mu ( s _ { i } , s _ { j } )$ (ii) $\mu ( s _ { i } ) > \mu ( s _ { i _ { 1 } } ) + \mu ( s _ { i _ { 2 } } ) , \mu ( s _ { j } ) > \mu ( s _ { j _ { 1 } } ) + \mu ( s _ { j _ { 2 } } ) ,$ for any $i , j ( i \neq j ) , s _ { i } \bigcap s _ { j } = \phi . H e r e s _ { i _ { l } } , s _ { j _ { l } } ( l = 1 , 2 )$ denotes partition of $s _ { i }$ and $s _ { j }$ ,respectively. (i) denotes that correlative measure of any subsystem itself which is obtained by partition is larger than correlative measure between it and any subsystem. (ii) denotes that correlative measure of any subsystem which is obtained by partition possesses strictly superadditivity.

When number of characteristic variable of system $X$ is very large, comprehensive observation for data is impossible.Even though the observation is possible,as obtaining complete data spends very long time and for other reason such that obtained data loses its available value.At this time，statistical method and theory can be applied to obtaining data and to analyzing problem.

Suppose $\pmb { x } _ { a } = ( x _ { a _ { 1 } } , x _ { a _ { 2 } } , \cdots , x _ { a _ { N } } )$ be index of quantity of characteristic variable $X _ { a }$ of the complex system $X$ ，data obtained by mean of random sampling is $\overline { { \pmb { x } } } _ { a } = ( x _ { a _ { 1 } } , x _ { a _ { 2 } } , \cdots , x _ { a _ { q } } ) , \overline { { \pmb { x } } } _ { a }$ be a random sample from population $X _ { a }$ . Obviously any $x _ { a _ { i } }$ necessarily is equal to some （20 $x _ { a _ { \theta _ { i } } }$ ,here $\theta _ { i }$ $( i = 1 , 2 , \cdots , q )$ is number of $i$ -th individual of $\pmb { x } _ { a }$ . For any $\theta _ { i }$ ，we have probability

$$
p \big \{ x _ { { a } _ { 1 } } = x _ { { a } _ { { a } _ { 1 } } } , x _ { { a } _ { 2 } } = x _ { { a } _ { { a } _ { 2 } } } , { \therefore } , x _ { { a } _ { q } } = x _ { { a } _ { { \theta } _ { q } } } \big \} = ( N - q ) ! / N !
$$

Corresponding to expressions $( 1 ^ { \prime } )$ and $( 2 ^ { \prime } )$ , we have

$$
H ( \overline { { \pmb { x } } } _ { a } ) = \frac { ( n - q ) ! } { n ! } \Bigg [ \log ( n ! q ) - \log ( n - q ) ! - \frac { 1 } { q } \sum _ { i } n _ { i } \log n _ { i } \Bigg ]
$$

$$
H ( \overline { { \pmb { x } } } _ { a } \cup \overline { { \pmb { x } } } _ { b } ) = \frac { ( N - q ) ! } { N ! } \bigg [ \log ( N ! q ) - \log ( N - q ) ! - \frac { 1 } { q } \sum _ { i } \sum _ { j } n _ { i j } \log n _ { i j } \bigg ]
$$

At this time, correlative measure of the system $\mathrm { \Delta } X$ is defined by way similar to the above-mentioned method. Actually,in order to obtain the partition of the complex system $X$ ，we often introduce coefficient of correlative measure $\mu _ { a b } = \mu ( X _ { a } , X _ { b } ) / H ( X _ { b } ) ( \mu _ { i j } = \mu ( s _ { i } , s _ { j } ) / H ( s _ { j } )$ ，obviously, $\mu ( a , b ) ( \mu _ { i j } )$ is between O and 1. Having computed $\mu _ { a b }$ for all $^ { a , b }$ ， those $X _ { a } , X _ { b }$ whose correlative measure is larger than other $\mu ( a , l )$ ， $l \neq b$ or $\mu ( \boldsymbol { r } , \boldsymbol { b } )$ ， $r \neq a$ ，are combined so as to accord with the preceding criterion (i)， (ii).Hence some corresponding subsystems $s _ { i }$ ， $i = 1 , \cdots , m$ , i.e.some neural functional units are obtained.

From neurophysiological studies, elementary unit representing information in human brain is mutually coordinated clique of neuron; by combining mutual function, neurons with different properties carry out correlative activity, form various dynamic neural networks which correspond to various function of information.

Above-mentioned partition or aggregation of neurons is carried out in self-organizable and self-adaptive form; the partition or aggregation ofneurons,andintegration make humanbrainto emerge consciousness.

# 3.Emergence of consciousness

Suppose system $X$ self-organizably and self-adaptively is partitioned intomsubsystem. Correspondingly， therearemsubspace $\left( \varOmega _ { i } , \Im _ { i } , P _ { i } \right)$ ， $i = 1 , \cdots , m$ ，of probability-space $( \varOmega , \Im , P )$ ，where $\varOmega$ is space of configuration of system $X$ ．Let $( \varOmega , \Im , P )$ be Radon measurable space, then for any $\Im _ { i }$ there exists regular conditional probability $p ( \cdot \mid \Im _ { i } )$ （204 obviously, $p ( \cdot \mid \Im _ { i } ) = p ( \cdot \mid \Im ( s _ { i } ) ) = p ( \cdot \mid s _ { i } )$ . Hence, for any $A \in { \mathfrak { I } }$ ，

$$
P ( A ) = \int _ { \Omega | \mu } p ( \cdot \mid s _ { i } ) d p ( s _ { i } )
$$

and for any random variable $\eta$ ，

$$
E ~ ( \eta ) = \int _ { \Omega \mid \mu } ~ E ( \eta ~ \mid ~ s _ { i } ) d p ( s _ { i } )
$$

where ${ \Omega \mid \mu }$ is quotient space induced by correlative measure $\mu$ on $\varOmega$ .Let us define following two functions,

$$
\begin{array} { c } { { f _ { \varepsilon } ^ { ( 1 ) } ( t ) = \left\{ \displaystyle \frac { P ( A ) } { \varepsilon } , ~ 0 \leq t \leq \varepsilon \right. } } \\ { { \left. \begin{array} { l c } { { 0 , } } & { { \varepsilon < t } } \\ { { } } & { { } } \end{array} \right. } } \\ { { f _ { \varepsilon } ^ { ( 2 ) } ( t ) = \left\{ \displaystyle \frac { E \eta } { \varepsilon } , ~ 0 \leq t \leq \varepsilon \right. } } \\ { { \left. \begin{array} { l c } { { 0 , } } & { { \varepsilon < t } } \\ { { 0 , } } & { { \varepsilon < t } } \end{array} \right. } } \end{array}
$$

where $\varepsilon$ is arbitrary positive number, $t$ is time.

To be this,we eventually have conscious experience, that is $P ( A )$ or $E ( \eta )$ under some situations or $\operatorname * { l i m } _ { \varepsilon \downarrow 0 } f _ { \varepsilon } ^ { ( i ) } ( t ) , \quad i = 1 , 2$ ，under other situations.

When $A$ or $\eta$ comes from outside of $X$ ,or when $A$ or $\eta$ is caused by“gene”or“experience” which is in inside of $X$ ，all will emerge consciousness,and is emerged by system $X$ self-organizably and selfadaptively to be partitioned (aggregated)， then to be integrated in hundreds of milliseconds. $P ( A )$ or ${ E \eta }$ is the integrated process.

Proposition. Suppose the neural system $\boldsymbol { X } = ( X _ { 1 } , X _ { 2 } , \cdots , X _ { a } , \cdots , X _ { p } ) ^ { T }$ is the Abstract neural automata[4], then the consciousness $P \left( A \right)$ is Gibbsian.

Proof. Because the neural system $\boldsymbol { X } = ( X _ { 1 } , X _ { 2 } , \cdots , X _ { a } , \cdots , X _ { p } ) ^ { T }$ is the Abstract neural automata[4],then conditional probability $p ( \cdot  { | } \Im _ { i } ) = p ( \cdot  { | } \Im ( s _ { i } ) ) = p ( \cdot  { | } s _ { i } )$ is Gibbsian ， $P ( A ) = \int _ { \Omega \mid \mu } p ( \cdot \mid s _ { i } ) d p ( s _ { i } )$ isa convex combinationof Gibbs measures,hence Gibbsian.

The proof is thus completed.

If there exists sequences of mutual information between any appointed two neurons and between any appointed two subsystems,and the sequences of the mutual information have nice asymptotical properties,that is,if there exists ergodic superadditive process for the sequence of mutual information in neural system X ,then the system can implement self-organization and self-adaptation.

4.Consciousness originates from quantum mechanical processes We have pointed out ever that “concept" “ consciousness" is generated at particle(ion) level in the brain and is experienced at the level of the neural network [6]. In the human brain system,there must be a transition from the microscopic or submicroscopic level to the macroscopic level， that is,from the quantum to the classical level for consciousness to generate at the ion level pass to be experienced at the level of neural network.When does the quantum process approach this transition point ?SW entropy can answer this question.

Suppose that human neural system at the ion level is denoted by Q,while the human neural system at level of neural network is denoted by $N$ .In $\mathsf { Q }$ system，given state $\rho$ ,then SW entropy is defined as[7]

$$
I [ \rho ] = - { \int } d z d z ^ { * } p ( z , z ^ { * } ) \log p ( z , z ^ { * } )
$$

Where $| z >$ is a coherent state, $p ( z , z ^ { * } ) = < z | \rho | z >$ ,probability density.Von Neumann(vN) entropy is defined as

$$
S ( \rho ) { = } { - } t r ( \rho \log \rho )
$$

In the case of discrete spectrum.

The lower bound of $I ( \rho )$ is determined by two inequalities

$$
I ( \rho ) { \geq } S ( \rho )
$$

$$
I ( \rho ) { \geq } 1
$$

The latter is saturated by coherent states, while the former by the thermal states of system Q(harmonic oscillator) in the high temperature regime.More precisely, $I ( \rho ) { = } 1$ ,if and only if the system $a$ is in the coherent states,while taking maximum by $I = S$ corresponds to the thermal states of system Q. From a mathematicalpoint of view, both of these states are Gaussian and therefore Gibbs states [8].Hence we can see that when the quantity $I = S + 1$ ， the system Q will approach the transition point from quantum to classical level. In essence， as the $a$ system evolves into the Gibbs states ,it will approach the transition point. Then the neural system will begin the process of experiencing consciousness.

# 5.Remarks

(1） That the neural system $X$ is self-organizably and self-adaptively partitioned into some subsystems is implemented by means of the correlative measure，i.e.mutual information，which does not touch upon change of location of space of neurons or subsystems. This is a “Internet’ in the neural System $X$ ．The some subsystems form large-scale hierarchical intelligent system. On the hierarchical intelligent system， basicprinciple ofthe intelligent system holds [5]. The layer with high intelligence is dynamic core of the neural system $X$ ：   
(2) Consciousness is some memory.   
(3) Unconsciousness is also consciousness.   
(4) Consciousness is conscious of consciousness.   
(5） Nearly 3OOO years ago，Chinese ancient learned men said consciousnessisthatSainthears at silence, looksat immateriality,and firmly believed memory all is in brain.,   
(6) Oneself-consciousness can only be experienced; it forever cannot be captured by oneself.

(7） Consciousness as one of the most fascinating neurological phenomena—spirit is represented byaprobability function. Essence of consciousness is probabilistic.Higher order state of consciousness is the thought. By means of learning (including perception， memory)，humnn brain produces concept. Before ideology emerged, the concept is highest result of human's brain, is intension of cognitive object. In the theory of abstract neural automata,the concept is limit Gibssprobability measure[4]. Thought is mutual connection of concepts,mutual transform of concepts, important stage of all evolutionary process of cognition.

In particular, In this paper, Gibbsness's proof of consciousness is given directly.

(8)From basic principle of the intelligent system[5],i have already proved the existence of universal intelligence ,thus proving the existence of universal consciousness ,because the consciousness has existed already,before the generation of the intelligence. (9)I have already proved “ concept“- “ consciousness” is generated at particle(ion) level in the brain and is experienced at the level of the neurons[6]. (10)Having decreased the corelative measure,maligant development ofinternet and artificial intelligence finally will lead up to weakening of of consciousness of human brain in most people. Obvious example is that miss among people will weaken.

Acknowledgments. This work was partly supported by the National 973 Project undrer Grant No.2003CB517106, China and the NSFC Projects undrer Grant No. 60621001, China.

# References

1.Giulio Tonoi and Gerald M.Edelman. Consciousness and Complexity, Science 828,1846-1851 (1998)   
2. Xiguangcheng. Entropy-method of Partition of Complex System, ACTA AUTOMATICA SINICA(in Chinese) 13(3), 216-220, (1987).   
3. Xi Guangcheng. The Entropy-method of Eologico-Economical Regionalization,ACTA AUTOMATICA SINICA (in Chinese) 16(2),170-173, (1990)   
4. Xi Guangcheng.Variability of structure of abstract neural automata and the ability of thought .International Journal of System. and Cybernetics， 32(3), 1549-1554, 2003; G. C. Xi “Abstract neural automata," Kybernete: The International Journal of System. and Cybernetics, vol. 27 No.1, pp. 81-6, 1998.   
5. Xiguangcheng. chinaXiv:201705.00829V1, Relative Entropy Minimizing-Based Theory of Intelligent Systems 2017-05-13.   
6. Xiguangcheng. chinaXiv:201803.01556V1,Gibbsian representation of knowledge in infinite dimensional random neural networks(IDRNN).   
7. C.Anastopoulos.Information measures and classicality in quantum mechanics,Physcal Review D,59 O4500-1-14.   
8.Ya.G.Sinai.(1982)Theory of Phase Transitions :Rigorous Results ,Pergamon press.