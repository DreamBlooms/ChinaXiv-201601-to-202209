# 中国高性能计算研究与应用调查报告

(1956至2011）

# 孙凝晖

摘要 本报告回顾中国高性能计算机系统研发和应用的历史，介绍目前政府所支持的高性能计算项目、高性能计算中心、主要研究机构、重要的高性能计算应用领域和国内厂家的现状。从系统研制、应用开发和长期规划方面，与美国、欧洲和日本发达国家的高性能计算进行了比较。此外，本报告还对中国在高性能计算方面的技术和应用发展方向进行了预测。

# 1介绍

1956 年在周恩来总理的主持下制定了中国《十二年科学技术发展规划》，选定了“计算机、电子学、半导体、自动化”作为“发展规划”的四项紧急措施，制定了计算机的科研、生产、教育发展计划，中国计算机事业由此起步。从那时起，中国计算机的发展已经历了五十五年。不断变化的应用需求引导了高性能计算机的发展方向。根据应用需求的变化，中国高性能计算机的发展可以大致可分为1956\~1990、1990\~2005和 2005以来三个阶段（详见本报告 $\{ \ S 2 \}$ ）°

本报告回顾中国高性能计算机系统研发和应用的历史，介绍目前政府所支持的高性能计算项目、高性能计算中心、主要研究机构、重要的高性能计算应用领域和国内厂家的现状。从系统研制、应用开发和长期规划方面，与美国、欧洲和日本发达国家的高性能计算进行了比较。此外，本报告还对中国在高性能计算的技术和应用发展方向进行了预测。

# 报告要点

从 20世纪50年代起步的中国高性能计算，最初只面向国家战略需要。如今，这仍然是高性能计算非常重要的方面，但商业方面的需求也逐渐占了较大的比重。西方国家对高性能计算长期实施出口限制，这使得国产的高性能计算相关的研制工作变得更加紧迫，国家也在这方面投入了大量的财力和人力，取得了一定的成效，如国产高性能CPU 和超级计算机。? 我国政府在高性能计算上有持续且长期的资助。相关的研究人员期望有更大力度的投入和资助以尽快缩小与发达国家的差距，这也促进了全国范围的高性能计算中心的建立和连接。这些超算中心将帮助培训专业的人才使用超级计算机并提高应用水平。据统计，目前我国至少有300所大学有自己的高性能计算中心。我国正大力鼓励国内企业研制和开发高性能计算机系统，如曙光、联想、浪潮、宝德等。同时也在努力推进自主的高性能计算标准化工作。  
$\bullet$ 除了系统的研制工作，我国也启动了大量网格计算和服务项目，如中国国家网格、中国教育和研究网格。许多大学和研究所都开发相应的网格应用原型系统，达到了数据共享的目的。但构建分布式计算环境这样的网格计算目标仍然没有完全实现，而且暴露出了一些网格中心归属和管理上的问题。  
$\bullet$ 国家重点资助的高性能计算项目有：高效能计算机、高端容错计算机、核心电子器件、个人高性能计算机、高通量计算机。  
$\bullet$ 主要的研究机构有中科院计算所、江南计算所和国防科技大学。  
$\bullet$ 研究正逐渐向大规模数据处理器和高效使用GPU等加速硬件的方向发展。  
$\bullet$ 缺乏真正的高性能计算机应用软件，这是政府决策层需要重点考虑的问题。目前，相对成功的高性能计算软件示例有：PHG、JASMIN、GeoEast、GRAPES 和NWP。  
$\bullet$ 各种应用领域中具备使用高性能计算机能力的研究生数量严重不足。  
$\bullet$ 一方面，工业界能够较容易把系统扩展到大规模，如千万亿次；另一方面，大学和研究机构的应用需求通常只能扩展到百万亿次，二者存在较大的差距。石油行业有点特殊，属于工业界用户，航空航天是高性能计算的重要用户，但很大程度依赖政府的支持，而不是经济发展的推动。物理和天气预报的需求在政府的支持下也正在增长。其他领域如汽车与船舶制造的高性能计算用户，应用规模都比较小。新兴的应用领域包括医疗、运输、金融和虚拟现实等。需要特别关注的是对在线服务的支持，如搜索、购物、游戏等。舆情分析也将是重要的应用。我国已经研制成功了两台千万亿次超级计算机，都使用了CPU-GPU异构系统技术。曙光6000将采用国产的高性能处理器。高性能计算研究在原始创新方面有待提高，缺乏长期的规划和追随美国是目前存在比较大的问题。未来，并行软件和硬件将成为主流，其发展也为我国提供了赶超美国和日本的契机。国产的高性能计算机已经占据了TOP500 前列，未来有望保持这样的势头。此外，我国的研究人员正积极参与国际上Exaflops计算的讨论，对其挑战性问题以及市场机遇进行分析。我们有信心认为国产龙芯处理器将成为科学计算的主流平台。我国在众核处理器、并行编程模型、并行算法设计和系统容错方面也展开了研究。软件方面的重点在于处理和分析大规模数据的工具和应用。

# 2 中国高性能计算的历史

本节简单回顾中国高性能计算机发展经历过的历史阶段以及高性能计算应用早期发展的概况，同时列出了一些重要的代表性的计算机来说明国产高性能计算机的发展历程。

1956年到1990年是第一阶段，高性能计算机的研制集中围绕国家战略应用的需求进行是这一阶段的主要特征。这是中国的计算机从无到有的阶段，经历了电子管、晶体管和集成电路三个时代。作为记述对中华文明发展起促进作用的重要历史事件，这一时期的103 机、104 机、119机、150机、757机、银河-1号巨型机、和银河-2仿真计算机七台计算机的名字铭刻在中华世纪坛的青铜甬道铭文上。改革开放之前，由于国外对中国进行封锁，满足国家战略需求是研制高性能计算机的主要动因，如为了“两弹一星”的需要研制了109丙机。从1978年改革开放到1990年是中国开始从计划经济向市场经济过渡的时期，政府、企业、学校需要的计算机已经可以从国外采购，但是机器的性能受巴统组织的限制。为了满足国防和一些由国家主导的行业，如石油勘探、气象，对计算能力的需求，我国自行研制了以向量机为主的多台高端计算机，如银河-1号、757机、银河-2号等。

1990 年到2005年是第二阶段，除了国家战略应用和国家主导的行业外，越来越多的高性能计算机服务于国民经济的各行各业。从20世纪90年代开始，由于中国经济的高速发展，不仅国家主导的行业产生了更多的高性能计算需求，许多工业领域的应用，如汽车制造、船舶设计、药物设计等，也产生了巨大的需求；同时，由于采用遵循摩尔定律的商用微处理器和大规模并行处理技术，中国的高性能计算机开始形成产业。旺盛的计算需求和日益成熟的高性能计算机技术极大地促进了中国高性能计算机的发展，继银河系列之后，又陆续推出了曙光、神威等系列产品，形成了国防科大、中科院计算所、江南计算所三个主要系统研制单位和曙光、联想、浪潮三个主要企业。

从2006年开始进入了第三阶段。越来越多的新型应用产生了新的需求，如在线视频、搜索引擎、电子商务、电子政务、网络游戏等。国家和地方政府正在全国范围内建设超算中心（Supercomputing Center），向政府、企业、社会提供公共的计算和数据资源。中国普及而完善的高性能计算基础设施正在形成。当前，研制与应用百万亿次、千万亿次高性能计算机是中国的主要任务。

# 2.1高性能计算机的里程碑

随着电子工艺的发展，中国的高性能计算机经历了电子管时代、晶体管时代、集成电路时代；在并行体系结构方面，先后经历了向量机、大规模并行系统、机群三个技术阶段。下面列举一些里程碑式的重要计算机。

# 2.1.11956—1980：起步阶段

$\bullet$ 1958 年，中国第一台计算机103 型计算机研制成功，运行速度每秒1500 次。  
$\bullet$ 1959 年，中国第一台大型通用计算机104机研制成功，运行速度每秒1万次。  
$\bullet$ 1964年，119机研制成功，运行速度每秒5万次，它是电子管时代中国最快的计算机。1965 年，中国第一台晶体管大型通用计算机109乙机研制成功，运行速度每秒6万次。  
$\bullet$ 1968 年，109丙机研制成功，运行速度每秒10万次浮点运算，为中国“两弹一星”事业做出重要贡献，被称为“功勋机”。  
$\bullet$ 1970 年，中国第一台小规模集成电路通用计算机111机研制成功，运行速度每秒30万次。  
$\bullet$ 1973年，中国第一台百万次的集成电路计算机150机研制成功。

# 2.1.21980—2006：重要发展阶段

在此期间中国高性能计算机百花齐放，取得了长足的进步，银河、神威、曙光三大系列成为中国高性能计算机的代表。

# 757向量机与KJ8920

1983 年研制成功的757大型向量计算机，浮点运算峰值每秒一千万次，是当时中国最快的计算机，是中国向量机的开端；历时7年于1991年完成的KJ8920石油勘探数据处理系统是中国最后一台采用大型机（Mainframe）结构的大型数据处理系统，它配有专门的石油应用软件。这两台系统都是中科院计算所完成的。

# $\bullet$ 银河系列

1983年，银河Ⅰ号巨型计算机研制成功，它采用向量结构，运算速度达每秒1亿次，是中国首台亿次计算机；1992年，中国第一台10亿次的银河Ⅱ号巨型机研制成功;1997年，银河一I并行巨型计算机研制成功，峰值性能为每秒130亿浮点运算；2000年，银河IV超级计算机系统问世，峰值性能达到每秒1万亿次浮点运算。此后，国防科大继续研制了多台具有世界先进水平的用于国防应用的银河系列超级计算机。

# 神威系列

1999 年9月，神威I号计算机系统投入运行，对社会开放，峰值速度为每秒 3840亿次浮点运算，是当时中国运算速度最快的高性能计算机。此后，江南计算所研制了多台具有世界先进水平的用于国防应用的神威系列超级计算机。

# $\bullet$ 曙光系列

1995年，曙光1000大规模并行处理系统研制成功，单精度每秒25.6亿次，双精度每秒19.2亿次，实际应用运算速度达每秒15.8亿次；1998年研制成功曙光2000-I，采用机群体系结构，系统峰值速度为每秒200亿次，1999年研制成功曙光2000-II，系统峰值速度为每秒1117亿次；2000 年曙光3000 研制成功，采用 $\mathrm { S M P } ^ { 2 }$ 机群体系结构，系统峰值速度为每秒4032亿次；2003年，曙光4000L研制成功，峰值速度每秒3万亿次；2004年，曙光4000A研制成功，峰值速度每秒11万亿次，使中国首次进入国际超级计算机TOP10，使中国成为继美国、日本之后第三个能研制十万亿次级商品化高性能计算机的国家。中科院计算所研制的曙光系列，十年左右时间性能提高了1万倍。

# 2.1.32006—现在：百万亿次到千万亿次的跨越

$\bullet$ 2008 年，曙光 5000 峰值 230TFlops，当年在 TOP500 排名第十，成为亚洲最快的超级计算机。同时，联想公司研制了100TFlops 的深腾7000。  
$\bullet$ 2009 年10月，国防科大研制了天河-1，Linpack 峰值563.1TFlops，在 TOP500 排名第5。2010 年6月，中科院计算所和曙光公司研制星云超级计算机，Linpack 峰值1.271PFlops4，在 TOP500 排名第 2。  
$\bullet$ 中科院过程所研制了Mole-8.5 系统，Linpack 峰值207.3TFlops，TOP500 排名第19。

# 2.2高性能计算应用的早期阶段

1980 年以前高性能计算应用与具体机器是紧密联系的，如1959年研制成功的104机用于原子弹的研制，1964年研制成功的119 机用于氢弹的研制，1967年研制成功的109 丙机用于两弹一星研制中的计算。1980 年出现了向量机，高性能计算应用的主流变为基于向量型并行计算的通用科学计算，如线性规划、傅里叶变换、滤波计算以及矩阵、线性代数、偏微分方程、积分等问题的求解。

1995 年随着并行处理系统的出现，高性能计算应用变为了并行计算，并且逐渐普及到工程计算、网络应用领域，同时各种商业应用软件产品大量出现。2005年中国自己开发的商业应用软件包开始出现，随着机群结构的高性能计算机的普及，已经有超过300所大学拥有了高性能计算机，使得高性能计算应用在中国逐渐普及。

# 2.2.1国家高性能计算中心

1990 年代中国的高性能计算只局限在石油、气象等个别应用领域，国家科技部为了推广国产的高性能计算机和在更多的领域推进高性能计算应用，于1995 年成立了第一个国家高性能计算中心，其主要作用有：

$\bullet$ 促进国产曙光系列并行计算机的应用；  
$\bullet$ 辅助科学、工程和环境科学中大规模研发项目；

$\bullet$ 为并行计算的教育提供环境。

随后相继建立了北京中心、合肥中心、成都中心、武汉中心、上海中心、杭州中心、西安中心、北京华大基因中心等多个国家高性能计算中心，它们都配置了国产的高性能计算机系统。这些高性能计算中心是后来的超级计算中心的早期形态。国家科技部还设立了国家高性能计算基金来支持这些中心的发展。基金于1995至2000年共资助了近340个课题，加上各个中心自己的经费支持的课题，到2001年11月有450个课题使用各个国家高性能计算中心的机器。应用领域包括物理、化学、石油、核能、气象、航空、航天、水利、生物医学等。

通过给学术界、商业用户和政府提供高性能计算硬件、软件、通信和服务支持，这些国家高性能计算中心在早期的应用开发中发挥了重要作用。同时，也积累了建设高性能计算中心的丰富经验，其中一些中心成为国家网格的重要组成部分。这些支持与美国国家科学基金会（NSF）从20世纪80年代到90年代对超级计算机研制支持很类似。

# 2.2.2早期应用开发研究的典型代表

高性能计算的发展带动了一大批学科的进步，许多科研人员借助高性能计算取得了丰硕的科研成果。以下是借助高性能计算在自身领域取得卓越的成绩，并在早期起到了示范作用的三个案例。

$\bullet$ 陈国良院士运用高性能计算进行防灾减灾研究：陈国良，计算机科学家，2003年当选中国工程院院士。他于90年代中期开展了高性能计算及其应用的研究，率先发起成立了中国第一个国家高性能计算中心，开发了基于曙光高性能计算机的“用户开发环境”软件包；通过承担国家863重大项目“安徽省防灾减灾智能信息与决策支持系统”和“淮河流域防洪防污智能调度系统”，在安徽省灾害性天气预报，汛期淮河流域群库优化调度，以及淮河防污治污中发挥了重要作用。  
$\bullet$ 王鼎盛院士运用高性能计算研究晶体光学性质：王鼎盛，物理学家，2005年当选中国科学院院士。他的主要科研工作包括以下四个方面：磁性材料，尤其是磁性体表面和界面性质的理论；表面吸附和表面电子性质理论；非线性光学晶体物理性质的理论计算；固体电子结构与磁性的理论计算方法。他在非线性光学晶体物理性质的理论计算方面，利用曙光高性能计算机求解了高阶复杂问题，并将理论计算成果应用于各种实际分析，取得了很好的成果。  
$\bullet$ 陈润生院士运用高性能计算进行非编码RNA研究：陈润生，生物学家，2007年当选中国科学院院士。他在基因组信息学领域，完成了中国第一个完整基因组泉生热袍菌基因组的全部生物信息分析，参加了人类基因组和水稻基因组的信息分析；在非编码基因领域，发现了百余个新的非编码基因，确定了两个非编码基因家族，发现了三个特异的非编码基因启动子。自1993年以来，他利用高性能计算进行非编码RNA研究，取得了丰硕的成果。

# 3 政府行为

# 3.1主要的政府计划

中国的高性能计算机产业是在 863 计划的支持下发展起来的。从1992年起863计划先后支持了曙光一号、曙光1000、曙光2000、曙光3000、曙光4000A、深腾 6800 的研制。十一五期间（2005-2010年)863计划设立了“高效能计算机及网格服务环境”重大专项和"高端容错计算机"专项。2006年国务院发布了《国家中长期科学与技术发展规划纲要（2006 年-2020 年)》，设立了16个科技重大专项。“核心电子器件、高端通用芯片及基础软件产品”专项（简称“核高基"）是16个科技重大专项之一，与高性能计算密切相关。

# 3.1.1高效能计算机项目

中科院“十一五”信息化建设专项的目标是研制两台百万亿次计算机和两台千万亿次计算机，开发网格软件和网格应用，建设服务全国的中国国家网格（CNGrid)。

中科院计算所为上海超级计算中心、联想公司为中科院网络中心各自分别研制了一台百万亿次高性能计算机，现在这两套系统都已交付使用。计算所研制的曙光5000系统又称为“魔方”（Magic Cube），峰值运算速度为233Tflops，Linpack 实测值达到180.6Tflops，在2008年11月份公布的全球高性能计算机TOP500 排行榜中名列第十。它是当时中国性能最高的通用计算机系统，也是当时除美国之外世界范围内性能最高的超级计算机系统，2009年6月份正式投入运行。联想研发的深腾 7000 百万亿次系统，实际 Linpack 性能突破每秒106.5 Tflops，并在国内第一个实现了PB级别的在线、近线、离线的三级结构海量存储系统，2009年4月开始正式投入运行。

863 计划高效能计算机专项于 2009 年启动了2台千万亿次系统的研制。两台千万亿次系统的用户分别是深圳市超级计算中心和天津市滨海新区超级计算中心。两台千万亿次高性能计算机的研制者分别为中科院计算所和国防科技大学。

中科院计算所研制的千万亿次系统称为曙光6000，是一台由计算分区和服务分区组成的异构系统。计算分区采用计算所提出的超并行（HPP,Hyper ParallelProcessing）体系结构，是一种改进型的星群结构。服务分区是一种典型的基于Intel处理器和GPU 加速卡的刀片服务器。

曙光6000 将使用国产高性能龙芯处理器，超节点由龙芯3B 和AMDOpetron 处理器构成，每种处理器所发挥的作用也有差异。多数应用软件是基于 $\mathbf { x } 8 6$ 指令集，但龙芯的指令集和 $\mathbf { x } 8 6$ 指令集不一样。为了解决兼容性问题，超节点中 $\mathbf { x } 8 6$ 处理器运行操作系统，编译和应用任务先提交给 $\mathbf { x } 8 6$ 处理器，再由硬件支持的二进制翻译将计算任务分配到龙芯处理器上运行。这样，应用程序不需要修改就能够在曙光6000上运行。

龙芯3B 是8核CMP 结构，每个核带有 SIMD5加速部件，其功耗远低于主流的 $\mathbf { \Delta x } 8 6$ 处理器，具有非常高的性能功耗比。龙芯3B的设计目标是主频 $1 \mathrm { G H z }$ ，浮点峰值运算速度128Gflops。计算分区由320个1U尺寸的超节点组成，每个节点包含8个龙芯3B处理器。超节点内以超并行系统控制器实现处理器间的高速互连，超节点间采用高速商用网络Infiniband互连。

曙光星云作为曙光6000 的服务分区于2010年6月份研制成功，在当时 TOP500 中排名第二。星云系统的主要参数为：

$\bullet$ 理论浮点峰值性能：2.98PFlops；Linpack 性能：1.271PFlops;  
$\bullet$ 9280个6核2.66GHzXeon处理器，共55680个核;  
$\bullet$ 每个刀片配置一个GPGPU加速卡，共 4640个Nvidia Tesla C2050 GPGPU;  
$\bullet$ 4640个两路曙光TC3600刀片，464个刀片箱，共116个机柜；  
$\bullet$ 系统内存：111.36TBDDR3;  
$\bullet$ 所有的计算节点通过三层Infiniband4xQDR网络互连；$\bullet$ 整系统功耗：3MW；  
$\bullet$ 运行Linux操作系统和曙光机群系统管理软件。

曙光 TC3600 刀片系统遵循 SSI（Server System Infrastructure）标准和中国高性能计算标准委员会制定的标准， $\mathrm { \mathbf { M P I } } ^ { 6 }$ 延迟是 $1 . 5 \mu \mathrm { s }$ ，点到点通信带宽是 $3 . 2 \mathrm { G B } / \mathrm { s }$ ，支持GPU-Direct 优化技术。星云异构系统需要CPU-GPU混合编程获得更好的性能，通过自适应负载平衡策略划分CPU和GPU上任务。由于整个系统并行度可达上千万级，需要MPI、OpenMP 和CUDA（还包括OpenCL兼容）三级并行。为了获得更高浮点效率，比如在线程数量和有限资源的权衡、片内共享存储中的数据重用等针对Fermi体系结构优化相当重要。通过一系列的优化，单节点的Linpack 效率可达到 $6 9 . 8 9 \%$ 。

国防科大研制的千万亿次系统称为天河（Tianhe）系列。天河-1的理论峰值是1206TFlops，Linpack 峰值是 563.1TFlops，Linpack 效率为 $60 \%$ 。天河-1是由IntelXeon4核和AMD 575MHzHD4870X2组成的异构系统，其 512个I/O节点配置为两路Intel Xeon 多核处理器。所有节点通过 40Gbps Infiniband 网络互连，内存总容量为98TB,存储容量为1PB。整个系统包括100机柜，功耗为1.8MW。2010年10月，天河-1的升级版本天河-1A研制成功，其系统结构和星云类似，采用 IntelXeon 多核处理器和Nvidia GPGPU 组成的异构体系结构，Linpack峰值达到2.5PFlops，为当前世界最快的超级计算机。

2009 年863计划启动了面向千万亿次高性能计算机的高性能计算应用研究项目，包括：面向制造业和资源环境领域的高性能计算与网格应用、千万亿次高效能计算机的算法库、面向千万亿次高效能计算机的大规模并行应用软件系统，候选领域包括气候、核能、石油、环境、航空、新能源、工业仿真优化、生物制药等。每个项目资助 400 万元。这次立项是近10 年来863计划在高性能计算应用软件上的首次投资。

# 3.1.2高端容错计算机项目

该项目目标是针对金融、电信等领域的关键应用（critical application)，研制高端容错计算机产品，以打破国外产品在这些领域的垄断，降低中国的信息化建设成本。高端容错计算机的可用度要达到 $9 9 . 9 9 9 \%$ ，兼容主流高端数据库和中间件，并在至少两个关键领域的生产性业务系统上实现规模化应用。第一阶段到2010年12月，研制32处理器的高端容错计算机，863计划支持经费5.9亿元，要求主机研制企业配套9.75亿元。第二阶段到2012年12月，研制64处理器的高端容错计算机，系统TPC-C值要达到2007年底的国际领先水平。

第一阶段共安排了4个课题，包括：高端容错计算机总体研究，3000万元；高端容错计算机评估与测量，4000万元；两个高端容错计算机主机研制，共5亿2000万元。主机课题的研究内容包括：主机研制和其在银行或电信领域关键业务的示范应用，主要的指标如下：

32路处理器，紧耦合共享存储结构；  
$\bullet$ 研制具有自主知识产权的芯片组;  
$\bullet$ 系统平均可用度达到 $9 9 . 9 9 9 \%$ ，年停机时间 ${ \leq } 5 . 2 6 \$ 分钟；  
$\bullet$ 系统峰值定点计算能力大于 ${ \bf 8 0 0 G I P S } ^ { 7 }$ ，浮点计算能力大于 800GFlops;；  
$\bullet$ 系统全局共享存储器容量大于2TB，系统存储器总带宽大于800GB/s;  
$\bullet$ 系统内部互连网络总带宽大于 $9 6 0 \mathrm { G B / s }$ $\bullet$ I/O子系统规模大于128条PCI-E通道；  
$\bullet$ 具有芯片级在线故障定位与诊断能力，部件级模块具有热插拔快速修复能力；$\bullet$ 操作系统支持LSB和POSIX标准，支持符合工业标准的系统状态监控;  
$\bullet$ 具有操作系统核心级故障检测、故障隔离、故障恢复的功能;  
$\bullet$ 支持操作系统核心进程的多副本备份与恢复；  
$\bullet$ 支持系统级进程检查点技术；  
$\bullet$ 虚拟机监控器的性能开销不大于 $10 \%$   
$\bullet$ 至少兼容一种商品化数据库、一种中间件及一种存储管理与备份软件；数据库、中间件能扩展到32路紧耦合处理器;  
$\bullet$ 实现与IBMp 系列或HP Sperdome 系列的互备运行，切换时间不高于60 秒。

华为公司和浪潮公司各自获得经费2.6亿元的项目资助，要求到2010年底完成32路处理器高端容错计算机的研制。华为公司的技术路线是采用 Sun Sparc 处理器和Open Solaris操作系统。虽然 SUN 公司已被甲骨文（Oracle）公司收购，但 Sparc 处理器已完全由富士通（Fujisu）生产，且Open Solaris 是开源操作系统，因此该项收购不会对华为公司的方案产生影响。浪潮公司的技术路线是采用 Intel X86QPI8和Windows/Linux 操作系统，已经得到英特尔公司的QPI授权。

该项目的巨大挑战除了开发高端的芯片组和交换芯片、购买昂贵的QPIPHY以外，系统软件和行业应用的移植和认证工作量将更为巨大。它挑战的竞争对手是IBMP系列和 HPSuperdome系列，以及这些国际企业的服务能力。

# 3.1.3核心电子器件、高端通用芯片及基础软件产品专项

2006年1月，国务院发布了《国家中长期科学与技术发展规划纲要（2006年-2020年）》，设立了16个科技重大专项。“核心电子器件、高端通用芯片及基础软件产品”（简称“核高基"）是其中之一。2008年，专项发布了《2009-2010年课题申报指南》，其中有三项与高性能计算密切相关。至今，申请其中“高性能多核CPU”项目的单位包括中科院计算所、国防科大和江南计算所。三项的技术指标如下：

# 1．高性能多核CPU:

$\bullet$ 采用 $6 5 \mathrm { n m }$ 或者更先进工艺；  
$\bullet$ 片内集成4个以上64位高性能处理器核;  
$\bullet$ 主频1.5GHZ以上;  
$\bullet$ 用于国产千万亿次高性能计算机系统；

# 2．支持国产CPU的编译系统及工具链：

$\bullet$ 支持主要的国产CPU;  
$\bullet$ 支持C、 $^ { C + + }$ 、Fortran、Java、OpenMP 等程序设计语言；  
$\bullet$ 支持面向多核体系结构的自动并行化；  
$\bullet$ 支持低功耗编译优化；  
$\bullet$ 支持国际主流CPU指令到国产CPU指令的二进制翻译。  
$\bullet$ 具有程序调试和性能分析等工具；  
$\bullet$ 编译性能与商用编译器相当；

# 3．服务器操作系统：

$\bullet$ 支持国际主流CPU及国产CPU，支持多核；  
$\bullet$ 支持SMP、ccNUMA、机群等多种计算机体系结构;  
$\bullet$ 支持国际和国内服务器操作系统相关标准或规范；

支持国内TCM等可信计算相关规范，并达到“GB/T20272操作系统安全技术要求”的第四级安全等级；$\bullet$ 支持软硬件协同虚拟化、动态升降级等可用性要求；$\bullet$ 支持网络化部署、远程管理与监控等可管理性要求；$\bullet$ 与国际主流Linux 服务器操作系统功能、性能相当；$\bullet$ 支持丰富的应用软件；

# 3.2超级计算中心

目前，中国有两个超级计算中心配有超过100TFlops 的系统，分别是南方的上海超级计算中心（SSC）和北方的中科院超级计算中心（SCCAS）。最近，又在深圳和天津分别建立了两个千万亿次超级计算中心，此外北京、广州、山东、成都、福建和沈阳等地方政府也在积极筹备超算中心。

# 3.2.1上海超级计算中心

上海市超级计算中心（简称上海超算）成立于2000年12月。

上海超算按照服务功能分为4个部门：（1）科学计算、（2）工程计算、（3）研究开发、（4）技术支持。技术支持部门负责机器和网络的维护和管理；研究开发部门关注网格和并行编程；科学与工程计算部门负责应用。主要的工作包括用户支持、软件移植和调优、用户培训和软件开发。技术支持部门的系统管理员均具有计算机科学方面的背景； $80 \%$ 的网格计算人员有计算机科学背景， $20 \%$ 的人员有计算科学背景。其他部门约 $40 – 45 \%$ 的人员有计算机科学背景， $34 \%$ 有计算科学背景， $20 \%$ 有应用背景。超算中心具备了技术咨询、支持和服务的能力。为了更好地给用户提供服务，上海超算与当地大学如上海交通大学、复旦大学建立了密切的合作，在具体应用开发方面开展了大量的工作。

# $\bullet$ 资源(硬件和软件）

上海超算的硬件系统至今已经更新了四代：神威-I（384GFlops）、神威新世纪-64P（307GFlops）、曙光4000A（10TFlops）和曙光5000A（230TFlops)。目前神威-I和神威新世纪-64P已经不再提供计算服务（神威新世纪-64P的技术指标见附表A)，现在正在运营的系统曙光4000A和曙光5000A的配置参数见表1和表2。值得一提的是，曙光5000A在研制测试时安装的是微软的Windows机群操作系

表1.曙光4000A配置一览   

<html><body><table><tr><td></td></tr><tr><td>系统峰值 10.2Tflops</td></tr><tr><td>计算结点 512个4路AMDOpteron处理器</td></tr><tr><td>存储结点 16 个4 路 AMD Opteron 处理器</td></tr><tr><td>接入结点 4个4 路 AMD Opteron 处理器</td></tr><tr><td>CPU AMD OPTERON 850, 2.4GHz,</td></tr><tr><td>总共2128个CPU</td></tr><tr><td>系统内存总容量 4256GB</td></tr><tr><td>磁盘总容量 20TB</td></tr><tr><td>体系架构 Cluster、Myrinet 2000</td></tr><tr><td>操作系统 Turbo Linux 8.0</td></tr></table></body></html>

统，Linpack测试性能排名世界第10。但是，由于上海超算 $9 9 \%$ 的用户要求用Linux系统，现在安装在上海超算的系统已经全部替换成Linux系统。据上海超算的统计报告，曙光5000A系统运行至今硬件系统非常稳定，只是水冷系统有待改善，比如机柜一级的智能控制和实时响应。曙光5000A采用的多核系统，其单核的性能比曙光4000A的性能稍弱，因此，性能的提高更依赖于系统软件、应用软件的优化。此外，文件系统在未来也需要改善。和其他超算中心类似，上海超算也提供了一些主流高性能计算商业软件的使用，包括 NASTRAN,FLUENT, PAMCRASH, LS-DYNA，MARC,ANSYS-Multiphysics, FEKO, CFX, GAUSSION03 等。此外，还安装有一些国内软件开发人员修改的软件如NWChem,EGO,BLAST,DOCK,

VASP, CPMD，WIEN 20O0,SIESTA，ABINIT,NAMD,GROMACS,COSMOS，MM5，WRFAPRS等。完整的软件列表见附表B附表C。

表2.魔方（曙光5000A）配置一览  

<html><body><table><tr><td>系统峰值</td><td>230Tflops</td></tr><tr><td>计算结点</td><td>刀片：每个刀片节点配置4颗AMD8347HE64位低功耗CPU; 胖节点：采用8CPU的SMP 结构，配置8颗AMD8347HECPU; 配置SKVM扩展系统，配置PCIE8X双端口ConnectXDDR Infiniband网卡</td></tr><tr><td>接入结点</td><td>32个普通接入节点，8个图形接入节点，每个节点2个CPU</td></tr><tr><td>CPU</td><td>AMD8347HE64位低功耗1.9GHzCPU</td></tr><tr><td>内存总容量</td><td>95TB</td></tr><tr><td>磁盘总容量</td><td>500 TBSAN存储</td></tr><tr><td>互连</td><td>Infiniband ConnectXDDR</td></tr><tr><td>操作系统</td><td>Suse Linux Enterprise Server10</td></tr><tr><td>语言</td><td>C, C++,Fortran 77,Fortran90</td></tr><tr><td>数学库</td><td>NAGACML1.5，ATLAS3.6</td></tr><tr><td>文件系统</td><td>Lustre</td></tr><tr><td>作业调度系统</td><td>LSF7.0</td></tr><tr><td>管理软件</td><td>曙光机群管理系统DCOS</td></tr></table></body></html>

# $\bullet$ 应用和用户

上海超算的主要应用涵盖了科学与工程计算、数据处理等领域，科学与工程计算方面的应用有：

气候和气象  
计算化学  
生物信息学  
计算物理  
应用数学  
计算流体  
有限元分析  
电磁场分析  
多物理分析  
系统仿真、规划  
数据处理方面的应用有：  
高能物理试验数据处理  
天文观测数据处理  
知识检索和挖掘  
遥感数据处理  
企业数据中心  
业务系统备份  
数据容灾备份  
商业智能

# 远程教育

如表3、4、5的统计数据所示，上海超级计算中心的用户累计达到356个，2008 年新增用户62个，其中科学用户42个，工程用户20个。用户来自全国27个省市地区， $9 5 \%$ 以

表3.各用户机构机时统计  

<html><body><table><tr><td>高校</td><td>5 983 947.66</td></tr><tr><td>工程设计院</td><td>73 687.73</td></tr><tr><td>工业企业</td><td>235 556.40</td></tr><tr><td>基础研究所</td><td>8 549 336.41</td></tr><tr><td>内部</td><td>632 851.48</td></tr></table></body></html>

上用户通过网络连接使用资源。2008年新增应用领域4个，包括桥梁工程、测绘、农业、生物医学等。这些统计数据表明：用户主要来自上海周边和北京的高校与科研院所；工业应用的机时占用比例不足 $20 \%$ ；应用项目多来自国家的各类科技计划；超过$90 \%$ 的用户的应用使用的CPU数小于64个；材料和物理领域占据了最多的机时。

表4.各领域项目数统计  

<html><body><table><tr><td colspan="2">项目领域 项目数 机时总数（CPU*小时）</td></tr><tr><td>材料</td><td>30 5 499 977</td></tr><tr><td>机械工程</td><td>19 147 546</td></tr><tr><td>物理</td><td>6 3 613 145</td></tr><tr><td>航空航天</td><td>10 380 453</td></tr><tr><td>化学</td><td>16 1 133 673</td></tr><tr><td>汽车</td><td>17 111 521</td></tr><tr><td>软件测试</td><td>8 102 025</td></tr><tr><td>生物制药</td><td>14 1 493 714</td></tr><tr><td>天文</td><td>8 2 069520</td></tr><tr><td>土木工程</td><td>7 214 192</td></tr><tr><td>纳米研究</td><td>8 614 687</td></tr><tr><td>其他</td><td>24 104 274</td></tr></table></body></html>

为了更大程度利用曙光5000A的计算能力，上海超算出资100万元鼓励用户使用大规模的并行节点。例如，某用户原来的程序使用了128-256个CPU，如果现在成功使用256-512CPU，该用户将获得资助。但上海超算的管理人员分析称很少有用户单位具备这样的能力改善并行软件。一个尴尬的事实是，目前超算中心很少有石油领域的用户，因为这类用户有雄厚的资金自己购买大型机器。

表5.各CPU使用段的用户数比例统计  

<html><body><table><tr><td>CPU使用段</td><td>用户百分比（%）</td></tr><tr><td>1-4 个 cpu</td><td>20.57</td></tr><tr><td>5-8个cpu</td><td>13.26</td></tr><tr><td>9-16 个 cpu</td><td>22.81</td></tr><tr><td>17-32 个 cpu</td><td>24.30</td></tr><tr><td>33-64 个 cpu</td><td>11.78</td></tr><tr><td>65-128个 cpu</td><td>6.07</td></tr><tr><td>128 个 cpu 以上</td><td>0.87</td></tr></table></body></html>

上海超算的长远目标是发展成为盈利的服务型单位，目前距这一目标还有一定距离。事实上，超算中心对学术界和工业界用户采取不同的收费标准。

# 3.2.2中科院超级计算中心

十一五期间，中国科学院就计划建设由一个大型超算中心和10个中型超算中心及大型科学数据库（如中国西南动物植物资源数据库）组成的 e-Science 超算环境。在 2008-2010年间，科学院的各个研究所对 5-10Tflops 量级系统的需求十分普遍。大型与中型超算中心之间是一种松散的关系，中型超算中心主要满足所在区域的研究所的中小规模的计算需求，以避免到大型超算中心的大量数据的移动。这些中心在2009年内都已建设完成。

中科院超级计算中心的前身是1996年成立的中科院超级计算应用与计算机网络信息中心实验室，1997正式更名为超级计算中心。目前有34个员工，由7个部门组成：（1）技术支持部；（2）系统管理和维护部；（3）客户服务部；（4）计算化学虚拟实验室；（5）计算金融虚拟实验室；（6）中国国家网格管理中心；（7）公共事务部。超过20个员工从事应用开发支持，如可视化、并行算法设计、网格计算技术。此外，该中心还有博士和在读的研究生

约20名。

# $\bullet$ 资源（硬件和软件）

中科院超算中心的计算机经历了从几十亿次的SGI系统（1996年）、近百亿次的日立系统（1998年）、千亿次的曙光2000ⅡI（2000年），5万亿次的联想深腾6800（2003年）、用于可视化的 SGIOnyx 350（2004）到目前 2.8TFlops 的IBMcel/broadband 机群（2007）、百万亿次的联想深腾 7000（2008年）的持续演变。目前，SGIOnyx、曙光 2000-II 和深腾 6800已经不再提供计算服务。

超算中心的主力计算服务平台是联想深腾 7000，理论峰值120TFlops，Linpack 峰值106.5TFlops，2008 年 TOP500 排名第19 位。该系统配置为：

1140个计算刀片，每个刀片由2路Intel3GHz的4核Xeon 处理器和 32GB内存组  
成，用于计算密集型应用  
38 个厚节点，每个节点是16路Intel4核Xeon处理器和512GB内存，用于访存  
密集型和数据库应用  
2 个胖节点，SGIAlitx4700 NUMA 系统，每个节点由192个Intel1.67GHz安腾2  
双核处理器和2.5TB内存组成，用于满足对内存需求大的应用  
可视化节点，12个双路Intel 3GHz 四核处理器、32GB 内存和NV8800GTS 显卡  
20Gbps 4XDDRInfiniband互连  
350TB 磁盘，1PB磁带  
65 个机柜，占地面积240平方米  
整系统功耗1.9MW  
配置有系统管理软件、并行开发环境和应用软件

软件资源包括一些商业软件、大量的开源软件以及超算和用于独立开发的软件，如由超算中心独立开发完成的 PSEPS 特征值问题并行求解软件和 PMDFFT 并行多维傅里叶软件包。

# 应用和用户

依托中国科技网，中科院超算中心累计为三百余用户提供了计算服务，提供计算机时逾三千万CPU小时，为国家973、863计划和国家自然基金等重大项目提供了高质量的计算服务。中科院超算中心的运营模式是完全由科学院财政支持，用户免费使用，是科学院信息化的组成部分。

根据统计，深腾7000的系统利用率已经从2009年4月安装初期的 $3 . 1 1 \%$ 提升到2009年6月份的 $3 6 . 6 2 \%$ 。2009 年6月到7月间，有165个用户，其中42个是超算内部开发人员的调试、软件安装和测试，其他的123的实际用户中，111个来自中科院的各研究所，消耗了4百万机时。用户计算题目涉及计算物理、计算化学、材料科学、生命科学、药物设计、地球物理、流体力学、气候模拟、天文学、农业和计算机科学等领域。

目前，最大运行规模的应用为天文星系风模拟计算，利用了联想深腾 7000 的8192 个CPU 核。商业软件受软件许可的限制，一般规模都不大。用户自己开发的并行程序，规模一般可以扩展得较大，如1024 核的生物蛋白质 inspect计算应用、1024 核的空间天气MHD计算应用、开源软件Lammps 的 8192 核计算应用等。深腾 7000 在2009.6至 2009.7期间，64个CPU及以上规模的作业使用总机时的 $69 \%$ ，2048-4096个CPU 规模的作业使用总机时的 $31 \%$ 。

# 3.2.3在建的超算中心

上海超算中心和中科院超算中心是百亿次级别的超级计算中心，如何有效利用其计算能力仍然是一个严峻的挑战性问题。但同时，天津国家超算中心和深圳南方超算中心两个千万亿次的超级计算中心已经批准建设。

# $\bullet$ 天津国家超算中心

天津超算是国家科技部、国防科大和天津市政府于2009年联合成立的，运行国防科大研制的天河千万亿次超级计算机，是国家网格的一个主节点。最近世界上排名第一的超级计算机天河-1A已经投入使用，该系统将主要用于石油勘探和大型飞行器的模拟，此外还包括科学发现、金融分析、汽车和船舶设计。

# 深圳南方超算中心

深圳超算是由国家科技部、中科院和深圳市政府联合成立的，总投资约8亿人民币。中科院计算所研制的曙光6000将于2011年底落户深圳超算，为深圳、香港、澳门、台湾和南亚地区提供计算服务。据初步估计，来自深圳地区的计算需求约800TFlops，来自港澳台和南亚地区的约 300TFlops。曙光6000上的应用可能非常灵活多样，除了传统的研究和教育用户，深圳超算中心还将面向更实际和具体的应用，如Big-Sciences 项目、生物工程、动漫等。

目前，中国许多地方政府对于建立超算中心都比较热心。多个地方政府的建设计划已经获得批准。第三个千万亿次的超算中心也在酝酿当中，计划建在山东省，由济南计算技术研究所负责具体操作。成都市、北京市2009年各建立了一个20万亿次左右的超算中心，山东省、广州市、沈阳市等都有计划要建设地方的超算中心。地方政府建立超算中心的目的概括起来主要有以下几点：(1）来自于现实的应用需求：2006年之前，对高性能计算的需求多来自国家尖端应用，现在，经济发达地区区域经济的发展不断推高高等教育、科研、工业、信息服务等领域对计算能力的需求；(2)政治需求：高性能计算的水平是衡量一个国家和地区综合实力和竞争力的重要指标，地方超算中心的建立，可以提升地方在国家自主创新战略中的地位；(3）作为科技创新拉动经济增长的一个重要手段：地方政府建立超算中心，将其作为科技领域中的地方标志性工程，为企业提供公共的科技服务，有助于政府招商引资，吸引科技人才；(4)作为惠及大众的公共信息化服务平台：政府将家庭、社区、城市的各种信息进行融合处理，大大提高城市管理的效率，给市民带来便利。

然而，一个严峻的事实是国内在千万亿次系统的有效使用支撑方面明显滞后，甚至天津和深圳超算中心的技术人员，在管理和使用这样大规模系统方面仍存在准备明显不足的现象，这和对超级计算的应用需求和所需技术支撑缺乏足够的认识和必需的调研有一定的关系。

地方超算中心建立之后，将打造科技服务平台、产业创新平台、科研研发平台和人才聚集培养平台，在国家科研项目、企业创新、城市管理数字化、支撑基础研究等方面发挥重要作用。地方政府提供运维和服务经费支持，除上述上海、天津、深圳中心外，其他地方超算中心的建立则全由地方政府出资。绝大部分超算中心都是独立的政府事业法人，包括电费在内的运维费用、人员费用，大部分由政府出资支持。除了面向企业提供的有偿服务外，超算中心提供的科研、教育平台和公共信息服务平台都是免费的。超算中心的运营需要巨大的维护开销。以一个千万亿次的超算中心为例，每年的电费就需要约3000万元，长期来说，对政府是一个很大的负担。目前还缺乏有效的激励机制使这些中心提高运营效率和服务水平。

所以，成都超算中心探索了一个新的模式，由高性能计算机制造企业出资成立一个由企业建设、运行的超算中心，政府和企业用户按照云计算的商业模式，租用它提供的服务。

# 3.3网格基础设施

1999 年\~2000 年，863计划实施了国家高性能计算环境项目，建立了由5个高性能计算中心构成的国家高性能计算环境，形成中国网格的雏形。此后，基础设施建设和网格技术研究得到了政府的大力支持。主要的网格项目包括：国家863计划支持的中国国家网格、中国空间信息网格、高效能计算机和网格服务环境；国家教育部支持的中国网格；国家基金委支持的中国科学网格。其中中国国家网格和中国网格是两个最大的项目。

# 3.3.1中国国家网格

中国国家网格（CNGrid）是在863计划“高性能计算机及其核心软件”重大专项的支持下建设的。一期建设历时4年（2002年\~2005年)，国内有23家单位，700多名研究人员参加。在网格环境、网格软件和网格应用等几个方面，完成了一批重要的研究课题。二期建设周期是2006年至2010年12月底，重点在于进一步完善和改进高性能计算机和网格服务环境。

中国国家网格装备了自主研制的高性能计算机，由10个节点构成了开放的网格环境。其中北方主节点为中科院计算机网络信息中心的联想深腾7000，南方主节点为上海超级计算中心的曙光 5000A。其它节点分别设在清华大学、山东大学、中国科技大学、香港大学、中科院深圳先进技术研究院、华中科技大学、西安交通大学和北京应用物理与计算数学研究所。通过自主开发的网格软件，中国国家网格能够支撑网格环境的运行和应用网格的开发。

中国国家网格的应用集中在资源环境、科学研究、服务业和制造业4个领域，一期包括了10个行业应用网格：

国家地址调查应用网格航空制造应用网格中国气象应用网格科学数据应用网格新药发现应用网格生物信息应用网格数字林业应用网格仿真网格油气地震勘探应用网格交通信息服务应用网格。

二期的应用网格是：

中国气象应用网格  
水利应用网格  
天体大规模并行数值计算软件平台  
中医药数据网格  
高性能计算化学应用系统  
药物研发网格  
基于网格的铁路货运信息综合应用系统  
科学数据网格

这些应用网络在推进行业应用，共享行业内的资源上发挥了很好的作用，但有显示度的重大成果较少。

# 3.3.2中国网格

中国教育科研网格（ChinaGrid）是在教育部“十五”211工程的公共服务体系建设重大专项的支持下建设的。一期的建设时间是2003到2005年，目的是充分利用中国国家教育科研网CERNET上的计算资源和信息资源，建立聚合能力超16TFlops，总存储容量超过180TB，实现有效共享的国家教育科研服务平台。参加ChinaGrid 计划第一期建设的高校有 20 所，联合开发了中国网格支撑平台（ChinaGrid Support Platform,CGSP)。目前 ChinaGrid 上的典型应用网格包括：图像处理网格、生物信息网格、大学课程在线网格、海量信息处理网格和计算流体力学网格。这些应用网格在共享大学内的资源上起到很好的示范作用，推动了各大学高性能计算的普及应用。

第二期是从2009年到2010年底，校园网格规模从20所大学扩展到50所，主要任务有：（1）建设6个高性能计算网格中心；（2）围绕8个关键性的科学应用建设e-Research 网格，开发网格应用；（3）CGSP的研究与开发。

# 4 高性能计算联盟

# 4.1高性能计算机专业委员会

中国计算机学会高性能计算专业委员会（简称高专委）是中国高性能计算机会议（HPCChina）的主办机构，高专委的主任委员是陈国良院士，副主任委员是迟学斌、孙凝晖、漆锋滨，秘书长是张云泉。高性能专委的目的是：

$\bullet$ 促进中国高性能计算研究的发展，主办中国高性能计算年会（HPCChina);  
$\bullet$ 为研究人员、工业界人士和高性能应用人员通过免费的平台和会议提供专业培训；  
$\bullet$ 为政府在高性能计算战略规划上出谋划策。

中国高性能计算年会会议参照国际超级计算大会（SupercomputingConference）的模式，涉及的领域包括高性能计算机、高性能存储、高性能网络、高性能计算应用、数据分析、网格等，会议的形式包括学术会议、用户论坛、技术论坛、企业论坛、企业展览、研究机构技术展览、培训、研究生教育等，并且与中国软件行业协会数学软件分会合作，发布中国高性能计算机Top100 评选结果。中国高性能计算年会广泛吸引了学者、企业界人士、用户代表、研究生、媒体的参与，参会人员近300人。中国高性能计算年会与产业界和媒体长期合作，已经成为中国高性能计算领域的盛会，也成为国内外企业展示技术、与用户交流的重要平台。

# 4.2高性能计算机标准化委员会

在国家工业和信息产业部的支持下，高性能计算机标准工作委员会（简称高标委）于2007年3月28日正式成立。高标委是中国电子工业标准化技术协会的直属分支机构，接受信息产业部、民政部民间管理局的业务指导和监督管理。主要的目的是：

$\bullet$ 建立工业界、学术界和政府之间沟通桥梁；  
$\bullet$ 从事高性能计算标准化和相关标准的研究；  
$\bullet$ 提供标准化相关的培训，提高标准化意识。

政府有引导和制定标准的传统，但高标委希望通过企业基于公平开放的原则形成标准化的新机制。高性能计算服务器还不在国际标准化组织（ISO）之列，高标委设刀片式服务器、高性能计算机安全、机群操作系统、个人高性能计算机、高性能计算机应用、基础架构、节能和知识产权八个工作组。已经有曙光公司、联想公司、AMD公司、中科院计算所、Mellanox公司、东方通公司、Platform公司、北京市气象局等30余家机构加入。《机群操作系统远程监控技术要求》（SN:S07018-T）和《刀片式服务器管理模块技术要求》（SN:S07019-T）已通过电子行业标准立项，在 2008年成为行业推荐标准。当前，高标委的主要工作是与企业合作制定中国刀片式服务器标准，包括基础架构、监控管理、计算刀片、交换模块、存储模块等方面，得到了微软、IBM、英特尔等国际企业的合作。

为了促进产业界接受国内的标准，2008 年高专委成立了中国高性能计算产业联盟，其成员包括8家公司和2家研究机构。此外，还计划发展高性能计算应用的标准，如石油、天气预报、电信和国防安全。

# 5 高性能计算研究

中国的高性能计算研究涉及到比较广的领域，如微处理器、编译、系统、算法以及应用软件等。除了几个领先的研究机构从事高性能计算相关的关键技术研究外，还有超过300所的高校及行业应用的研究机构都在应用层面开展相关的研究。

# 5.1主要研究机构

中科院计算所、国防科大和江南计算所是公认的在中国开展全方面高性能计算技术研究的领先研究机构。其中国防科大和江南计算所主要涉及与国防相关的研究和应用。此外，清华大学在机群计算和高性能计算系统评测领域享有盛誉。

# 5.1.1中国科学院计算技术研究所

创建于1956年的中科院计算所被视为中国计算机产业、计算机人才以及计算机公司的摇篮。该研究机构由于独立自主研制出“曙光”超级计算机和类MIPS处理器芯片“龙芯”而受到全世界的关注。

![](images/b8ba2936e6504eccfc2dc4af09949eb36617bb4160271079f32517b365aa414b.jpg)  
图1．龙芯3八核结构

1．微处理器设计

龙芯处理器是中科院计算技术研究所研制的CPU，包括3个系列。龙芯3是目前中国设计的最先进的CPU。龙芯1是低功耗的处理器IP核，已经被应用于多种低端嵌入式系统中 (如基于龙芯的销售点终端和收税计算机等已研制成功并已投入市场)。龙芯2是面向桌面应用的低成本计算机的SoC处理器（如基于龙芯-2F的笔记本已投入市场)。龙芯3是面向服务器和高性能计算机的多核处理器，采用45 纳米工艺，龙芯3四核处理器将首先用于曙光刀片服务器和由中国科技大学陈国良教授领导的团队开发的 PHPC KD-60。8核处理器将用于曙光 6000 千万亿次高性能机和中国科大下一代PHPCKD-50-III。

8 核龙芯3号处理器的结构如图1所示，采用可伸缩二维Mesh互连结构，每个结点有两级AXI交叉开关，第一级AXI交叉开关连接四个处理器以及分成四个体的共享二级缓存（Cache)，并与东南西北四个方向的其他结点或者读写（I/O）进行互连，传递缓存一致性相关信息；第二级交叉开关连接二级缓存和内存控制器，采用标准的AXI协议。每个核拥有2个256位的浮点向量运算部件。

# 2. 编译技术

中科院计算技术研究所在编译方面的代表性研究成果是开发了英特尔安腾处理器的编译器ORC，该编译器已经在学术界和工业界广泛使用。与龙芯3处理器紧密相关的编译器开发工作包括二进制翻译和向量编译。针对 SPECCPU 测试程序，龙芯编译器的二进制翻译已经可以获得GCC $70 \%$ 的性能，龙芯3的每个核包括2个向量运算部件，因此需要向量编译。

# 3．高性能计算系统

中科院计算所多年来都处于中国高性能计算系统研究的最前沿，尤其是在技术路线研制方面，是 SMP、MPPl、Cluster 和超并行高性能计算体系结构的开创者和领导者。中科院计算所研制的曙光系列高性能计算机包括曙光1号SMP服务器、曙光 $1 0 0 0 { \mathrm { M P P } }$ 系统、曙光2000机群系统、曙光3000SMP机群系统、曙光4000X86/Linux机群系统以及曙光5000刀片机群系统。最近发布的星云作为曙光6000的高通量计算部分，在世界超级计算机中排名第3，其Linpack 性能测试达到1PFlops以上。曙光6000 的高性能计算部分基于龙芯3处理器与超并行体系结构，目前正在研发中。此外，中科院计算所还研制超级服务器、个人高性能计算机和高通量计算机。

除了通用高性能计算，计算所也开发了一些针对特殊应用的专用高性能计算机，如：面向基因测序应用的曙光4000H、面向网络安全应用的曙光4000L，正在研制面向国家“蛋白质大科学工程”应用的超龙一号和相应的并行算法、并行软件包。超龙一号是一台集合了龙芯3 四核处理器、现场可编程门阵列（FPGA）加速卡和GPU 加速卡的百万亿次高性能异构超级计算机。

在高性能计算相关的关键技术方面，计算所在生物信息处理的算法研究和高性能存储系统上也处于中国的前列。如将存储虚拟化到潜在的远程服务器上的大容量网络存储系统“蓝鲸”，最近已经商业化。

# 5.1.2国防科技大学

国防科技大学作为中国高性能计算机研制的主要机构之一，在处理器、编译、并行算法、高性能计算机等方面有较深入的研究。国防科技大学最近由于研制出中国第一台 PFlops 系统天河一号，特别是其升级版天河-1A，2010年11月荣登世界超级计算机TOP 500 榜首而受到全世界的关注。

国防科技大学研制出银河飞腾系列流处理器YHFT64，开发了一种新型的流编程语言——StreamFORTRAN95（SF95）及其编译器。该编译器采用了面向流体系结构的优化技术，包括循环流化（loop streamizing）、向量流化（vector streamizing）和流重用（stream reusing）等。国防科技大学研制的银河系列超级计算机已经被装备到气象和国防应用。

此外，国防科技大学在数值气象预报并行算法、遥感图像并行算法、分子动力学相关的并行算法、经典数学问题的并行算法上有深入研究，还开发了面向服务器的具有高安全等级的麒麟操作系统。

# 5.1.3清华大学

清华大学计算机科学与技术系成立至今已有接近50年的历史，在国内享有盛誉，在机群计算、CPU 设计、网格计算、网络存储和高性能系统评测等领域都有很强的研究实力。

# 1． 机群计算

清华大学的主要研究方向是并行计算所需的各种工具，如并行调试器、容错工具、性能调试、并行化编译等，另外还包括机群计算的实际应用程序的开发，如生物信息学、电力系统模拟与仿真等。近年来取得的典型研究成果包括：

$\bullet$ 基于用户级文件系统的文件操作缓冲技术；  
$\bullet$ 基于虚拟机技术的虚拟机群；  
$\bullet$ 数据回滚和序回滚相结合的并行程序调试技术。

# 2. 系统评测

2007年11月，863计划高性能计算机及其核心软件专项在清华大学成立了"863 高性能计算机评测中心"。该中心的工作主要包括高性能计算机测试标准和测试方法的研究、开发与实施，为高性能计算机系统的研制厂商与用户提供公正、标准的测试服务。在性能评测技术方面，该中心着重进行性能预测研究，通过建立分层次的性能模型，支持功能、性能故障的分析。

# 5.2并行算法

中国在并行算法的研究领域包括经典问题的并行化、多核处理器上的并行算法、大规模并行算法和并行编程工具等。中国目前有实际应用背景的并行算法的最大规模在4000 核左右，极个别算法能突破1万核并行度。中国在并行算法领域取得的典型成果包括：

$\bullet$ 中科院软件所在方程求解器和预条件技术上的研究;  
$\bullet$ 中国科学院计算数学与科学工程计算研究所在自适应有限元算法上的研究;  
$\bullet$ 北京应用物理与计算数学研究所在自适应多重网格算法上的研究；  
$\bullet$ 中国科学院过程工程研究所在多尺度方法上的研究。

# 5.3高性能计算软件

近年来，中国在高性能计算硬件研制方面取得了重要的进展，但是相对薄弱的软件开发和稀缺的高端应用限制了国家高性能计算的发展。大多数的中国大学和研究机构都活跃在高性能计算硬件的研究，但对相关软件开发却较少涉足。在工业界应用极为广泛的 $\mathbf { C A E } ^ { 1 1 }$ 软件，长期被以ANSYS、NASTRAN 和LS-DYNA等为代表的国外供应商垄断。高端的开源软件包也主要来自美国、日本以及欧洲国家。在计算规模方面，能够扩展到上千核规模的应用还非常少，而达到上万核规模的并行应用至今仍属空白。

尽管中国在高性能计算软件开发上整体比较落后，在并行软件开发平台和自主应用软件开发方面还是取得了进展。前者的代表性工作是中国科学院计算数学所的3维自适应有限元设计的并行程序开发平台PHG 和北京应用物理与计算数学研究所的并行自适应结构网格应用支撑软件框架JASMIN。自主应用软件包大多用于石油勘探、气象等传统高性能计算应用领域。突出的例子如 PRIS、GeoEast 和Grapes，这些软件均已成功在生产实践中得到使用。

# 5.3.1PHG

PHG(Parallel Hierarchical Grid）是中国科学院科学与工程计算国家重点实验室开发的专门为三维自适应有限元设计的并行程序开发平台（见图2)，其核心是分布式的层次网络结构。PHG 处理的网格对象是一维、二维三角形和三维四面体协调网格。PHG 采用C 语言开发，基于MPI消息传递机制实现并行。PHG 的研制得到了国家973项目“高性能科学计算研究”和国家自然科学基金的支持。

PHG通过面向对象的数据结构以及用户接口实现了并行网格剖分、动态负载平衡和网格局部自适应加密与放粗，在隐藏并行细节的同时为并行自适应有限元程序的开发提供了足够的灵活性。用户通过PHG 灵活的自由度(DOF)管理模块能轻松完成有限元离散和刚度矩阵的组装，通过线性解法器或特征值解法器接口能方便地完成方程组和特征值问题的求解。PHG支持将计算结果以VTK或

PETSc,HYPRE, VTK, PARPACK,   
Trilinos/AztecOO, OpenDX Trilinos/Anasazi,   
MUMPS,SuperLU, JDBSYM, SPOOLES, ↑ LOBPCG, Built-in PCG 可视化 PRIMME ↑ ↑ + 线性代数 PHG 特征值 求解器 求解器 ↑ Tcl/Tk MPI METIS/ParMETIS

OpenDX 的格式输出进行可视化处理。PHG 提供了与包括PETSc、HYPRE、Trilinos、MUMPS、SuperLU_Dist、SPC、LASPack、PARPACK、LOBPCG、SLEPC、Tcl/Tk、VTK、Zoltan、ParMETIS、METIS等众多软件的接口。

# 5.3.2 JASMIN

JASMIN框架由北京应用物理与计算数学研究所研制，全称为并行自适应结构网格应用支撑软件框架（J parallel Adaptive Structured Mesh applications INfrastructure）。JASMIN 框架针对科学计算中的结构网格应用，借鉴了美国劳伦斯利弗莫尔（Lawrence Livermore）国家实验室的SAMRAI框架，其架构如图3所示。JASMIN框架基于 $\mathrm { C } { + } { + }$ 和Fortran77语言编制，基于消息传递接口MPI实现并行计算，2009年2月发布了JASMIN1.5版。

JASMIN框架通过封装数据结构，集成成熟的数值算法，屏蔽大规模并行网格自适应应用的底层计算技术，支撑用户在物理建模、数值方法、高性能算法上的创新研究。基于该框架，用户无需熟悉并行计算、自适应计算和高性能算法的实现细节，根据离散格式、初边值条件和误差评估方法，就可以研制高效率的并行应用程序，求解偏微分方程。JASMIN 是一个开放的支撑软件框架，创新的数值方法和高性能算法可以不断地集成到该框架中。

JASMIN框架特别适于求解多物理、多尺度、多介质、多组分的复杂流动问题的并行应用程序的快速开发。JASMIN框架支撑“单块均匀矩形”和“单块变形”两类单层结构网格，并在此基础上支持由多个单块结构网格协调拼接而成的多块结构网格，以及自适应加密的多层网格结构。在自适应结构网格之上，JASMIN支持两类自适应计算，即h-自适应和 $\mathbf { r } -$ 自适应。

JASMIN框架的核心数据结构为网格片。通过调整网格片的尺寸，可以使并行程序更好地适应微处理器的多级存储体系结构。数值模拟过程中所输出的大规模数据场以HDF5的格式存储，可以由并行与分布可视化系统JaVis实施交互式可视化分析。

物理模型 计算参数 计算方法 高效算法 …专家经验 实现JASMIN的抽象接口函数 物理数学个性层：支撑JASMIN实际应用的抽象接口函数 接口函数 应用接口层Interface Function for User Application多时间积分算法 面向应用的计算工具 时间积分算法 数值共性层  
MultiHierarchyTimeIntegrator ApplicationUtilities 自适应算法时间积分算法 网格几何 求解器 几何述 JASMIN框架  
HierarchyTimeIntegrator Grid Geometry Solvers 计算工具箱网格自适应 数据传输 数学运算 数实现 并行自适应计算支撑层  
Mesh Adaptivity Communication Math.Ops. 数学运算网格网格片层次结构 数据片 变量Patch Hierarchy Patch Data 数据基本工具箱 TooIBox 基本工具

# 5.3.3 PRIS

PRIS 由中国科学院软件研究所并行软件研究开发中心开发，是针对油藏数值模拟的能够有效地处理井、断层等油田特性的并行线性求解器。1997年形成 PRIS1.0PVM版，1998年形成PRIS $1 . 0  { \mathrm { M P I } }$ 版，1999 年通过大庆油田百万网格点数据测试，2000 年4月通过了美国BakerAtlas SSI公司的测试，2000 年9月发布了PRIS2.0版。

PRIS 支持数据并行方式，适用于MPP 和机群体系结构的并行计算机。PRIS 采用克雷洛夫（Krylov）子空间方法求解线性方程组，其中克雷洛夫子空间方法采用混合预处理技术，集成了加法 Schwartz、 $\mathrm { I L U } ^ { 1 2 }$ 分解、克雷洛夫子空间迭代、约束剩余等预条件子。针对不同的迭代算法，PRIS 自动选取相应的预条件子组合。PRIS 采用独立的模块子程序来支持并行程序消息传递，目前支持MPI与PVM1。

# 5.3.4GeoEast

高性能计算在物探信息的数据处理和解释中扮演关键角色。在物探数据处理软件中，美国西方地球物理公司（WGC）的OMEGA和法国CGG 公司的GeoCluster在市场中占有大部分份额。面对中国石油物探业务的快速国际扩张，WGC于 2002年宣布，不再向中国石油集团出售OMEGA软件及其升级版本。法国CGG公司随后也提出了苛刻的出售软件的限制条件，包括软件仅限在中国本土使用，不能处理源自中国石油集团海外业务的地震资料，在软件使用的过程中要随时接受核查等。这些限制使中国石油集团的国际竞争力大打折扣。

在这样的背景下，2003年1月，中国石油集团投资1.4亿元，开始自主研发物探处理解释一体化软件。经过两年攻关，具有自主知识产权的GeoEast 物探处理解释一体化系统问世。这是中国油气勘探软件发展史上一个具有里程碑意义的事件，结束了西方公司长期在该领域的垄断历史。

GeoEast的技术特点包括：

$\bullet$ 用户环境：一个主控界面，用于激发与返回数据。  
$\bullet$ 数据环境：统一的数据接口，如转储功能接口、文件系统接口、磁盘存储接口等。支持Lustre 并行文件系统。运行环境：(1) 满足地震资料处理的特殊需求的作业调度功能;(2) 可视化的磁带设备管理界面与工具；(3） 对几百个功能模块的组装、加载工具；(4) I/O 管理工具。  
$\bullet$ 开发环境：基于交互、批处理、可视化三个框架的开发工具。  
$\bullet$ 核心应用功能：包括处理、解释、一体化三类。  
$\bullet$ 通信平台：类似于COBRA的RPC。

# 5.3.5全球/区域多尺度通用同化与数值预报系统

GRAPES 是中国具有自主知识产权的"新一代全球/区域多尺度通用同化与数值预报系统”。GRAPES 中的短时临近天气预报系统，曾作为唯一的中国发展的预报系统，加入世界气象组织北京2008年奥运会预报示范和研究示范计划，为奥运会提供气象服务。2009 年5月GRAPES被投入到中国气象中心准运作。

GRAPES在创新上有三个重要进展：

$\bullet$ 建立起中国首个能够直接同化卫星辐射观测资料的同化系统；  
$\bullet$ 采用多尺度通用动力框架作为不同应用模式的共同基础，实现了静力与非静力可选、全球与有限区域可选、水平与垂直分辨率可选的通用框架;  
$\bullet$ 在提高预报准确率、可用预报时效和预报精细程度方面具有较好的性能，水平精细度可以达到 $_ { 1 \sim 2 }$ 公里，可以对台风、暴雨等强对流天气取得较好的预报结果。

# 6 中国高性能计算应用与用户需求

目前中国的高性能应用主要集中在科学计算、能源、气象、工业仿真、金融等传统领域。同时一些新的领域，如影视动画、在线网络游戏对高性能计算的需求强劲，发展速度比较迅猛。在一些国家重大活动中，高性能计算起到了重要作用。比如2009年建国60周年阅兵中仿真装备对城市道路、桥梁可能的损伤；2010年上海世博会中仿真70万人群在发生意外时的疏散方案。

当今，中国高性能应用普遍存在，需要大量同时掌握高性能计算技术和特定应用领域知识的专业人员。然而，中国高性能计算发展的历史相对较短，只有一部分高校开设了高性能计算课程。在人才培养方面，目前国内科学计算中的自主开发的并行软件大多还是由计算机专业出身的科技人员将其他学科现有串行程序改写为并行程序，但是具备写并行程序能力的其他学科的研究人员数目呈上升趋势。以中科院超算中心为例，该中心每两个月就举办一次针对非计算机专业的应用人员的并行计算培训。在中科院研究生院的课程设置中，有针对非计算机专业研究生设置的并行计算选修课。

# 6.1科学计算应用

正如 $\ S 3 . 2 . 1$ 和 $\ S 3 . 2 . 2$ 提到的，中国两个最快的超级计算机一一上海超算中心的曙光5000A 和中国科学院超算中心的深腾 7000主要用于科学计算，说明科学研究依然是中国高性能计算应用的最主要领域。上海超算中心2008年的年度报告显示，当年共有 200 多个研究团体使用上海超算的计算资源。研究成果包括：114篇 SCI索引的论文和7篇发表于顶级期刊如《物理评论快报》（PRL）和《美国化学会志》（JACS）上的文章。中科院超级计算中心服务的对象主要是科学院内的各个研究所，应用基本上都是和科学计算相关。

科学计算具有一些共同的特点，如数据量庞大、计算复杂性高、多为开源软件。

大规模数据处理

典型例子：

中科院高能物理研究所正负电子对撞机实验 $\stackrel { \cdot } { = }$ 数据的处理。在对撞机中，每一对粒子的碰撞是完全独立的，在一次实验中可能会发生上万次这样的碰撞，探测器探测到的一次碰撞结果的数据量在1GB左右，所以要处理的数据量极大。由于高能物理是基础性的研究，同时受到实验设备的限制，高能物理领域的全球合作非常广泛，因而针对高能物理的软件几乎都是共享的。该应用软件分为三个层次：最底层的Linux 操作系统、中间开源的物理计算平台软件、上层的针对每个具体问题开发的应用软件。

位于西藏羊八井的中-意ARG0 实验探测宇宙射线数据处理。每次实验获得的数据是大量完全独立的数据组，但由于每次实验的代价很高，因而这些数据需要永久保存，以供以后的研究使用。目前的系统有6.4PB存储设备，2500个核的计算能力。随着探测手段的提高和实验项目的扩展，获得的数据量不断增加。用户需要一个统一的存储系统，而不是多个甚至多种存储介质，这就需要大规模数据管理的技术，同时大数据量也需要提高I/O吞吐性能。

超新星爆炸模拟

南京天文台的模拟超新星爆发过程的应用，在深腾 7000上使用8192个核进行了687亿个网格的计算，这是目前国内科学计算应用中采用核数最多的。得到计算结果以后，需要通过体绘制的方法将数据用图像的形式实时地显示出来，由于网格数巨大，所以这一部分工作也必须借助于高性能计算机。

# 6.2能源领域的应用

地球物理勘探(简称"物探")是利用地壳中岩 $( \sharp ) ^ { * } )$ 石物理性质的差异来研究地质构造或探测地下矿产的一门科学。它是以各种岩石和矿石的密度、磁性、电性、弹性、放射性等物理性质的差异为研究基础，用不同的物理方法和物探仪器探测地球物理场的变化，通过分析所获得的物探资料，推断、解释地质构造和矿产分布情况。目前主要的物探方法有：重力勘探、磁法勘探、电法勘探、地震勘探、放射性勘探等。其中目前工业界广泛采用的勘探方法是地

震波勘探法。

中国的油气探测领域，高性能计算在数据处理和油气探测信息的解释起到重要作用，并且该领域的应用需求仍在不断增长。以中国领先的地球物理服务公司（BGP）为例，虽然该公司已经拥有15,000个计算节点，总计算能力达到 200 TFlops，BGP仍希望每15个月将计算能力翻番。由于对高性能系统的依赖，BGP 同样面临高能耗的挑战。随着计算机规模的增大，基于刀片的集群所需的空间随之增大，水制冷系统将会更加复杂。

# 6.2.1油气应用中计算机发展历史

中国油气应用中高性能计算的发展历史与计算机体系结构的发展密不可分。如图4 所示，过去的40 年里，该领域使用的计算机系统共经历了4次重大变革。全球最大的地球物理服务公司（CGGVeritas）使用的计算机和BGP在这四个阶段的对比见附表D。

1．1970 年代—主机 $. +$ 数组处理机。数组处理机是一种外部向量协处理器，可以对数组进行操作，包括地震数据处理中常用的相关、褶积和快速傅里叶变换(FFT)。主机系统附加数组处理机后，价格只增加十分之一，而处理地震数据的性能提高四倍以上。中国石油应用使用过的典型系统包括：IBM2938数组处理机(1969 年)，IBM3838数组处理机(1974 年)和FPS 公司的 AP-120B 数组处理机(1975 年)。  
2. 1980 年代—向量计算机。使用过的典型系统包括：Cray-XMP(1982 年)，Cray-YMP(1988年)，IBM3090（1985 年）和国防科技大学的YH-1巨型机（1983年)。  
3. 1990 年代——工作站和并行计算机。交互处理应用在UNIX工作站上运行，批量处理应用在并行计算机上运行。使用过的工作站包括：DECstation3100（1989年)，IBMRISC System/6000（1990 年）。使用过的并行计算机包括：IBM Scalable PowerPARALLEL 2(1994 年，MPP)，Convex SPP-1000（1994 年，DSM)，SGI Origin 2000（1996年，cc-NUMA）。

4．2000 年代——机群计算机（Cluster)。主要使用CGG 公司的软件。2000 年CGG 推出GeoCluster，所有CGG软件都支持Linux，并基于机群架构优化。

![](images/47ba996662260d85001afa549fed62db2f14d5b844e1deddbf9e58a9d1aad154.jpg)  
图4.地震数据处理算法的复杂性随计算机技术发展

随着计算能力和存储容量的提高，油气地球物理计算方法也不断发展。在20世纪60年代，计算机只能够进行简单的地震道计算；20世纪70年代出现数组处理机（也称为褶积器或阵列处理机)，大大提升了褶积等运算效率，可以进行二维叠后地震成像；20 世纪 80年代的向量处理机，可以实现二维叠前地震成像，能够处理三维数据；20世纪90年代的大规模并行机，能够实现三维叠后地震成像，开始试用三维叠前地震偏移；进入21世纪，利用机群计算机能够实现三维克希霍夫叠前时间偏移。未来随计算机性能提升，三维波动方程偏移、全波动方程偏移、多分量成像技术将得到广泛的应用。在数据规模方面，为了提高预测精度，每个陆上地震探区由80年代的几百炮发展到今天的几万炮至几十万炮。每炮部署几百到几千个检波器，每个检波器每炮采集大约3000\~6000个数字化样点。这就是说陆上探区要产生若干GB至若干TB 数据。海洋探区产生的数据量更多，可以达到几十TB至几百TB 数据。

# 6.2.2油气高性能应用软件

根据地震探测中计算任务划分，高性能应用软件分为三种：地震数据处理、油藏数值模拟和计算可视化。其中地震数据处理对计算能力的需求最高。如表6所示，目前中国石油行业主要使用国外的商业软件，也采用一些国内自主开发的软件包。

表6.目前油气高性能计算应用软件  

<html><body><table><tr><td>种类</td><td>应用软件</td><td>供应商</td></tr><tr><td>地震资料处理</td><td>ProMax, SeisSpace</td><td>Landmark</td></tr><tr><td rowspan="4"></td><td>Geodepth,Focus</td><td>Paradigm</td></tr><tr><td>Omega</td><td>Western Geco</td></tr><tr><td>Geocluster</td><td>CGG</td></tr><tr><td>GeoEast</td><td>BP</td></tr><tr><td rowspan="3">油藏摸拟</td><td>VIP/ Nexus</td><td>Landmark</td></tr><tr><td>Eclipse/Intersect</td><td>Schlumberger</td></tr><tr><td>RMS</td><td>Roxar</td></tr><tr><td rowspan="3">计算可视化</td><td>Geoprobe</td><td>Landmark</td></tr><tr><td>Petrel</td><td>Shlumberger</td></tr><tr><td>VoxelGeo, GoCad</td><td>Paradigm</td></tr></table></body></html>

目前中国石油行业应用的商用地震资料处理软件主要来自国外厂商，有Paradigm 的 Geodepth 和Focus、WesternGeco 的Omega、CGG 的 GeoCluster。这些应用均为机群并行版本。目前国内自主开发的这类软件有BGP开发的GeoEast、中国石化南京石油物探研究所开发的iCluster，采用PSTM叠前时间偏移和PSDM叠前深度偏移的算法。应用软件计算精度和复杂度仍在不断增长。国外主要厂商WesternGeco 等已开始将RTM逆时偏移等消耗CPU资源更多、精度更高的偏移算法投入生产。国内的BGP等企业也开始尝试Para-digm公司的CRAM共反射角偏移模块

油藏模拟类的应用软件有Landmark 的 VIP/Nexus 和 Schlumberger 的 Eclipse。国内自主开发的相关产品主要有大庆研究院的PRBS并行黑油模拟。

# 6.2.3油气应用计算的需求特征

地震资料处理以求解数据密集的波动方程为主要计算模式，是典型的浮点计算密集型应用，对计算能力的增长有持续的需求。下图给出了各种地震资料处理算法对计算能力和存储空间的需求。图5中以当前广泛采用的克希霍夫时间偏移对计算能力和存储的需求为单位1，发展到逆时偏移对计算能力需求是目前的1000倍，存储容量扩大到目前的10倍，而理想的全波形反演算法对计算能力需求是目前的1百万倍，容量仅需要提升为目前的20倍。

目前地震资料处理应用最大的并行度能够达到1000节点以上，国内能达到的并行度并不高，在 $1 0 0 { \sim } 2 0 0$ 节点之间。限制扩展性的主要问题之一是广播通信瓶颈，当节点数目增大，广播开销呈线性增长。由于算法的限制，计算能力的提高很难通过无限增大节点数目实现。

目前的主要方法还是采用提高单节点计算能力实现。此外，地震数据处理不需要太多的内存

容量，内存带宽的占用中等；但需要较高的本地I/O带宽，这主要是由于其数据集较大，无法完全放入内存处理。

油藏模拟主要是使用稀疏矩阵方程的迭代求解，对内存带宽的要求非常高，并需要大缓存支持，可归为对内存带宽高度敏感的计算密集应用。中国石油用户期待针对这些需求特征能开发出更高效的高性能计算系统。

FW-Elastic  
10 全波形反演  
105The Holy Grail圣杯Reverse Time  
10 逆时偏移DW-WEM  
10² Shot向下延拓波动方程偏移KPrSDM(TTI) 地域波动方程偏移10 克希霍夫叠前深度偏移 图中园圈大小表示存储PrSTM CAZWEM(VTI) 共方位角波动方程偏移 与磁盘空间克希霍夫时间偏移 2012成像复杂性

# 6.2.4影响油气勘探高性能计算应用的新技术

新型体系结构处理器

通用GPU（GPGPU)。根据报道，对于频率域有限差分算法核心，用GPU相对CPU实现了8到16倍的加速。  
异构众核处理器架构。IntelLarrabee基于IA架构，扩展了新的指令系统，增加了新的向量处理部件和新的高速缓存。IBMCell处理器由1个通用PowerPC 核和8个 SPU（协同处理器）高速计算核组成。它们的峰值性能十分吸引人，研究人员正在探索如何利用这些处理器进行逆时偏移的计算。  
基于现场可编程门阵列的加速计算。把3DPSTM（三维叠前克希霍夫时间偏移）中最费时的核心部分编成现场可编程门阵列的协处理器程序,可提高地震成像效率十多倍。例如，德克萨斯A&M大学开发的SPACE（“具有重配置引擎的地震处理加速器"）在单一现场可编程门阵列中集成全流水线并行模块，克希霍夫求和能比Pentium4（2.4GHz）快15倍。地震处理中基于向下延拓的偏移处理和逆时偏移等大量采用的快速傅里叶变换、褶积，共方位角偏移、夹方位角偏移、弹性/声波正演建模、2D/3DSRME、波形反演和波动方程偏移速度分析等，也适合用现场可编程门阵列加速。例如，斯坦福大学SEP小组的研究结果表明，向下延拓偏移FK步获得8倍加速，逆时偏移3-D 褶积步获得5-8倍加速。津波（Tsunami）声称利用基于现场可编程门阵列的超级计算机Cray XD1，已经能够有效进行逆时波动方程偏移。

并行文件系统

地球物理应用既是计算密集型，又是数据密集型。机群具有高的性能价格比，但是也受限于输入输出和节点间通讯能力。地球物理程序员已经采取一些有效的优化措施，例如，采用大块通讯减少启动次数，优化区域分解减少要传送的数据量等。采用并行文件系统，以及利用I/O线程与计算线程并行执行，提升系统输入输出性能。目前机群采用的基于 NFS 文件系统的共享I/O 技术已经不能够满足需求，地震数据处理开始探索使用并行文件系统提升I/O 性能，例如 Lustre、PVFS2、Panasas ActiveScale。减少输入输出开销的行之有效的一种技术利用独立的输入输出线程与计算线程并发执行的"软流水”技术。当计算线程处理上一段输入的数据时候，输入线程输入新数据，而输出线程

输出已经完成计算的数据。

计算模式

20 世纪70年代，以IBM为代表的计算机厂商，曾经推动了地震数据批量处理技术变革。20世纪90年代，以微软为代表个人计算机应用技术，曾经推动了地震数据处理人机交互界面技术变革。今天，以谷歌（Google）为代表的海量数据处理技术，是否会推动地震数据并行处理技术的变革？中国的石油用户需要这样的网络化地震数据并行计算平台，这个平台需要具有以下特性：

能够利用大于1Pflops 的超级机群;  
能够利用网格计算技术形成资源共享环境；  
能够支持新型计算节点，比如每个节点20或更多的混合的 $\mathrm { C P U + G P U }$ 核，具有专门加速器;  
能够提供海量分布式存储和并行数据存取能力；  
基于这样的并行计算平台开发的新一代石油应用软件，需要具备以下功能：$^ *$ 提供并行应用程序的控制框架;  
$^ *$ 提供高性能的数据并行访问接口和通信接口;  
$^ *$ 支持地震数据并行应用程序的快速开发；  
\* 能同时用于地震数据处理过程和地震数据解释过程。

# 6.3气象领域的应用

数值天气预报就是通过求解流体力学、热力学微分方程组来描述大气运动规律。通过对该方程组的求解以获得最高气温、最低气温、降水量、湿度、气压、风向和风速7个未知量的时空分析，来预测未来一段时间内的大气运动状态和天气现象。由于数值天气预报所需的计算量非常大，并且计算的实时性要求高，必须利用高性能计算。

数值天气预报开始于20世纪20年代，由英国数学家理查德森（Richardson）提出。1950年，查尼（Charmey）等人首次利用电子计算机ENIAC进行数值预报。今天，数值预报已经成为天气预报的基础。数值天气预报也是高性能计算的一个传统领域。

虽然高性能计算系统的性能不断提升，对天气预报质量的需求——更高频度的天气预报和更精确的范围，也在不断提高。例如天气预报的频度从每天一次增加到三小时预报一次，甚至需要实时预报。天气预报的分辨率从1千米缩小到100米，在某些情况甚至是20米。随着高质量数值天气预报需求的增长，中国计划将高性能计算作为全国性的基础设施建设。中国将要建立大量的地方超级计算中心，安装计算能力为10\~50TFlops 的超级计算机。

# 6.3.1数值天气预报

中国数值天气预报的发展历史可追溯到1955年。从1969年国家气象局正式发布短期数值天气预报起，数值预报模式得以逐步改进。1982年，中国第一个数值预报业务系统—短期数值天气预报业务系统（简称B模式）在中型计算机上建立并正式投入业务应用。1991年，中国第一个中期数值预报业务系统（简称T42）在CYBER大型计算机上建立并正式投入业务运行。2002年9月，建立了全球T213L31全球中期分析预报系统（简称T213），预报水平有了质的飞跃。中国自主研发的应用软件全球同化与中期数值预报系统（简称GRAPES），成为中国新一代的中期数值预报业务系统的基础。2009 年，中国气象局由T213L31、Grapes_Meso、全球台风路径数值预报、T106L19中期集合预报和NMC 中尺度数值预报系统构成NWP基本业务体系。在气候模式系统方面，中国也建立起了由大气环流模式、海洋环流模式及其耦合的海气耦合模式、区域气候模式组成的气候动力模式系统，能够制作季节和年度气候预测。

# 6.3.2气象应用中高性能计算的挑战

随着经济和社会的发展，人们对气象服务质量的要求也越来越高，尤其是一些重大社会活动对气象预报的精度和时效提出了极高的甚至苛刻的要求。以2008年北京第29 届奥林匹克运动会为例，气象因素往往会对体育赛事产生关键的影响，例如风会对田径、射箭等比赛产生影响，高温湿热会对马术比赛造成影响，各专项体育比赛对天气预报都有很高的技术要求。气象部门需要在天气复杂多变的主汛期，提供每3小时预报的高精细化服务，预报落点要精确到每个场馆。为了做好北京奥运会气象保障工作，北京市气象局采用了峰值性能为9.8TFlops 的IBMCluster1600 机群和曙光4000A高性能计算机双机互为备份系统运行奥运会期间的数值天气预报业务。与2008年北京奥运会的气象保障工作相比，2010年上海世界博览会的气象保障工作更加艰巨。与北京奥运会相比，上海世博会的时间跨度长，且正值汛期等灾害性天气多发的时期。另外，世博会展馆多为临时建筑，大量应用新型建材，其防御气象灾害能力的不确定性有所增加，易造成次生灾害。这些都对上海世博会的气象保障提出了更高的要求，要求曙光5000A为上海世博会提供优质的气象计算服务。

数值天气预报经常会受到可用计算能力的限制。如果将三维空间分辨率提高两倍，时间步长缩短为原来的一半，需要的计算能力将提高16倍，精确度不断提高的预测也使得计算量不断增加。目前中国气象用户面对的更突出的问题是大规模并行数值预报软件的可扩展性较差，并行规模一般仅在128-256个处理器核左右。

# 6.4工程仿真应用

中国的工程仿真应用起始于航天工业，与发达国家的起步时间大致相同，而其应用领域却长期囿于航天工业、国防相关机构和一些研究所。近些年产品生命周期管理技术获得了各级政府的支持，同时中国厂商在全球经济危机中面临严峻压力，希望将工程仿真作为关键技术来改善产品和增进创新。近来，工程仿真在汽车、船舶、机械和电子工业等领域得以发展。

工程仿真既可以运行在超级计算机，也可以运行在各种小型计算系统和工作站上。随着硬件成本的降低，高性能计算在中国工业和制造业上广泛应用。中小规模的制造商通常使用公共超级计算中心的资源，如上海超算中心。然而中国工程仿真软件市场主要由国外厂商开发。一部分自主开发的系统虽然尽力商业化，却少有成功。大多数原因是由于缺少市场需求，而且点对点服务支持也比较薄弱。

# 6.4.1汽车设计

汽车碰撞试验在中国汽车工业广泛使用。汽车碰撞试验的主要领域包括非线性动力学仿真、疲劳耐久性分析、汽车外流场数值模拟、碰撞仿真。整车振动仿真和舒适性评价等方面对于整车设计也发挥着重要作用。基于高性能计算系统的碰撞仿真数值模拟越来越被广泛采用。

碰撞仿真着重于模拟车辆的安全性和碰撞时对乘员的保护，模拟操作可以由计算机迅速完成并且花费较低。厂商有机会在车辆投产前对设计进行进一步优化。例如国内厂商比亚迪（简称 BYD）的碰撞模拟着重在碰撞历程仿真和与其他车辆相撞时乘员的受伤情况，其中后者是不能在现实生活中测试的。碰撞仿真利用有限元技术求解复杂问题。该过程目前使用上海超级计算中心的32个CPU和商用软件进行仿真需要4至5个小时。中国汽车企业应用高性能计算机的主要动因是：开发一辆新车通常需要36个月和100辆汽车碰撞试验用样车，采用模拟计算之后，只需要14个月和少于10辆碰撞用样车，大大缩短了时间，降低了成本。汽车碰撞试验着重在碰撞时乘员约束系统对乘员的保护、碰撞过程仿真和车身安全性评价等方面。当前碰撞过程仿真主要是模拟速度为 $8 0 \mathrm { k m / h }$ 的汽车发生碰撞时在时间长度为 $1 2 0 \mathrm { m s }$ 的过程，约有100万个有限单元。

中国汽车制造商三强之一——上汽集团为了更方便快捷地使用高性能计算资源，铺设了与上海超算中心的网络专线，并购置了两台总共近 500个CPU的高性能计算机。上汽集团第一辆自有品牌Roewe750就是在这些高性能计算平台开发的。

奇瑞是国内为数不多的拥有研发能力并在车身、引擎和汽车零部件上拥有自主技术的汽车生产厂家。当前奇瑞自称拥有国内汽车工业最大的计算机辅助设计部门，并且把计算机辅助设计应用到整个研发过程。奇瑞还独立开发了一些针对自身产品的计算机辅助设计软件包，如车辆怠速时的性能分析、在任意路面的车辆行驶舒适度分析、数字虚拟原型动态分析的模拟平台(INTEDYNA/CHERY)和多种用途的显示有限元程序(C-SAFE)。奇瑞的新车型 A3轿车，在设计过程中与上海超算中心合作，成功通过C-NCAP碰撞，荣获C-NCAP五星安全评定。

# 6.4.2飞机设计

飞机制造业是中国工程仿真的发源地，从工业的视角来看，代表了工程仿真的综合需求。随着计算机辅助设计在中国制造业的普及，飞机制造业中的工程仿真已经高度成熟。其中飞行器的计算结构力学、流体力学、材料力学和电磁模拟技术是高级飞行器设计与制造的核心技术。此领域中主要的计算机辅助设计包括：整个飞行器设计、气动设计、结构强度分析、飞行控制、燃料系统和着陆装置设计。其中用到系统级协同仿真平台、多学科仿真和仿真过程控制。然而目前可用的商用软件包尚有一些关键的航空问题不能解决。飞机制造业还需要依赖自身的能力独立开发需要的软件。传统的飞机设计模式是将升力、推力、平衡、航向稳定和控制等功能指定于相应部件，由不同的项目组独立开发不同的模块，需要花费大量的时间进行协调。此外，还要进行大量的风洞试验，开发周期长、成本高。

中国的飞机制造业在1970年代末开始尝试发展数字化工程，并且在一些重点型号飞机的研究中加以应用。实现数字化后，在设计阶段，数字样机取代实物样机，数字化风洞试验取代 $90 \%$ 以上的气动选型试验，CAE仿真减少 $60 \%$ 以上的零部件强度试验和所有全机静力试验，可以实现快速的多学科、多目标优化，设计出最佳方案。

计算空气动力学（CFD）是流体力学的一个分支，使用数值方法分析并求解流体问题。在飞机研制过程中，从设计要求确定、概念初步设计，到详细计算、原型机试制、试飞以及设计定型，处处需要气动分析。除单个部件的气动计算外，还需要分析部件之间的融合设计。ARJ21-700飞机是中国商用飞机公司上海飞机设计研究所与上海超级计算中心紧密合作自行研制的首架中短途商用支线喷气式飞机。作为ARJ21的研发主体，上海飞机设计研究所通过ARJ21项目使用超过100万CPU小时用于CFD计算分析。

上海飞机设计研究所和上海超级计算中心还合作开展了大型客机的研发。曙光 5000A用于进行大飞机高低速机翼设计、大飞机翼身组合体设计、发动机吊挂设计等。计算流体力学还将用于解决大型客机研制中的各个方面问题，包括气动布局研究、部件优化设计、全流场分析、动力影响计算、结冰计算分析、气动噪音分析、环境温度分析等。

# 6.4.3船舶设计

中国是世界上排名第三的造船大国，出口市场遍及世界60多个国家和地区。在船舶设计中，船体外部流场分析及阻力计算、螺旋桨空泡模拟、复杂舱室的换热分析、发动机流场模拟、海水与空气的液面运动分析、船体结构的鲁棒性设计、船舶碰撞分析等，都需要高性能计算。

中国造船总公司下属的中国船舶及海洋工程设计研究院是中国船舶和海上建筑研究、开发、设计和工程方面规模最大并且综合性最强的机构。中国船舶及海洋工程设计研究院模型优化技术的发展经历了三个阶段。早期的模型试验阶段，模型的制造时间限制了可供选择的设计方案的数量；后来的模拟计算阶段，使用小型计算系统需要花费很长的时间得到最终结果；现在的高性能计算阶段，使用上海超级计算中心的高性能计算资源，原来需要近一个月计算时间的问题，现在一到两天就可以得到结果。然而中国的船舶设计应用的主要问题在于软件。目前造船业受限于国外成熟商用软件的昂贵价格。由于软件许可的限制，中国船舶及海洋工程设计研究院在使用上海超算的资源时一般仅仅使用32-64个CPU，应用本身良好的并行潜力没有充分挖掘，精度也得不到提高。

# 6.4.4机械和电子设计

计算机辅助工程分析在国内机械制造业的使用尚处于发展阶段。主要的领域包括：静态机械分析/动态仿真分析、模态分析、压力和形变分析、动态特征的仿真分析、稳定性仿真分析、屈曲仿真分析、疲劳仿真分析、部件干扰检测和虚拟数字样机。

电子工业中计算机辅助工程分析的应用情况各个公司不尽相同，但总体上看处于发展阶段。一些制造商可以有效使用计算机辅助技术，在不同设计者之间开发标准的工作流。一些公司将计算机辅助工程分析（CAE）集成到产品整体生产流程中。然而。大部分厂商只是最近在产品发展中引进CAE工具，仍需时间来改善使用技能。电子工业中CAE的使用领域包括：产品可靠性、切削加工性、疲劳寿命、冲击阻力、矢量振动和电子元件振动的耦合、适配器设计、热分析、电子芯片组装、焊接点的疲劳分析、振动与噪声分析、传输包装中跌落仿真分析、控制系统和机械系统的耦合分析。格兰仕是国内，甚至世界最大的微波炉生厂商，与中国科学院深圳先进技术研究院合作利用高性能计算系统设计和仿真微波炉封装，优化的设计每年为公司节省约1千万元（150万美元)。另外，一些大厂商也对多种物理场耦合仿真和产品动态工作环境进行了高性能计算优化。

# 6.5互联网应用

网络应用可以划分为信息获取、交流沟通、网络娱乐、商务交易四种类型，基本涵盖了目前的网络新闻、搜索引擎、即时通信、博客、网络游戏、网络音乐、网络购物、网上支付、网络金融等各种具体应用。

# 6.5.1中国互联网应用的历史

1999 年至2002年，中国互联网应用如网上教育、网上银行、电子商务、第四媒体出现并快速发展起来。1999 年8月，在全国高等学校招生工作中，六个省、市的二百余所高校使用“全国高校招生系统”在CERNET上进行了第一次网络招生。1999年9月，招商银行率先在国内全面启动“一网通”网上银行服务，成为国内首先实现全国开通“网上银行”的商业银行。1999 年9月6日，中国国际电子商务应用博览会在北京举行，标志着中国全面采用电子商务技术。2000年12月12日，人民网、新华网、中国网、央视国际网、国际在线网、中国日报网、中青网等获得国务院新闻办公室批准进行登载新闻业务，率先成为新闻网站。2001年，盛大网络在大陆运营韩国网络游戏《传奇》，成为中国网络游戏市场上的第一个霸主。

从2003 年开始，中国互联网应用进入了多元化阶段，并一步步拥有世界上最大的用户群。搜索引擎成为中国网民获取信息的重要入口，深刻影响着网民的网络生活和现实生活。截止2009年6月，国内使用搜索引擎的用户达到2.3亿，网络新闻读者达到2.6亿，电子邮件用户数量达1.8亿，即时通讯用户数量达到2.4亿，博客的用户数量为1.8亿，网上论坛用户数量1亿，网络游戏用户达到2.1亿，网络音乐用户2.8亿，网络视频用户2.2亿，网络购物用户8千万，网上预订用户1千万，网上支付用户7千万，网络炒股用户3千万。

# 6.5.2互联网应用的计算需求

# 1．系统规模

中国互联网应用具有大规模并且快速增长的用户群，系统通常需要面对海量的高达几千万的并发访问，常常需要至少几千个节点的计算能力。不同的应用业务经常不共享底层的硬件，有各自独立的系统。

# 2. 资源使用特征

大部分互联网应用所做的操作可以归纳为数据管理，即数据的移动、更新操作，各级存储系统的I/O是应用的性能瓶颈。中国互联网应用大部分属于在线服务业务，需要支持高并发访问，保证低延迟响应。虽然所有用户的访问涉及到的总数据量很大，但这些数据可以被很好地划分到不同节点上，并且有高效的索引支持，单个访问请求的IO量并不大。多级的缓存机制可以缓存住很大部分访问，因此用到的物理磁盘读写较少。

政府的互联网应用大多属于离线分析业务，主要是数据索引、数据分析、数据挖掘，并发度不高，但单个任务涉及的数据量巨大、执行时间长，磁盘读写是性能瓶颈。

# 3．软件

在线实时类应用的数据和访问特征通常可以很好地抽象为键值（key-value）模型。数据库可以支持这种数据模型，但互联网应用的数据访问方式相对简单且数据规模巨大。服务商通常对开源数据库进行裁剪，或者在文件系统之上开发新的针对这一模型的结构化数据管理软件。离线的数据分析类应用，多使用开源的Hadoop软件。

# 6.5.3网络游戏公司

盛大目前是中国最大的网络娱乐公司，主要业务包括盛大游戏、盛大在线、盛大文学。盛大游戏是核心业务，其基础设施包括分散在全国几个地点的约2-3万台PC服务器，划分为登录服务器、网关服务器、储存游戏数据的游戏服务器、计费服务器、用户数据中心等。盛大网络游戏的种类很多，但是大多数的规模并不大，用户数超过5万的就属于比较成功的游戏。

由于游戏对可靠性的要求不是太高，游戏数据每天备份即可，用户数据则采用分布式备份的方式。在安全性方面，用户数据中心的要求稍高一些，做了较多的防护；对于游戏服务，主要是在网络层上预防攻击。游戏对网络延迟的要求比较高，但网络延迟主要受限于国内的网络环境，盛大仅能针对服务器做少量优化。目前主要性能瓶颈是磁盘读写和网络包的处理，但还不严重，通过增加服务器的数量即可解决问题。目前游戏的开发主要使用图形工作站。盛大在2007年开发了虚拟化方面的产品，已于2008年应用在服务器上；同时对云计算相关技术有较大的需求。国产品牌的PC机群系统在网络游戏市场占据很大的市场份额，国内企业都能提供定制的游戏服务器。

# 6.6新兴应用领域

除了科学计算、天气预报、油气开发等传统的高性能计算应用，还存在着高性能计算应用的新兴领域，如医学、物流、金融、高级人机交互（如虚拟现实)。下面列出三种中国高性能计算应用的非传统案例。

# 6.6.1华大基因：基因测序研究

基因测序是指分析特定DNA片段的碱基序列，也就是腺嘌呤（A）、胸腺嘧啶（T）、胞嘧啶（C）与鸟嘌呤（G）的排列方式。DNA测序方法起始于20世纪70年代，使用基于2D层析法的“加减法”。随着测序技术和自动分析的发展，DNA测序变得更加容易并且速度也提升了若干数量级，大大加速了生物学研究进程。从20世纪90年代至今，大规模基因研究已经完成了对多种有机体完整测序，如人类、细菌、酵母菌和果蝇。

华大基因建立于1999年，参与了国际人类基因组计划，并完成其中 $1 \%$ 的工作。以后陆续参与了国际人类HapMap工程（ $10 \%$ )、超级杂交水稻基因测序、蚕基因工程、中-英鸡基因工程、首个亚洲二倍体基因工程和 SARS 研究，建立了基于大规模基因测序的技术平台，进行有效的生物信息学分析和创新性的基因卫生保健。这使得华大基因的基因测序和分析能力位居亚洲第一和全球第三。华大基因于2001年和2007年分别在杭州和深圳建立了分部。

基因测序所产生的数据量巨大，在拥有新的测序仪后，华大基因每天新增数据量在10TB左右，整个数据库的规模达到PB级别。对如此庞大的数据进行基因序列的拼接、比对、排序、识别和功能分析，需要高性能计算环境。基因测序应用的计算负载通常可以均衡地划分到各个计算节点上，并且计算单元之间的通讯非常少。但整个计算过程会涉及到多次的磁盘读写，因此成为性能瓶颈。另外，一些应用，如集合和交叉种类定位，需要大量的内存空间，同时TB级甚至PB级的磁盘空间来存储数据和定位的结果也是不可缺少的。

华大基因维护了三个高性能计算中心，分别位于深圳、北京和杭州，总计算能力达到52TFlops，内存总量为10.3TB，磁盘总量达到5.1PB。目前，华大基因对计算能力的需求是200TF。其高性能计算环境包括曙光2000、3000、4000H，SGIO2，Sun E10K，Origin 3800,IBM p690及自己开发的生物信息学Linux机群（简称BLCs)。应用软件大多数是开放源码软件，也包括一些自己开发的并行软件。

华大基因计划将来为个人提供基因测序。目前为个人进行基因测序的成本大概在10 万美元/人，随着技术的发展，不久的将来会实现1000美元/人的低成本测序。大众化基因测序服务是华大基因的未来目标，基于低成本的高性能计算与存储设备的云计算平台将会成为他们新的选择。

# 6.6.2上海证券交易所：证券指数计算

上海证券交易所是中国目前最大的证券交易机构。上海证券交易所的证券计算包括前端数据处理、后端交易处理和证券指数计算。上海证券交易所用到的高性能计算系统大多是IBM-PC服务器机群，商业软件包括专用操作系统openVMS 和统计软件 SARS，每年的许可费用高昂。前端数据处理主要处理私人投资者的数据，可以并行化。后端交易系统对性能要求高，订单处理能力的要求为3万笔/秒。例如中国建筑上市当天的处理量就为500万笔，相当于2006 东京交易所一天的处理量。查询、成员申报、交易的撮合等都需要提供实时的支持。后端系统对可靠性和数据备份要求高，需要专用操作系统的系统级备份和灾备中心，如果10分种不能正常处理业务就必须报到国务院。

证券计算应用对浮点计算能力要求不高，对定点处理能力要求高。白天主要是高吞吐量，对 CPU 和内存要求高，晚上对磁盘处理能力要求高。其中，证券指数计算业务需要为100多家券商提供计算支持，能够对基金公司一到两年历史数据进行处理。指数计算为CPU 密集型，需要使用高性能计算机，还需要在物理机层面提供隔离。

高性能计算越来越广泛地用于处理银行和安全数据。2008年全球金融危机爆发导致的经济低迷。这使人们期望高性能计算将来能像天气预报一样用于预测金融危机。因此投资和金融的数值模拟逐步成为中国的新热点应用。

# 6.6.3水晶石公司：动漫应用

虽然电影、电视和动画产业在其他国家已经发展了20余年，在中国却是新兴产业。文化工业最近被中国中央政府选为近期刺激经济发展的支柱产业。随着生产技术的提升，国内团队逐渐采用高性能计算来模拟特效和进行渲染。水晶石公司是渲染应用中的领先企业之一，主要业务领域为建筑设计图渲染、电影动画、数字城市，代表性用户包括北京奥运会、伦敦奥运会、电影《赤壁》、动画片《福娃漫游记》。

水晶石采用了曙光高性能刀片机群系统，软件主要为商业软件3Dmax 和Maya，3Dmax有些插件支持多核的并行计算，目前还很少采用GPU进行渲染计算。水晶石公司使用高性能计算机主要用来渲染场景。渲染一个 $1 0 2 4 \times 1 9 2 0$ 大小的电影和动画场景中的图像需要1个CPU小时左右，而渲染一幅分辨率达到 $4 0 0 0 0 \times 3 0 0 0 0$ 的建筑设计宣传图，在80个核的机器上需要8个小时左右。北京奥运会开幕式所需的渲染应用，由于采用了曙光高性能计算机，将以往需要一个月完成的任务缩短到一个星期。

渲染应用的计算独立性高，每一幅图像都可以单独计算，不需要相互通信，即使对于$4 0 0 0 0 \times 3 0 0 0 0$ 的大场景，也可以分块进行计算。另外，渲染应用看重图像的质量。这类应用既不是交互的（如3D）也不是实时的（如在线游戏和虚拟现实)。

# 7 中国高性能计算企业

受益于新兴领域应用软件的扩展，中国的高性能计算市场正在快速增长，并且这种增长将会持续下去。像惠普（HP）和 IBM这样的跨国公司占据了领导地位，但一些本土企业，如曙光、联想、浪潮，正在努力增加它们的市场份额。中国高性能计算企业经过15 年的发展，从无到有，在产品上已经可以和国际企业竞争，在服务上有一定优势，在技术上还有很大差距。中国企业要在市场占有量和技术竞争力上再上一个台阶，面临着十分巨大的挑战。

今天，高性能计算的使用者不单只希望购买到一套系统，还希望从供应商那里得到全面的解决方案。在这方面跨国公司相比本土企业具有更大的优势，因为前者在复杂系统上具备更丰富的解决方案开发经验。而那些缺乏行业解决方案经验的供应商将面对更多的挑战，甚至失去部分市场份额。本章将主要介绍国内几家较大的高性能计算供应商和另外两家较小的供应商。

# 7.1曙光

曙光公司（Dawning，全称曙光信息产业有限公司）成立于1995年，由中国科学院计算技术研究所和其他几家单位共同出资。然而不同于其他投资者的是，中科院计算所的出资中包含了价值 2000万人民币的无形资产（折合美元约290万)，这部分无形资产来自于其对曙光I号的开发所形成的知识产权。在过去的十年中，曙光已经成长为中国本土高性能计算供应商的领导者。在国产品牌的高性能计算机市场，曙光高性能计算机连续11年居第一位，拥有 $70 \%$ 以上的份额。而中科院计算所也成为曙光公司的技术基地。曙光每年将其销售收入的 $5 \%$ 用于研发投入，并且建立了一个约30人的研发中心。

曙光公司与中科院计算技术研究所为上海超算中心研制的曙光5000A在2008年11月公布的全球最快超级计算机排行榜上排名第十，同时也是前十名中唯一来自美国之外的高性能计算系统。曙光 5000A采用AMD 的Opteron1.9GHz四核处理器，总计拥有 30720 个计算核心、122.88TB 内存，通过Infiniband 4XDDR 技术互连，其峰值性能达到 233.472万亿次，Linpack 性能达到180.6万亿次。目前计算所与曙光公司正在联手开发千万亿次的曙光6000高性能计算机。2010年六月发布的“星云”，是曙光6000的服务分区，在2010年六月的 Top500上排名第二。此外，曙光公司在以下领域在中国处于领先位置：

$\bullet$ 曙光公司自行开发了中国首个刀片服务器TC2600。每个刀片节点在7U的机箱内集成了10个基于AMDCPU的4路SMP刀片或IntelCPU的2路SMP刀片、双机群管理网络、双千兆以太网、Infiniband网络、 $5 + 1$ 冗余热插拔电源、3个风扇、10个PCIEX8扩展槽。2009 年九月在英特尔开发者论坛上发布的 TC3600 刀片系统是第一个同时符合SSI 标准和 HPCSC 相关标准的刀片系统。它具有基于PCIe的扩展能力、灵活的Infiniband/Ethernet开关、系统管理模块、电源、磁盘存储等；曙光公司推出的机群管理软件GridView，能在网格环境下集中管理多个机群，支持远程KVM，具有能耗管理功能，在中石油BGP的应用案例中节约耗电 $30 \%$ 为了提高高性能计算技术支持和服务水平，建成了全国性技术支持网络和首席应用工程师团队，并在北京建设了高性能计算用户体验中心。

# 7.2联想

联想公司（Lenovo）是中国最大的电脑供应商，由于其收购 IBM个人电脑业务为世人所知，自 2001年开始涉足高性能服务器业务。2002 年联想开发成功的基于IA架构的深腾1800 高性能计算集群是第一个进入Top500排行版的中国本土超级计算机，当年排名43。2003年，联想开发的基于64X86集群架构的深腾 6800 在 Top500上排名第14。为商业计算研制的深腾2600 IA64集群于2004 年发布后被中国的科学、商业和网格计算用户所广泛采用。联想开发的深腾 7000 在 2008 年11月的 Top500 上排名第19位，峰值性能超过120TFlps。然而联想高性能计算研究开发小组的一些核心人物最近离开联想去了北京航空航天大学，虽然他们仍然与联想保留合作开发关系。

2007年，联想为AT&T威廉姆斯车队提供了价值超过一千万元人民币的8Tflops机群系统，用于F1赛车的设计。这是中国企业第一次向海外提供这样规模的高性能计算机。联想相信，作为一家跨国公司，他们能生产和提供这种产品的能力说明了中国制造的高性能计算机能够参与国际市场的全球竞争。

# 7.3浪潮

浪潮公司前身是山东电子设备厂，生产计算机外围设备和低频大功率电子管。目前其经营范围已覆盖了个人电脑、服务器、税控机、通讯产品、信息安全、行业应用软件及软件外包、技术支持服务和存储设备。浪潮是目前中国最大的服务器制造商和服务器解决方案提供商。

浪潮的员工超过5000人，总部和产品基地位于济南，三个市场中心分别位于北京、上海和广州，研发中心位于北京、上海、济南和青岛。公司同时在美国加州圣何塞建立了一个

30 人的小型研发中心。此外在日本、香港、新加坡都有其海外研发部门。浪潮同时专注于政府和教育行业，并在这些市场上表现出色。

浪潮是国内最早涉足服务器研发和制造的供应商之一。2003年公司提出了“灵活部署”（FlexibleDeployment）概念并开发了相关技术和产品。其天梭系列的服务器是高性能计算领域的关键产品。除了建造系统外，近年浪潮还加强了对高性能计算应用的研究。公司还与华中科技大学共同组建了一个高性能计算生物信息联合实验室，以及与北京航空航天大学联合组建了一个高性能计算虚拟现实实验室。正如之前在\$3.1.2中提到的，浪潮正在开发受863项目支持的基于IntelCPU和QPI接口的32 路高端容错计算产品。

# 7.4宝德

位于深圳的宝德科技集团有限公司（PowerLeader）是一家主要从事服务器制造的民营企业。作为英特尔公司在中国最重要的战略合作伙伴之一，以及英特尔安腾解决方案联盟(ISA)的唯一中国成员，宝德专注于IntelIA架构的服务器开发。通过与法国布尔（Bull）公司合作成为国内目前唯一提供32路SMPCC-NUMA小型机的服务器厂商。

宝德公司搭建的高性能计算系统安装在深圳华大基因研究中心用来进行亚洲黄种人基因组项目和大熊猫基因组项目。

# 7.5红神

上海红神信息技术有限公司（RedNeurons）创立于2005年，公司创始人是美国纽约州立大学石溪分校（Stony Brook）应用数学系教授邓越凡。在中国科技部和上海市科委以及一些合资公司的支持下，红神开发了一个能够支持应用程序运行的原型系统。该系统使用 16个 IBMPoWerPC 低功耗CPU 模拟它设计中的16个CPU核，用现场可编程门阵列实现了一个性能相当于千兆以太网的互连网络。

红神公司 2008 年向科技部的国家高科技863 计划申请了RedNeurons-2项目，申请经费1.2 亿元（其中科技部 6000万，上海市6000 万)，进行高性能计算机的探索性技术的研究，预期用2年时间完成一个全部采用现场可编程门阵列实现的、体现新型体系结构和创新技术的、相当于100万亿次通用计算能力的原型系统。然而该项目还存在一些争议，一些中国的科学家仍对RedNeurons设计中采用的方法存疑。

# 8 2009年中国高性能计算百强（Top100）

我国有一个最快的前一百名计算机系统排行榜，称为“中国 HPC TOP100”。该排行榜由中国软件行业协会数学软件分会、中国软件行业协会和863计划高性能计算评估中心共同维护。自 2002 年起每年秋季发布。至今，一年一次的中国 HPC Top100 已经得到研究人员、用户、厂商和政府部门的广泛认可，成为观察中国高性能计算现状的重要窗口，很多用户购买高性能计算机的必要参考和评标依据。

2009 年11月1日，中国HPC Top100 排行榜 2009 版正式发布，其中Top10见下表7。2009年中国Top100排行榜主要体现出了以下几个趋势：

$\bullet$ 国产系统继续位居榜首，且在历史上首次出现峰值超过1PFlops 的机器，比预期时间提前一年;  
$\bullet$ Top100 总体Linpack 性能达 2.2PFlops，与 2008 年相比，性能提升了2.12 倍，标志着中国高性能计算机市场已处于快速增长期;

Topl00 平均Linpack 性能达 22TFlops，而 Top500 平均Linpack 性能达到 22TFlops 的时间是2008 年6月，中国与世界的差距保持在一年半；$\bullet$ 上榜系统的性能门槛大幅提高，Linpack 性能需要达到6.8万亿次，是 2008 年的 2.04倍；$\bullet$ 96 套系统采用机群结构，机群结构继续占据主导地位，计算机体系结构创新面临挑战；$\bullet$ 首次出现CPU+GPGPU混合加速的PFlops机器；$\bullet$ 国内服务器品牌首次在上榜数量、总体性能上超过国外品牌。

表7.中国TOP100 排行榜 2009 年TOP10  

<html><body><table><tr><td>序号</td><td>研制</td><td>型号</td><td>安装地点</td><td>年份</td><td>应用领域</td><td>处理孩</td><td>LinFack s </td><td>(G峰ps)</td></tr><tr><td>1</td><td>国防 科大</td><td>天河一号</td><td>国家超算 天津中心</td><td>2009</td><td>科学计算/ 工业</td><td>24576</td><td>563100</td><td>1206210</td></tr><tr><td>2</td><td>曙光</td><td>魔方-曙光</td><td>上海超级</td><td>2008</td><td>科学计算</td><td>30720</td><td>180600</td><td>233472</td></tr><tr><td>3</td><td>联想</td><td>深腾 7000</td><td>中国科学院 超算中心</td><td>2008</td><td>科学计算</td><td>10096</td><td>106500</td><td>145293</td></tr><tr><td>4</td><td>IBM</td><td>Blad2Ceuter</td><td>网络公司</td><td>2009</td><td>工业/游戏</td><td>7168</td><td>38790</td><td>72540</td></tr><tr><td>5</td><td>IBM</td><td>BladeCeuter</td><td>网络公司</td><td>2009</td><td>工业/游戏</td><td>7168</td><td>38790</td><td>72540</td></tr><tr><td>6</td><td>IBM</td><td>BladeCeuter</td><td>网络公司</td><td>2009</td><td>工业/游戏</td><td>7168</td><td>38790</td><td>72540</td></tr><tr><td>7</td><td>IBM</td><td>Blad2Ceuter </td><td>南京大学 计算物理国家</td><td>2009</td><td>科教计算/</td><td>3200</td><td>31310</td><td>34048</td></tr><tr><td>8</td><td>曙光</td><td>曙光5000 BladeCenter</td><td>重点实验室</td><td>2009</td><td>科学计算</td><td>3360</td><td>31048</td><td>40320</td></tr><tr><td>9</td><td>IBM</td><td>HS22 Cluster</td><td>网络公司</td><td>2009</td><td>工业/游戏</td><td>5376</td><td>31030</td><td>54410</td></tr><tr><td>10</td><td>IBM</td><td>BladeCeutr</td><td>网络公司</td><td>2009</td><td>工业/游戏</td><td>5376</td><td>31030</td><td>54410</td></tr></table></body></html>

# 9 中国高性能计算技术和应用发展趋势

中国的高性能计算技术正在向多样化发展，以满足不同应用对系统和软件的不同需求。从以往同质化的机群和单纯追求计算速度，向异构体系结构和应用主导的计算发展。同时，在应用类型方面，与以计算为中心的应用相比，以数据为中心的应用正变得越来越流行。

# 9.1新高性能计算项目

# 9.1.1个人高性能计算机（Personal High-Performance Computer，PHPC）

个人高性能计算机（PHPC）是一种普及型超级计算设备，开发动机是促使超级计算设备能普及到所有的科学家和工程技术人员，以及所有需要高性能计算的各领域的用户个体。但是，简单地把高性能计算机小型化远远不能满足个人高性能计算机系统的实际需求。个人高性能计算机核心技术包括：低功耗处理器、适合桌面环境的系统设计、面向大众用户的并行编程模型以及与PC兼容的使用环境等。目前，国内已有一些个人高性能计算机系统面世。到2015年，万亿次级别的个人高性能计算机有望成为主流产品。下文对国内的个人高性能计算机工作进行简述。

在中国科技大学计算机系，由陈国良院士领导的团队开展了KD-50个人高性能计算机研究，在2007年12月完成了一台如图6所示代号为"KD-50-I"的基于龙芯2F 处理器的万亿次系统。该系统是一台使用千兆以太网的单机柜机群，共使用336颗750MHz龙芯2FCPU，其中每个1U机箱内放置12颗CPU。2008年，KD-50-1又生产了2台。

2010年4月，该团队又发布了KD-50-II万亿次系统。它包含 $1 0 \times 1 \mathrm { U }$ 计算节点和2U服务节点，共使用80颗龙芯3A四核处理器和160GB内存。该系统理论峰值运算能力为1TFlops，但大小仅相当于KD-50-I的三分之一，也即比普通洗衣机还小。另外，它的功耗为 $2 . 4 \mathrm { k W }$ ，不足KD-50-I功耗的 $56 \%$ 。

此外，该团队计划在2011年底推出KD-50-III万亿次系统。这是一台使用定制的64端口千兆以太网的桌面式机群系统，共8

![](images/6eb148a62f19aca5f7d174f51de06432b0be224694b100d07055cfeee9f207f8.jpg)  
图6.KD-50-I（a）和KD-50-II（b)

颗1GHz的带向量处理部件的8核龙芯3处理器，在系统软件和并行编程方法上将更关注个人用户体验。

中国科学院计算技术研究所和英特尔公司合作，推出了一款基于16个低功耗的英特尔4 核Xeon 处理器的桌边型（deskside）个人高性能计算机原型系统，整机电流小于5安培，使用定制的64端口千兆以太网交换机作为互连。该系统在2009 年的英特尔北京信息技术峰会（IDF）、英特尔中国研究院开放日、SC09、HPCChina09等会议上进行了展示。

曙光公司在 2008 年也推出了曙光天潮 PHPC100 个人高性能计算机。这是专门为办公室应用环境设计的桌边型高性能计算机产品，其定位与CrayCX1类似。它具有10颗4核处理器，使用Infiniband互连网络，采用整体优化设计，有效地降低了系统功耗。

从产业化角度分析，如何定位个人高性能计算机系统的用户群是需要考虑的问题。英特尔和AMD的片上多处核理器芯片已经成为面向大众的并行计算系统，可以满足普通用户绝大多数的应用需求；以云计算为代表的要求对计算资源集中管理的使用模式，是普及高性能计算的另一条道路。广大科研工作者、金融分析师、设计与规划人员、生物学家能成为个人高性能计算机的潜在用户群体，但是其多变的计算类型和各异的计算特征使得一套通用系统解决方案很难同时满足各种用户的需求。这就要求个人高性能计算机的处理器具有一定的可定制性，而一台个人高性能计算机系统的性能应该比PC高一个数量级以上。未来个人高性能计算机的用户群体将是各行业内对高性能计算有特定需求的人群，系统也将分为固定的桌面型和移动的膝上型。

# 9.1.2 GPU Cluster

GPU 最初是面向图形和游戏应用开发的高性能处理器，其中NVIDIA是主要的生产商之一。目前，不论西方还是中国，都将GPU用于更加通用的应用中作为一个重要的研究课题。

GPU 机群是一种在标准机群的计算节点中增加GPU 加速卡的高性能计算机，其性能和兼容性已经得到中国许多用户的广泛认可，国防科技大学开发的Tianhe-1和由中科院计算所与曙光公司合作开发的Nebulae都采用了这种技术。

中国科学院过程工程研究所获得了约1亿元人民币的资助从事多相反应研究，研究颗粒和流体同时存在的系统中物质运动、传递和转化过程的共性规律。他们研制的属于分子动力学范畴的粒子模拟并行算法具有鲜明的特点：并行度大、计算密集、数据量小、通信具有明显的局部性。MDGRAPE、QCDOC、Anton 等专用高性能计算机解决的就是这类问题，这类问题也非常适合用GPU进行计算。

过程所于2008年搭建了一台代号为Mole-9.7（图7）的GPU机群用于粒子模拟应用。该系统使用了120台HPPC机，搭载了240块NVIDIAGPU图形卡，处理能力达到单精度200TFlops。其网络采用 2D-mesh 点到点的连接方式，即一台PC 配置4个千兆以太网卡，不经过交换机进行两两互连。此外还有一台用于全局通信的以太网交换机，组装方式是将120台立式PC放置在2排共30个架子上，由普通立式空调进行散热。2009年，Mole-9.7升级后峰值性能达到 450TFlops。此外，过程所还基于GPU 建立了一个全新的系统，即Mole-9.7F，峰值性能达到单精度150TFlops。同时，曙光、联想公司分别为过程所研制了2台单精度 200 TFlops 的基于 Infiniband 的GPU 机群系统，即 Mole-9.5L 和Mole-9.5D。四组GPU 机群通过千兆以太网互连成一个多层体系，其性能达到了单精度1PFlops。中国科学院还在10个研究所推广了这个项目中曙光、联想公司研制的GPU机群产品。

在 2010 年早些时候，过程所使用NVIDIATeslaC2050GPGPU搭建了另一个GPU机群，即Mole-8.5。这个系统包含372个计算和数据处理节点。单个节点是4U双路Xeon服务器，与6个Tesla C2050GPGPU耦合，并且通过infiniband互连。系统的理论性能峰值达到双精度1.138PFlops。

![](images/2ecffc471ac1b731b92fa7a7ea2689ff5d46a2a89ee53987b8d3f30981781d80.jpg)  
图7．Mole-8.5多尺度离散模拟系统

我国本土供应商生产的几款GPU机群系统

已经进入商业化阶段。由曙光公司开发的GPU机群代号为GHPC1000，其计算节点采用单机箱双节点（Twins）技术，在2U 的空间内提供4个AMD 四核处理器、2块GPU卡、8块 SSD硬盘。GHPC1000互连网络可以是Infiniband网络或千兆以太网。同时，浪潮公司推出了倚天 Tesla HPC 机群，采用 NVIDIA Tesla S1070 计算系统搭建。宝德公司也推出了PowerScale 8000G，包括一台PR4710D四路四核服务器，配备2块Nvidia Tesla1070 GPU。此外，还计划把Nvidia Tesla系统引入宝德PS1000个人超级计算机中。

# 9.2技术发展趋势

# 9.2.1众核机群

过去 20年微处理器技术的发展让中国的高性能计算机的水平迅速追上国际水平，通过X86多核处理器和机群技术中国目前有能力建造千万亿次系统。

通过曙光 5000A的技术路线，使用12800 颗左右的AMD12核"马尼库尔"处理器，约3200 个刀片，可以构建能耗在2兆瓦内的PFlops 系统。从产业的角度考量，这样的技术路线在未来依然是满足大多数中小型高性能计算用户需求的主要方式。国际上，正在研发的BlueWater在 2012年有望能达到20PFlops 的性能。在这样级别的高性能系统，面向主流IT市场的通用X86多核处理器将无法解决在功耗、编程、可靠性等方面的挑战。

拥有片内大规模并行能力，具有一定显式存储访问特点的GPU、CELL、现场可编程门阵列等处理器，与通用处理器一起构成的异构系统在部分应用领域已经成为用户的选择。美国的 RoadRunner 和中国的 Tianhe-1 系统问鼎 Top500，GPU 机群在中国流行就是有力的证明。这种专用部件加通用平台的方式，将成为中国未来高性能计算机的一种有效结构，但由于处理器体系结构的专用性质，不可能成为市场主流技术。

中国学术界普遍认为基于能扩展到千核规模的众核处理器的并行系统（大规模并行或星群或机群）是未来高性能计算机的主流技术，尤其是众核机群将会是市场主流产品。同时，龙芯处理器的研制成功和产业化使中国拥有了研制科学计算领域通用处理器的能力和市场基础。中国学术界目前在众核机群上的研究主要集中在并行编程模型、并行算法设计、并行系统冗余设计三个方面。

$\bullet$ 如何定义千核处理器的编程模型，以及设计与之对应的编程语言是一个十分具有挑战性的问题。中科院计算所的Godson-T64核处理器、国防科大的流式处理器和基于MIPS的64核处理器研究，都试图在保证良好的可编程性的前提下提高众核的性能。  
$\bullet$ 并行算法的可扩展性是决定一个应用在一个包含数以千计的处理器芯片、数以百万计的计算核的超大规模并行系统上的运行效率的重要因素。加州大学伯克利分校在他们的技术白皮书中也指出，未来科学计算的负载将呈现出多样与非规则访存的特性。中国学者正在算法和体系结构支持两个层面研究如何支持细粒度多维度并行性以及如何应对非规则访存。随着高性能计算机系统规模的持续增加，对容错性的支持显得尤为重要。单纯通过物理器件层面的改进将无法满足未来系统需求。中国学者正在容错的并行算法，自诊断、自恢复的操作系统上开展相关研究。

# 9.2.2高通量计算机

面向高通量应用（High Throughput）的高性能计算机（HTC）是未来应着重研究的技术方向。传统的高端计算机主要分为两类：高性能计算机(High performance computers，HPC)和高可靠计算机（High reliability computers,HRC)。高性能计算机追求单个并行应用的计算性能，主要用于科学计算领域；高可靠计算机则追求系统及应用的高可靠性和高可用性，主要用于商业计算领域。以海量数据和基于互联网的服务为特点的新兴应用，如谷歌，则大多具有高通量的负载特征，而且中国的网络用户众多，吞吐率的瓶颈问题将更加突出。

高通量计算机定义为“提供交互式、高并发负载服务的高可扩展、低成本的大型计算机软硬件系统”。高通量计算与传统的高端计算机相比有如下一些显著差异：

$\bullet$ 性能高通量计算追求单位时间内所处理的并发负载数量而非单一峰值计算速度；  
$\bullet$ 耦合度 高性能计算的处理单元之间耦合度高，应用的性能依赖于负载均衡、同步与通讯开销；高通量计算应用中通常存在大量的并发任务，单任务内的耦合度高，不同应用任务间耦合度低，很少需要全局同步与通信；可靠性 高通量计算应用的任务间松散的耦合关系，使得软硬件部件的故障不会影响整个系统;  
$\bullet$ 成本 高通量计算系统对成本比高性能计算要敏感得多，谷歌等公司因此会自己开发专门的服务器和网络交换机。

高通量计算这一概念由工业界提出，谷歌、亚马逊（Amazon）、Facebook 等国外公司已经展开了相关的研究，但主要集中在应用层面，在系统层面则多是一些技术集成和解决方案方法；Sun（升阳）公司的T1 处理器可以说是一款高通量计算处理器。国内对高通量计算的研究还处于起步阶段，在支持数百TB数据的并发存储、索引、并发访问的数据管理系统上有一些研究。中国学者认为需要首先研究如下关键性问题：

$\bullet$ 由于学术界难以对工业界高通量计算应用进行跟踪，要理解用户行为、分析应用特征，必须通过设计合理的性能测试标准（Benchmark)，定量地分析不同高通量计算应用的特征;  
$\bullet$ 为高通量计算系统专门设计可支持海量并发线程的处理器，并按吞吐率优先的原则优化访存模式;专门设计可支持海量并发轻量级线程、资源能够按细粒度供给的系统软件栈，包括操作系统、运行时环境；  
$\bullet$ 高通量计算应用的一个重要的特点就是海量数据的大规模并发处理，这就要求按吞吐率优先原则设计数据的存储、传输、访问模式。细粒度轻量级线程级并行编程模型将更适合于高通量计算应用。

# 9.3应用发展趋势

# 9.3.1基于海量数据的高性能计算服务

基于海量数据的科学研究和服务正日益成为当前的主流，互联网服务就是其典型应用。例如，谷歌和百度的互联网搜索服务，亚马逊和淘宝的B2C 服务，Facebook 和校内网的 SNS服务，Fliker 和GooglePicasa的图片服务，Google Earth 和IBMSmarter Planet的地理定位服务。除了上述互联网应用外，下面几个基于海量数据的高性能计算服务在中国正不断发展：

$\bullet$ 生物信息处理服务基于基因序列库、蛋白质序列库、蛋白质结构库的高性能计算服务；  
$\bullet$ 智能信息处理服务 如基于大规模实例学习的统计翻译、大规模领域知识库的构建和应用；面向大众的教育服务采用云计算模式，实现中国数亿学生的普及教育；  
$\bullet$ 国民健康工程为中国的全体国民提供低成本的体检服务，并建立全民健康档案;  
$\bullet$ 感知中国工程这是中国政府正在大力倡导的与IBM智慧地球对应的工程，前端是对物理世界的感知，中间是基于宽带无线通信技术的数据传递，后端则是基于海量传感数据的智能处理与决策；  
? 基于大科学工程的计算服务大科学服务基础设施，如上海光源（同步辐射）、LAMOST天文望远镜、上海蛋白质科学研究基础设施、深圳散列中子源、北京正负电子对撞机等，都产生海量的数据，而科学发现高度依赖于这些数据以及高性能计算。

在互联网应用领域，谷歌的GFS、BigTable 和MapReduce，雅虎（Yahoo!）的HDFS,微软的基于有向无环图的数据流编程模型Dryad，谷歌的大规模图结构编程模型 Pregel，正成为事实上的工业标准和它们各自的核心技术。中国需要开发类似的技术以满足上述应用在数据存储和管理技术方面的需求。此外，以数据为中心组织计算的计算模型，提供不间断服务的容错技术，近似精确计算模型等方面的需求也需要新技术的支持。这些新兴应用也蕴含着新的商业机会。

# 9.3.2舆情计算

互联网可以与真实的人类社会建立映射关系。对互联网上的内容、用户行为的监控、分析可以等价于对社会和消费者的研究，这一方式正成为中国政府、媒体、企业十分关注的新兴应用。

舆情泛指在一定的社会空间内，民众对社会管理者持有的社会政治态度，在商业活动中所持有的生活态度等。舆情计算就是通过面向网络的自然语言分析、信息提取、热点分析、信息检索、文本聚类、主题检测与追踪、观点倾向性识别、自动摘要和信息预测等技术发现

这些态度，从而指导政府或者企业决策。

舆情计算涉及到对大规模互联网日志、互联网文本数据进行处理、分析。其中，与高性能计算相关的技术包括：数据的存储、数据的移动、数据的分析，分布式的分词提取方法，大规模并行数据库，并行查询方法，超大规模实时查询等。舆情计算也是高通量计算机的一个杀手铜应用。

中国通过863计划资助的科研项目建立起来的舆情分析系统包括以下核心功能：（1）热点和敏感话题的识别；（2）人群倾向性分析；（3）主题跟踪；（4）针对对各类主题和倾向的自动摘要；（5）趋势分析；（6）突发事件分析与预测；（7）对特定事件、话题的及时发现与报警；（8）统计报告与决策支持。但现有的系统无论在处理数据的规模上，还是在数据分析的能力上都远远不能满足用户的需要。

# 10与国际发展的比较

上世纪90年代以来，中国已经在高性能计算领域取得了重大进展，掌握了一些关键技术，形成了自己的产业，高性能计算得到广泛应用。但是，与美国、日本和西欧相比，在系统开发层次、应用层次和长期规划方面，依然存在差距。下面以中国研究人员的视角，从几个重要方面与国际水平进行对比。

# 10.1 高性能计算系统

![](images/d1d2520953c24414d0eccacf992156b227b2922b1e675104f3961728acce370c.jpg)  
图8．主要国家占有率演变图（1993-2010)

随着中国经济的发展和在计算机系统领域研发的进展，中国在TOP500 中席位由1990年代的一两台发展到进入本世纪来的几十台（如图8所示)。在2010年6月的 TOP500 列表中，中国以24台系统在总数上与德国并列排名第四，份额在TOP500 台系统中占到 $4 . 8 \%$ 。中国在TOP500 中所占份额显示了国内市场对高性能计算机的需求呈上升趋势，与中国经济占世界经济的份额基本相当。

美国始终是高性能计算领域的领导者，其霸主地位无可争议。在2010年6月的排行榜中，美国有 282台系统。其中，TOP10中的7台（表8）都安装在美国。我们通过比较过去

50年中中美有代表性的高性能系统，以年来衡量两国的技术差距，如表9所示，中美的技术差距正在缩小。当2004年曙光 4000A在TOP500 中排到第10位的时候，这个差距缩小到两年；而星云系统则进一步将差距缩短到仅仅一年。但是，在将来一段时间内，这个差距将持续存在。

根据中国TOP100 的数据，高性能计算总性能正在快速增长。中国TOP100系统的性能总和从2008年到2009年已经翻了一番多，由1.036PFlops增长到2.2PFlops；同期，TOP500中所有系统的总性能的增长则是1.63倍。中国TOP100系统的平均Linpack 性能在2009 年是22TFlops；作为对比，世界TOP500系统在2008年6月的数据则是23.4TFlops。在Linpack平均性能方面，中国落后了将近1.5 年，也就是摩尔定律的一代（18个月）。2009 年12月，世界 TOP500 和中国 TOP100 的平均Linpack 性能比是2.5,小于2008 年的3.27，这说明中国正在追上世界水平。

表8.主要国家所占份额（2010年6月）  

<html><body><table><tr><td>国家 数量</td><td>份额</td></tr><tr><td>美国 282</td><td>56.40%</td></tr><tr><td>英国 38</td><td>7.60%</td></tr><tr><td>法国 27</td><td>5.40%</td></tr><tr><td>中国 24</td><td>4.80%</td></tr><tr><td>德国 24</td><td>4.80%</td></tr><tr><td>日本 18</td><td>3.60%</td></tr><tr><td>俄罗斯 11</td><td>2.20%</td></tr><tr><td>瑞典 8</td><td>1.60%</td></tr><tr><td>加拿大 7</td><td>1.40%</td></tr><tr><td>意大利 7</td><td>1.40%</td></tr><tr><td>新西兰 7</td><td>1.40%</td></tr></table></body></html>

表9.中国与国际同行的技术差距  

<html><body><table><tr><td>计算机</td><td>参照</td><td>中国</td><td>美国</td><td>差距 (年)</td></tr><tr><td>103机</td><td>ENIAC</td><td>1958</td><td>1954</td><td>13</td></tr><tr><td>109B机</td><td>IBM7090</td><td>1965</td><td>1959</td><td>6</td></tr><tr><td>150机</td><td>IBM360</td><td>1973</td><td>1964</td><td>9</td></tr><tr><td>757向量机</td><td>Cray-1</td><td>1983</td><td>1976</td><td>7</td></tr><tr><td>曙光1000</td><td>Paragon</td><td>1995</td><td>1990</td><td>5</td></tr><tr><td>曙光2000</td><td>IBM SP2</td><td>1998</td><td>1994</td><td>4</td></tr><tr><td>曙光4000</td><td>EarthSimulator</td><td>2004</td><td>2002</td><td>2</td></tr><tr><td>曙光5000</td><td>BlueGene/L</td><td>2008</td><td>2006</td><td>2</td></tr><tr><td>天河-1</td><td>Roadrunner</td><td>2009</td><td>2008</td><td>1.5</td></tr><tr><td>曙光星云</td><td> Jaguar</td><td>2010</td><td>2009</td><td>1</td></tr><tr><td>天河-1A</td><td> Jaguar</td><td>2011</td><td>2011</td><td>0</td></tr></table></body></html>

在体系结构方面，中国要落后于其他居于前列的国家。中国2009 年TOP100 中有96个（2008年为92个，2007年为75个）系统是机群，机群不但继续占主导地位，且份额还在继续快速上升，大有一统天下的趋势。国际TOP500虽然也是这个趋势，但机群仅占到 $8 3 . 4 \%$ ，远低于中国2009年 TOP100中的 $9 6 \%$ 。2009 年6月公布的最新 TOP500 中，只有3套机群系统进入

TOP10，表明美国、日本等国家在采用新体系结构的高性能计算机方面积极探索，而在中国还看不到这种转变，体系结构日趋同步化。在超级计算机的体系结构创新上，国家投入力度不够，研究机构也缺乏创新勇气，这将会影响国产高性能计算机的发展后劲。

# 10.2 高性能计算应用

尽管在硬件和系统方面，中国与发达国家的差距在迅速缩小，但是在高性能计算应用方面却不尽如此。中国高性能计算社区普遍认为这是未来中国高性能计算发展的瓶颈。中国学者认为，中国的高性能计算总资源还不能满足国内用户的需要。尽管中国已经拥有一些性能很高的高性能计算系统，但是大部分用户却难以获得足够的计算资源，许多应用因此受限于计算规模和求解精度。商业软件的昂贵价格和拥有自主知识产权的软件的缺乏是中国高性能计算应用发展的主要障碍。此外，发展高性能计算应用的生态环境也有待建立。中国已经意识到，必须改善跨学科的教育和培训以及国家信息网络基础设施的性能和服务，并建立一个有效机制来促进计算机硬件、并行算法、软件工程和高性能计算应用等方面的研究人员之间的短期和长远合作。

从应用领域来看，中国10TFlops级以上的大型高性能计算系统主要分布在能源、信息服务和科学计算方面。表10是世界TOP500计算机和中国10万亿次/秒以上计算机应用领域的比较，以及拥有世界TOP500计算机最多的9 个国家在应用领域的比较。从这些数据可以得出一些结论：

表10. 世界T0P500计算机与中国10万亿次/秒以上计算机应用领域的比较  

<html><body><table><tr><td>类别</td><td>世界 TOP500</td><td>中国 （10T以上）</td></tr><tr><td>科学计算</td><td>185 37.00%</td><td>10 18.50%</td></tr><tr><td>能源/地球物理</td><td>56 11.20%</td><td>21 38.90%</td></tr><tr><td>气象</td><td>10 2.00%</td><td>3 5.60%</td></tr><tr><td>电信</td><td>10 2.00%</td><td>3 5.60%</td></tr><tr><td>金融</td><td>58 11.60%</td><td>1 1.90%</td></tr><tr><td>政府应用</td><td>8 1.60%</td><td>5 9.30%</td></tr><tr><td>工业制造</td><td>70 14.00%</td><td>0 0.00%</td></tr><tr><td>信息服务</td><td>103 20.60%</td><td>11 20.40%</td></tr><tr><td>合计</td><td>500 100.00%</td><td>54 100.00%</td></tr></table></body></html>

$\bullet$ 中国用于科学计算研究的高性能计算机比例偏少。世界TOP500计算机中 $3 7 \%$ 安装在

国家实验室、大学、公共超级计算中心用于科学计算，并且其中几乎囊括了最强大的计算机系统，而中国用于科学计算的高端计算机不到 $20 \%$ 。  
中国用于金融业和制造业的高性能计算机比例偏少。2008年12月的世界TOP500系统中，58台计算机用于金融业，70台用于工业制造，包括航天航空、汽车、建筑、医药、半导体工业、计算机硬件和系统设计等，而中国仅1台超过10万亿次计算机用于金融业，制造业几乎是空白。  
中国虽有11台超级计算机用于信息服务业，但其中却有6台在游戏公司，而世界上主要用于信息处理与服务和企业管理。  
中国用于能源领域的高性能计算机比例很高。这与中国的石油企业的强大和国家能源战略是密切相关的。

如表11所示，2008年12月的TOP500中，有447台安装在9个国家。与其他8个国家相比，中国在科学计算和制造业领域的应用方面相对薄弱。

表11． 拥有世界TOP500计算机最多的国家分应用领域的比较  

<html><body><table><tr><td>类别</td><td>美国</td><td>英国</td><td>法国</td><td>德国</td><td>日本</td><td>中国</td><td>意大利</td><td>印度</td><td>俄国</td></tr><tr><td>科学计算</td><td>84</td><td>12</td><td>8</td><td>14</td><td>14</td><td>3</td><td>7</td><td>6</td><td>7</td></tr><tr><td>能源/地球物理</td><td>46</td><td></td><td>2</td><td></td><td></td><td>4</td><td>1</td><td></td><td></td></tr><tr><td>气象</td><td>5</td><td></td><td>1</td><td></td><td></td><td></td><td>1</td><td></td><td>1</td></tr><tr><td>电信</td><td>1</td><td></td><td>1</td><td></td><td>1</td><td>5</td><td>2</td><td></td><td></td></tr><tr><td>金融</td><td>35</td><td>8</td><td>7</td><td>3</td><td>1</td><td></td><td></td><td></td><td></td></tr><tr><td>政府应用</td><td>6</td><td>1</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>工业制造</td><td>57</td><td>6</td><td></td><td>2</td><td>2</td><td></td><td>1</td><td></td><td></td></tr><tr><td>信息服务</td><td>57</td><td>18</td><td>7</td><td>5</td><td></td><td>3</td><td></td><td>2</td><td></td></tr><tr><td>合计</td><td>291</td><td>45</td><td>26</td><td>24</td><td>18</td><td>15</td><td>12</td><td>8</td><td>8</td></tr></table></body></html>

在应用高性能计算求解问题的规模上，中国和世界先进水平的差距更大。除Linpack外，中国尚未有求解十万亿次以上规模应用问题的实例。而美国和欧洲已经有许多百万亿次量级的应用。当前在先进国家的百万亿次计算机上，会有上万个处理器并行工作。中国目前应用的并行计算规模，绝大多数只用到几十到几百个处理器，只有极少数应用能用好上千个处理器。中国的硬件研制能力与软件应用水平差一到两个数量级。因此，中国如何用好千万亿次计算机，充分发挥它们应有的作用，显然是一个大问题。

上述问题与中国在高性能计算领域长期以来重硬件、轻软件、更轻应用的政策是分不开的。中国的科技计划还没有对一个应用软件或一个并行算法项目投入超过千万人民币的例子。国内学者认为，在软件开发的政府资助方面中国应该向日本学习。对于个人用户来说，软件应该是免费的。而对于企业用户，则应该收取一定费用。JASMIN 和PHG 这样的应用平台，应该得到政府的大力资助，从而获得持续发展。另外，超大规模并行（VLSP）的研究平台也亟需获得国家资助和支持。

# 10.3 国家长远规划

高性能计算作为国家的战略支撑技术，它的研究和建设是政府行为，需要制定国家规划，并给予长期稳定的资金投入。美国通过在1983 年实施的"战略计算机（SCP）计划”、1993年实施的"高性能计算与通信（HPCC）计划”、1996年实施的"加速战略计算创新（ASCI）”计划及随后的"先进模拟和计算（ASC）"计划，对高性能计算的持续发展进行部署。2005年6月，美国总统信息技术咨询委员会（PITAC）提交了《计算科学：确保美国竞争力》报告，再次将高性能计算的战略地位提升到国家核心科技竞争力的高度。

中国目前对高性能计算尚没有一个互相关联的统一规划，缺乏一个有效的、长远的国家目标，缺少对国家行为的有效规划和组织。突出的问题表现在：

$\bullet$ 在国家层面没有达到美国、日本的重视程度，没有国家级整体科技规划；  
$\bullet$ 国家缺乏对高性能计算机核心技术的研发投入，把技术创新与满足用户生产性需求经常混同起来;  
$\bullet$ 在国家的整体部署上各自为战，缺乏整体考虑。比如973、自然科学基金、863、产业化项目之间缺乏统一协调；对高性能计算应用的投入严重滞后于系统开发；高性能计算系统研发、计算基础设施平台建设和应用程序开发由关注重点不同的多个部门负责，因而无法形成协同效应；  
$\bullet$ 当前，中国的高性能计算系统中机群体系结构占主要地位，这是因为在开发阶段应用需求没有被充分考虑。这样，就无法提供有效的相应高端系统以满足对系统性能要求很高的一些重大应用的特殊需求。

# 11挑战与机遇

# 11.1 挑战：Exascale级计算技术的竞争

1986 年世界上最快计算机Cray2 达到1Gflops（十亿次每秒)，1997 年世界上最快计算机Intel ASCIRed 达到1Tflops（万亿次每秒)，2008 年世界上最快计算机 IBMRoadrunnner达到 1Pfolops（千万亿次每秒)，高性能计算机技术在 22年从G 级计算，到T 级计算，再到P 级计算，从现在到2020年世界各国的竞争是E级（Exascale，百亿亿次每秒）计算。光靠摩尔定律和大规模并行已经不足以实现这一目标，必须依赖器件的革命和编程的革命。

当前，无论是从单个CPU的计算能力，还是从访存与通信带宽来看，现有硬件实现技术还远没有达到实现Exascale级系统所需的能力。首先，芯片引脚数的限制使得在现有封装技术下无法达到所需的通讯带宽延迟比。访存延迟与带宽的性能需要至少联合提高1000 倍才有可能满足Exascale 级系统的要求；再者，虽然从 $\mathrm { I T R S } ^ { 1 4 }$ 预测的工艺路线图中可以看出未来集成电路工艺尺寸、工作电压、计算能耗比等重要参数会持续减小，但是如何保证在额定功率下设计并实现包含上千个核的CPU是一个极具挑战性的问题。总之，需要在物理器件层面取得突飞猛进的提高，3D封装、新半导体材料、光互连等更多新兴技术需要取得突破。

每一次编程技术的革命都推动着高性能计算的发展。Fortran 使得计算科学成为理论研究、实验科学外的第三种科学发现手段；Vector和OpenMP 使得并行成为可能，并行度达到十数量级；MPI使得大规模并行成为可能，并行度达到百、千、万数量级。那么什么样的并行编程能使得超大规模并行（VLSP）成为可能，并行度达到百万、亿数量级呢？什么样的高层抽象并行编程方法能让广大程序员驾驭复杂层次的并行系统呢？

未来五到十年内 MPI，OpenMP 作为主流并行编程环境的地位不会受到动摇。UPC、Co-array Fortran、Cray Chapel、IBM X10、Sun Fortress、CUDA、OpenCL 等这些编程语言都还远不是一种理想的并行编程语言，还在继续发展。从算法和应用的角度，设计出与 E级系统体系结构适应的多层次细粒度的并行算法的任务将变得更为复杂、艰巨；随着系统规模的增加问题规模也要增大，否则同步开销将抹杀并行效果；非规则访存也是要解决的问题。

在 E 级计算的竞争中，中国与美国、日本相比，在器件和编程上都不占优势，中国的战略、技术路线和研究重点应注意以下3个方面：

$\bullet$ 速度和市场同步增长：过去20年中国高性能计算机的速度与市场规模是同步增长的，而E级计算这样的高端技术用户群狭小，缺乏足够的市场支撑。中国应学习美国的UHPC计划，在发展E级计算的同时，更加注重P级的单机柜系统和T级的可移动高性能计算设备。另外，发展E级计算的技术应考虑同时满足高通量计算的需求。  
$\bullet$ 速度和用户群同步增长：过去20年中国高性能计算机的速度与用户群也是同步增长的。机群技术路线在普及高性能计算上起到十分重要的作用。中国必须坚持机群技术路线，将低成本、易用、易编程放到最重要的位置，重点发展与个人高性能计算机相关的软件技术，并将E级计算的主要技术用到个人高性能计算机中。  
$\bullet$ E级计算的应用：中国应发展有自己特色的E级计算应用，在满足国家重大需求的技术创新中采用高性能计算技术。

# 11.2 机遇：并行软件成为市场主流

在过去的几十年里，单处理器的速度一直按照摩尔定律快速发展，但是未来将无法延续这种趋势；面向众核体系结构和发挥片内并行性的并行软件将成为未来提高性能的主要方向。并行软件将成为市场主流，这是中国在高性能计算领域走向世界的机遇。

在科学与工程计算领域，由于软件市场长期完全由国外软件所垄断，许可费用昂贵（表12)。随着高性能计算普及时代的到来，中国经济从应用技术走向创造技术，科学与工程计算并行软件市场会扩大几个数量级。中国应抓住这一机会，大力发展并行应用软件产业。

另一方面，战略性的、高水准的工业工程数值模拟，如大飞机设计，迫切需要高水平的自主开发的大规模并行软件。目前市场上的国外软件大多发展于半个世纪前，软件框架未能考虑现在及未来的超大规模计算需求。特别是前处理部分，已经被证明成了大规模计算时的严重瓶颈。软件架构已不适应新型体系结构，需要重新设计，这也正是中国的机遇。

此外，基于互联网的应用模型，如云计算以及其他更广泛的计算模型，将会改变传统的操作系统的角色。操作系统将会成为基于互联网的系统的一小部分。新的商业模式也给发展面向大众的低成本的高性能计算软件带来了机会。例如,NewServers 公司提出了Hardware asa Service（HaaS，硬件即服务）的概念，使得企业可以通过NewServers 公司的数据中心来维护可扩展的、可负担表12．国外软件在中国市场上并行规模和售价（估计）统计

的、安全的自己的数据中心；Gompute 公司提供了以一种按需的高性能计算使用方法，允许用户通过互联网使用高性能的计算资源，并按照实际使用的资源付费；Mellanox公司提出了 HPC as aService（HPCaaS，高性能计算即服务）的概念，为用户提供更加灵活和有

<html><body><table><tr><td>软件名称</td><td>价格（万元）并行规模</td><td></td><td>许可证年费或 年技术支持费</td></tr><tr><td>ANSYSCFX&FLUENT</td><td>1000</td><td>128</td><td></td></tr><tr><td>ABAQUS</td><td>360</td><td>64</td><td>18%</td></tr><tr><td>MSC Nastran</td><td>520</td><td>32</td><td></td></tr><tr><td>MSC Dytran</td><td>100</td><td>-</td><td></td></tr><tr><td>MSC Patran</td><td>160</td><td>-</td><td></td></tr><tr><td>Gaussian</td><td>50</td><td></td><td>10</td></tr><tr><td>LS-DYNA</td><td>127</td><td>128</td><td>38</td></tr><tr><td>Matlab开发工具</td><td>67</td><td>128</td><td></td></tr></table></body></html>

效的高性能计算能力。学习这些商业模式，中国的高性能计算软件可以迅速地走向世界。

# 12结论

中国的高性能计算发展了半个多世纪，国家在芯片、硬件、系统设计和基础设施如网络和计算中心等方面有了大量的投入。最近，地方政府也积极参与进来，提供了强有力的支持。正是这样全方位的协同努力，我国研制出了目前世界上最快和若干名列前茅的超级计算机，其中至少有一台将采用国产高性能处理器。这样的发展还是得益于政府研究项目资助，用户（尤其是商业界的）在其中发挥的作用还非常少。

国产高性能计算机企业在中国市场正形成了对国际大公司如IBM、惠普的挑战。如果只考虑硬件因素，我国与美国、日本的差距约1-2年。我国在体系结构和软件方面的创新急需加强，这也是国内高性能计算的短板，涉及到应用和系统软件各个层次。目前，我国几乎没有具有国际竞争力的国产软件，但国外软件高昂的授权费用和出口限制迫使我们必须开发自己的软件产品。最近出现的新的研究，包括基于GPU的软件开发和面向大规模数据处理应用，很值得重视。目前，在大量商用组件、产品和服务的流行的形势下，日益重要的大规模数据处理问题将为我国确立真正有竞争力的品牌提供重要的机遇。

# 致谢

国家智能计算机研究开发中心的许多研究生和员工参与了调查和本文的写作与校对工作，在此一并致谢！

# 参考文献：

[1]Sun,NH(Sun, Ninghui); Kahaner,D(Kahaner,David); Chen,DB(Chen,Debbie),High-performanceComputing in China: Research and Applications International Journal of High Performance ComputingApplications,24(4):363-409 NOV 2010  
[2]http://www.ciw.com.cn/News/hotnews/2006-10-26/9905.shtml  
[3]杨学军，高性能计算机进展与挑战，中国计算机科学技术发展报告，2006  
[4]李国杰，从103机到曙光机—中国高性能通用计算机研制历程回顾  
[5]孙凝晖，计算所高性能计算机的研究  
[6]高庆狮，中国第一台大型向量计算机—757机  
[7]油田开发大型数据处理系统，光明日报，2000年02月09日  
[8] 银河计算机(维基百科)，http://zh. wikipedia.0rg/wiki/%E9%93%B6%E6%B2%B3%E8%AE%A1%E7%AE%97%E6%9C%BA  
[9]htp://www.ncic.ac.cn/product/product.htm  
[10] 曙光天潮系列应用成果一优秀论文汇编(第三集)，2001年11月  
[11] 胡胜友，淮河大堤上的"神算子”记新当选的中国科学院院士陈国良教授，中国科大报，Vol.489，2003.12  
[12]中国科学院物理研究所，http://hre.iphy.ac.cn/ability/rjcwds.asp  
[13]中国科学院生物物理研究所，htp://www.ibp.ac.cn/c/04/archive/05/c/runshengchen.html.  
[14]CNGrid， htp://i.cs.hku.hk/\~clwang/grid/CNGrid.html  
[15]高性能计算机标准工作委员会，http://www.hpcsc.org/  
[16]上海市超级计算中心 2008 年度报告  
[17]上海超级计算中心《高性能计算发展与应用》，2005年第四期，总第十三期  
[18]“高性能计算要普及应用才是关键”，http://server.doit.com.cn/article/2009/0107/2399593.shtml  
[19] 国家高技术研究发展计划（863计划）信息技术领域"高效能计算机及网格服务环境"重大项目，2009 年度课题申请指南  
[20] 胡伟武，张福新，李祖松，龙芯2号处理器设计和性能分析，计算机研究与发展，2006.43(6):959-966.  
[21] 张俊霞，张焕杰，李会民，基于龙芯2F的国产万亿次高性能计算机KD-50-I的研制，中国科学技术大学学报，2008.38（1)：105-108  
[22] 张云泉，孙家昶，袁国兴，张林波，2007年高性能计算机排行榜对比分析，中国计算机学会通讯，2008.4（4):80-85  
[23] Chengyong Wu,Ruiqi Lian, Junchao Zhang,et al. An overview of the open research compiler.Languages and Compilers for High Performance Computing. 2O05:17-31.  
[24] 钱德沛，“高效能计算机及网格服务环境"重大项目进展，中国计算机学会通讯，2009.5(4)：74-78  
[25] 孙凝晖，李凯，陈明宇，HPP：一种支持高性能和效用计算的体系结构，计算机学报，2008.31(9):1503-1508  
[26] 孙凝晖，安明远，包云岗等，综述：可扩展应用与可扩展系统，2009.7（1)：1-34  
[27] Zhang Zhi-Hong. Meng Dan. Zhan Jian-Feng,et al. Easy and reliable cluster management: theself-management experience of Fire Phoenix. IPDPS 2006.  
[28]樊莉亚，张法，王功明等，单颗粒重构软件EMAN 的并行设计与实现，2008 年全国高性能计算学术年会，无锡  
[29] David A. Bader, Petascale Computing Algorithms and Applications.  
[30] http://www.biotech.org.cn/fruit/news_show.php?id=33969  
[31] 生物信息学的发展历史，http://www.xinkexue.com/wiki-doc-docid-530.html  
[32] http://www.it.com.cn/f/market/0810/20/675599.htm  
[33]孙凝晖，HPC China 2007 Report，2007  
[34]孙凝晖，HPC China 2008 Report，2008.  
[35]孙凝晖，面向蛋白质科学的高性能计算研究，2007  
[36] Krste Asanovic,Ras Bodik, Bryan Christopher and et al,“The Landscape of Paralel ComputingResearch: A View from Berkeley",Technical Report No. UCB/EECS-2006-183,December 18, 2006.  
[37] P. M. Kogge and et al, "ExaScale Computing Study: Technology Challenges in AchievingExascaleSystems," DARPA Information Processing Techniques Ofice, Washington, DC, pp.278, September 28,2008.  
[38]詹剑锋等，High Throughput Computing: ICTView，2009  
[39] Semiconductor Industries Association.“International Technology Roadmap for Semiconduc-tors".2006.  
[40] Jeffrey Dean and Sanjay Ghemawat.MapReduce: Simplified Data Processing on Large Clusters,OSDr04: Sixth Symposium on Operating System Design and Implementation, San Francisco, CA,December,2004.  
[41] http://esearch.microsoft.com/en-us/projects/Dryad/  
[42] http://googleresearch.blogspot.com/2009/06/large-scale-graph-computing-at-google.html  
[43] http://www.top500.org/， Jun 2009  
[44] http://www.samss.org.cn，中国 TOP100 排行榜  
[45]桂文庄，“应当重视高性能计算的应用和基础性研究”，e-Science，2008，第三期  
[46] 中国科学院数理学部"高性能计算战略研究"咨询组，“加速发展中国高性能计算的若干建议”，e-Science，2008，第三期  
[47]王宏琳，地球物理计算机的变革，中国石油集团东方地球物理公司  
[48]王宏琳，地球物理勘探中的高性能计算，中国石油集团东方地球物理公司

附表A：神威新世纪-64P（已停用）系统配置  

<html><body><table><tr><td>系统峰值 302.4Gflops</td><td></td></tr><tr><td>计算结点</td><td>32个双P4Xeon 处理器</td></tr><tr><td>服务结点</td><td>1个双P4Xeon 处理器</td></tr><tr><td>CPU</td><td>IntelP4 Xeon,2.4 GHz，总共 66CPU</td></tr><tr><td>系统内存总容量</td><td>66GB</td></tr><tr><td>磁盘总容量</td><td>2.7TB</td></tr><tr><td>体系架构</td><td>Cluster、Gig-Ethernet</td></tr><tr><td>操作系统</td><td>RedhatLinux 7.3</td></tr></table></body></html>

附表B：上海超算中心使用的商业软件列表  

<html><body><table><tr><td>序号</td><td>软件名称</td><td>描述</td><td>相关网站</td></tr><tr><td>1</td><td>FLUENT</td><td>通用计算流体力学分析软件</td><td>www.fluent.com</td></tr><tr><td>2</td><td>LS-DYNA</td><td>瞬态响应动力学分析软件</td><td>www.ansys.com</td></tr><tr><td>3</td><td>PAM-CRASH</td><td>瞬态响应动力学分析软件</td><td>www.esi-group.com</td></tr><tr><td>4</td><td>ANSYS.Multiphysics</td><td>多物理场耦合分析软件</td><td>www.ansys.com</td></tr><tr><td>5</td><td>NASTRAN</td><td>通用有限元分析软件</td><td>www.mscsoftware.com</td></tr><tr><td>6</td><td>MARC</td><td>非线性有限元分析软件</td><td>www.mscsoftware.com</td></tr><tr><td>7</td><td>FEKO</td><td>高频电磁场分析软件</td><td>www.feko.info</td></tr><tr><td>8</td><td>STAR-CD</td><td>通用计算流体力学分析软件</td><td>www.cd-adapco.com</td></tr><tr><td>9</td><td>CFX</td><td>通用计算流体力学分析软件</td><td>www.ansys.com</td></tr><tr><td>10</td><td>OPTIMUS</td><td>过程集成与优化设计软件</td><td>www.optimus.pl</td></tr><tr><td>11</td><td>HyperWorks</td><td>有限元前后处理和优化设计软件</td><td>www.altair.com</td></tr><tr><td>12</td><td>ABAQUS</td><td>非线性有限元分析软件</td><td>www.abaqus.com.cn</td></tr><tr><td>13</td><td>IDEAS</td><td>有限元分析及前后处理软件</td><td>www.ugs.com.cn</td></tr><tr><td>14</td><td>ICEM-CFD</td><td>CFD前后处理软件</td><td>www.ansys.com</td></tr><tr><td>15</td><td>AI*ENVIRONMENT</td><td>CAE前后处理软件</td><td>www.ansys.com</td></tr><tr><td>16</td><td>GAMBIT</td><td>FLUENT配套前处理软件</td><td>www.fluent.com</td></tr><tr><td>17</td><td>MasterFEM</td><td>有限元前后处理软件</td><td>www.ugs.com.cn</td></tr><tr><td>18</td><td>Medina</td><td>有限元前后处理软件</td><td>www.oracle.com</td></tr><tr><td>19</td><td>Oracle</td><td>数据库软件</td><td></td></tr><tr><td>20</td><td>GEFEP-P</td><td>汽车碰撞并行仿真软件</td><td></td></tr><tr><td>21</td><td>CADEM-P</td><td>汽车覆盖件冲压并行仿真软件</td><td></td></tr></table></body></html>

附表C：上海超算中心使用的科学计算应用软件列表  

<html><body><table><tr><td>序号</td><td>软件名称</td><td>描述 说明</td></tr><tr><td>1</td><td>QCHEM</td><td>量子化学计算软件</td></tr><tr><td>2</td><td>NWCHEM</td><td>量子化学计算软件 开源代码</td></tr><tr><td>3</td><td>GAUSSIAN 量子化学计算软件</td><td></td></tr><tr><td>4</td><td>MOLPRO</td><td>量子化学计算软件</td></tr><tr><td>5</td><td>GAMESS</td><td>量子化学计算软件 开源代码</td></tr><tr><td>6</td><td>CPMD</td><td>分子动力学从头计算程序 开源代码</td></tr><tr><td>7</td><td>ABINIT 电子结构计算程序</td><td>开源代码</td></tr><tr><td>8</td><td>VASP 分子动力学模拟</td><td></td></tr><tr><td>9</td><td>SIESTA</td><td>分子和固体的电子结构计算和分子动力学模拟 开源代码</td></tr><tr><td>10</td><td>WIEN2K 固体电子结构计算</td><td></td></tr><tr><td>11</td><td>ESPRESSO</td><td>分子动力学模拟</td></tr><tr><td>12</td><td>SMEAGOL分子动力学模拟</td><td></td></tr><tr><td>13</td><td>NAMD</td><td>分子动力学计算程序</td></tr><tr><td>14</td><td>EGO 分子动力学程序</td><td></td></tr><tr><td>15</td><td>GROMACS分子动力学通用软件包</td><td>开源代码</td></tr><tr><td>16</td><td>DOCK 分子对接程序</td><td></td></tr><tr><td>17</td><td>BLAST 序列相似性检索程序</td><td></td></tr><tr><td>18</td><td>MM5</td><td>有限区域的非静力平衡的中尺度数值模式的计算程序</td></tr><tr><td>19</td><td>GRAPES</td><td>全球和区域气象分析预报系统 中国气象局研发</td></tr><tr><td>20</td><td>FDS 火灾模拟程序</td><td>开源代码</td></tr><tr><td>21</td><td>COSMOMC 天体物理计算</td><td></td></tr></table></body></html>

附表D：CGGVeritas和BGP的计算机对比  

<html><body><table><tr><td>年代 (计算机 变革)</td><td>CGGVeritas</td><td>BGP</td></tr><tr><td>1970 (主机+数 组处理机)</td><td>1966年：CGG 建立第一个处理中心， 使用 SDS9300 计算机</td><td>1973年：建立第一个处理中心，使用 150 计算机 1977 年：引进Cyber172-4 机+MAPII</td></tr><tr><td>1980 (向量计算</td><td>1972年：开发GeoMaster 软件 1980年：DIGICON开发DISCO处理</td><td>1983年：引进IBM3033+3838 1986年：建立YH-1巨型机地震数据</td></tr><tr><td>机)</td><td>系统运行在DECVAX11/780上 1984年：CGG GeoVector（CRAY批</td><td>处理系统</td></tr><tr><td></td><td>量处理版本）软件安装在当时最大计 算机Cray1S上</td><td>1987年：开发PE3284+AP2704多数 组处理机多辅处理机地震数据处理系</td></tr></table></body></html>

附表D（续）  

<html><body><table><tr><td rowspan="3">1990 (工作站和 并行计算 机)</td><td>1991 年：CGG GeoVectorPlus 将批量 和交互处理集成在UNIX平台上运行</td><td>1991年：引进IBM3084+3838</td></tr><tr><td>1994年：CGG 并行GeoVector 运行在 Convex SPP1000 上 1996年：CGG GeoVector 支持 IBM,</td><td>1992 年：GRISYS 在UNIX 平台上运 行 1995年：引进IBMSP2并行机</td></tr><tr><td>2001年：GeoCluster在PC机群上运行 (机群计算 2002年：GeoClusterl.1全面取代了 GeoVectorPlus 2003年：GeoCluster2.1全部软件可以</td><td>2001年：安装曙光3000（EP460）地 震数据处理系统 2003年：安装曙光4000L机群地震数 据处理系统</td></tr></table></body></html>

作者简介：

![](images/f5ce1ca5a9f19cb240ec267396078785cda0901642486ebf0e7515425bc11244.jpg)

孙凝晖研究员，中国科学院计算技术研究所常务副所长。主要从事计算机体系结构方面研究，发表论文100多篇，现担任中国计算机学会高性能计算机专委会副主任、中国科学技术大学客座教授，《计算机学报》主编，《Journal of Computer Science and Technology》领域编委。作为项目负责人，于1999年获中国科学院青年科学家奖一等奖，2001、2003、2006年三次荣获国家科技进步二等奖，2005年获中国科学院杰出科技成就奖，并于2006年获得“中国青年科技奖”和“中国十大杰出青年”荣誉称号。

# 计算机体系结构国家重点实验室颁牌

科技部、教育部、中科院、中国工程院、国家自然基金委员会于2011年5月 23日共同在北京召开全国基础研究工作会议，会上，为24个新建国家重点实验室举行了颁牌仪式。计算所常务副所长孙凝晖从科技部副部长王志刚手中接过“计算机体系结构国家重点实验室”的牌匾。

目前，我国有国家重点实验室382个，包括在高校和研究院所建设的国家重点实验室261个，试点国家试验室6个。据悉，今年召开的全国基础研究工作会议是继1989年、2000 年以来召开的第三次全国性基础研究工作大会。刘延东、万钢等出席会议并做重要讲话。

基础研究的战略意义和重要作用得到了前所未有的广泛共识和高度重视，中央财政对基础研究的投入大幅增长，为基础研究发展提供了重要保障；学科研究体系更加完备；创新基地建设成效显著；人才队伍不断壮大；基础研究整体水平大幅提升；基础研究引领经济社会发展的能力不断增强。

（选自http://www.ict.cas.cn《计算所新闻》）