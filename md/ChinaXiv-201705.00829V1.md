# Entropy-partition of Complex Systems and Emergence of Human Brain's Consciousness

Xi Guangcheng

（Institute of Automation, Chinese Academy of Science,Beijing,1Ooo8o,P.R. China） Guangcheng.xi $@$ ia.ac.cn

Abstract. Consciousness is discussed from viewpoint of theory of Entropypartition of complex system.Human brain's system self-organizably and adaptivelyimplementspartition 、aggregationandintegration， and consciousness emerges.

Keywords:Humanbrain'ssystem;Entropy—partition(aggregation); Integration; consciousness

# 1. Introduction

In article of Giulio Tononi and G.M.Edelman[1],studying of consciousness is concentrated on description of kinds of neural processes which can account for key properties of conscious experience. They have emphasized two key properties of consciousness: conscious experience is integrated (each conscious scene is unified) and at the same time it is highly differentiated (within a short time,one can experience any of a huge number of different conscious states),and have proposed notions so-called functional clustering，neural complexity and dynamical core hypothesis. Those researching methods and train of thought to problem of consciousness agree to a certain extent, with theory and method of Entropy-partition of Complex System I proposed 23 years ago [2]. At that time related specialists (appraising group of achievements in scientific research） ever have pointed out: theory of Entropy-partition (Aggregation） of complex system not only has been applied to Ecologico-Economical regionalization [3] of Ecologico-Economico-social complex system but also will certainly be applied to researching of human Brain's neural system.Therefore having read the article of G.Tononi and G.M.Edelman, I have much sensation seem to have met before，and excited feeling arises spontaneously. Though such，we strongly have felt it still necessary to discuss problem of consciousness from viewpoint of theory of entropy-partition (aggregation) of complex system.

# 2. Entropy-partition (aggregation) of neural system $X$

Suppose neural system $\boldsymbol { X } = ( X _ { 1 } , X _ { 2 } , \cdots , X _ { a } , \cdots , X _ { p } ) ^ { T }$ is consisted of $p$ neuron, $p \in N \mathrm { ~ ( ~ } N$ set of natural number)，where $X _ { _ a } = ( X _ { _ { a _ { i } } } ) \ , a = 1 , 2 , \cdots , p \ ;$ （204号 $i = 1 , 2 , \cdots , q$ . Let $C _ { _ a } ( a = 1 , \cdots , p )$ be set of classification of $X _ { a } , C _ { a _ { i } } = i$ be $i$ 1 th element of $C _ { a }$ ，then we have $C _ { a } = \{ 1 , 2 , \cdots , i \cdots , k \} , k \leq q$ ，and let $n _ { i }$ be quantity for $X _ { a }$ belong to $i$ -th class,then entropy of $X _ { a }$ is defined as

$$
H ( X _ { _ a } ) = - \sum _ { i = 1 } ^ { k } { n _ { i } } / q \log { n _ { i } } / q
$$

joint entropy of $X _ { a } , X _ { b }$ is similarly defined as

$$
H ( X _ { a } \bigcup X _ { b } ) = - { \sum _ { i } \sum _ { j } { n _ { i j } } } / { q \log n _ { i j } } / { q }
$$

where $n _ { i j }$ is quantity for $X _ { a }$ belong to $i$ -th class of $C _ { a }$ simultaneously $X _ { b }$ belong to $j$ -th class of $C _ { b }$ .For the convenience of application, expressions(1) and (2) can respectively be represented as

$$
H ( X _ { \mathfrak { a } } ) = \log q - { \frac { 1 } { q } } { \sum _ { i = 1 } ^ { k } n _ { i } \log n _ { i } }
$$

$$
H ( X _ { a } \bigcup X _ { b } ) = \log q - { \frac { 1 } { q } } { \sum _ { i } \sum _ { j } n _ { i j } \log n _ { i j } }
$$

Having had above-mentioned definition of entropy,in what follows,correlative measure by which statistical dependence between the $X _ { a }$ and the $X _ { b }$ is denoted is defined by their mutual information.

Definition 1. Suppose $X _ { a } \cap X _ { b } = \phi$ , then entropy

$$
H ( X _ { a } , X _ { b } ) = H ( X _ { a } ) + H ( X _ { b } ) - H ( X _ { a } \bigcup X _ { b } )
$$

is called correlative measure $\mu ( X _ { a } , X _ { b } )$ between the $X _ { a }$ and the $X _ { b }$ （24号

Definition 2. Suppose $X _ { a } \cap X _ { b } = \phi$ for arbitrary $a , b ( a \neq b )$ ,then

$$
\mu ( X _ { 1 } , X _ { 2 } , \cdots , X _ { p } ) { \overset { \Delta } { = } } \sum _ { a = 1 } ^ { p } H ( X _ { a } ) - H \left( \sum _ { a = 1 } ^ { p } X _ { a } \right)
$$

is called correlative measure among $X _ { 1 }$ $\mathbf { \psi } _ { X _ { 2 } } , \ldots$ ，and $X _ { p }$

Definition 2:Suppose system $X$ bepartitioned into $m$ subsystems （20 $s _ { 1 } , s _ { 2 } , \cdots , s _ { m }$ , for arbitrary $i , j \ ( i \neq j ) , s _ { i } \cap s _ { j } = \phi , X = \sum _ { i = 1 } ^ { m } s _ { i }$ ，then

$$
\mu ( s _ { 1 } , s _ { 2 } , \cdots , s _ { m } ) { \stackrel { \Delta } { = } } \sum _ { i = 1 } ^ { m } H ( s _ { i } ) - H \left( \sum _ { i = 1 } ^ { m } s _ { i } \right)
$$

is called correlative measure among S1,S2,·,Sm ·

Let us consider nonempty finite set $X$ and set-family $E ( X )$ consisted of its subset, $P _ { e }$ a set-function defined on $E ( X )$ with properties

(i) $P _ { e } ( A ) \geq 0$ ， $\forall A \in E \left( X \right)$ ，(ii) $P _ { e } \left( \phi \right) = 0$ （204号

Definition 3. If for arbitrary nonempty finite set $S _ { i } \in E ( X )$ ， $\boldsymbol { S } _ { j } \in E ( \boldsymbol { X } )$ $i \neq j , S _ { i } \bigcap S _ { j } = \phi$ ，have

$$
P _ { e } ( S _ { i } \bigcup S _ { j } ) \geq P _ { e } ( S _ { i } ) + P _ { e } ( S _ { j } )
$$

then,set-function $P _ { e }$ satisfied conditions (i),(ii) is called superadditive.

Although finite superadditivities of mutual information or the so-called measure of cohesion of the components of the set of entities is well known result, it is not still unnecessary to write down our proof about finite superadditivity of the correlative measure among some subsystems of the system $X$ which is given.

Theorem[2]. Correlative measure $\mu ( s _ { 1 } , s _ { 2 } , \cdots , s _ { m } )$ isfinitely superadditive, countably superadditive and unique.

Proof.Finite superadditivity

Suppose system $X$ is partitioned into m subsystems $\mathbf { s } _ { 1 }$ ，S2，， $\mathbf { s } _ { \mathrm { m } }$ ，and that $s _ { i } \in R , s _ { i } \neq \emptyset ; s _ { j } \in R , s _ { j } \neq \emptyset ; s _ { i } \bigcap s _ { j } = \emptyset , X = \sum _ { j = 1 } ^ { m } s _ { j } \in R \ ,$ ，for any $i , j ( i \neq j )$ ，where $\mathtt { R }$ is a algebra on set $X$ which is given.By definition of the correlative measure,we have

$$
\mu ( S ) = \mu ( \sum _ { j = 1 } ^ { m } s _ { j } ) = \mu ( s _ { 1 } , s _ { 2 } , . . . . . . , s _ { \mathrm { m } } ) = \mu ( X _ { 1 } , X _ { 2 } , . . . . . . X _ { p } ) = \sum _ { i = 1 } ^ { p } H ( X _ { i } ) - H ( \sum _ { i = 1 } ^ { p } X _ { i } )
$$

$$
\begin{array} { r l } { \displaystyle \sum _ { s _ { j } \in S } \mu ( s _ { j } ) = \sum _ { s _ { j } \in S } ( \displaystyle \sum _ { X _ { j } \in s _ { j } } H ( X _ { j } ) - H ( \displaystyle \sum _ { X _ { j } \in s _ { j } } X _ { j } ) ) } & { } \\ { = \displaystyle \sum _ { i = 1 } ^ { p } H ( X _ { i } ) - \sum _ { s _ { j } \in S } H ( s _ { j } ) } & { } \\ { = \displaystyle \sum _ { i = 1 } ^ { p } H ( X _ { i } ) - \sum _ { j = 1 } ^ { m } H ( s _ { j } ) } & { } \end{array}
$$

Subtracting(8) from(7) leads to

$$
\begin{array} { c } { { \displaystyle \mu ( \sum _ { j = 1 } ^ { m } s _ { j } ) - \sum _ { j = 1 } ^ { m } \mu ( s _ { j } ) = \sum _ { j = 1 } ^ { m } H ( s _ { j } ) - H ( \sum _ { i = 1 } ^ { p } X _ { i } ) } } \\ { { = \displaystyle \sum _ { j = 1 } ^ { m } H ( s _ { j } ) - H ( \sum _ { j = 1 } ^ { m } s _ { j } ) \geq 0 } } \end{array}
$$

# Countable superadditivity and uniqueness:

Suppose system $X$ is partitioned into a sequence of subsystems $s _ { 1 } , s _ { 2 } , . . . , s _ { n } . . .$ $\begin{array}{c} \begin{array} { l } { { \displaystyle s _ { n } \in R \mathrm {  ~ \left. ~ \right.} - \mathrm {  ~ \ a l g e b r a ~ } \mathrm {  ~ \ o n ~ } X \mathrm {  ~ } , s _ { n } \in R , \mathrm {  ~ \Sigma ~ } _ { n } \neq \emptyset ; s _ { m } \in R , s _ { m } \neq \emptyset ; s _ { n } \bigcap s _ { m } = \emptyset } \\ { { \displaystyle X = \sum _ { n = 1 } ^ { \infty } s _ { n } \in R , \mathrm { f o r ~ a n y ~ } n , m ( n \neq m ) } . } \\ { { \mathrm {  ~ \ L e t ~ } E _ { k } = \sum _ { n = 1 } ^ { k } s _ { n } \mathrm {  ~ , ~ o b v i o u s l y , w e ~ h a v e ~ } } } \end{array}  , }  \end{array}$

$$
E _ { 1 } \subset E _ { 2 } \subset \cdots \subset E _ { k } \cdots , E _ { k } \in R , \operatorname* { l i m } _ { k  \infty } E _ { k } = \sum _ { n = 1 } ^ { \infty } s _ { n } \in R
$$

From nonnegativity and monotonicity of $\mu$ ,we have

$$
\mu ( E _ { \mathrm { 1 } } ) \leq \mu ( E _ { \mathrm { 2 } } ) \leq \cdots \leq \mu ( E _ { k } ) \leq \cdots \leq \mu ( \sum _ { n = 1 } ^ { \infty } s _ { n } )
$$

If $\operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) \to \infty$ , from monotonicity of $\mu$ , then inevitably we have $\mu ( \sum _ { n = 1 } ^ { \infty } s _ { n } ) = \infty$ . Hence by using finite superadditivity,we have

$$
\mu ( \sum _ { n = 1 } ^ { \infty } s _ { n } ) = \infty = \operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) = \operatorname* { l i m } _ { k \to \infty } \mu ( \sum _ { n = 1 } ^ { k } s _ { n } ) \geq \operatorname* { l i m } _ { k \to \infty } \sum _ { n = 1 } ^ { k } \mu ( s _ { n } ) = \sum _ { n = 1 } ^ { \infty } \mu ( s _ { n } )
$$

If $\operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) = c < \infty$ ,above-mentioned process of proof is understood easily.And either of the

$\operatorname* { l i m } _ { k  \infty } \mu ( E _ { k } ) = c < \infty$ and $\operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) = \infty$ makes uniqueness of the limit to hold still, i.e.

$\operatorname* { l i m } _ { k \to \infty } \mu ( E _ { k } ) = \operatorname* { l i m } _ { k \to \infty } \mu ( \sum _ { n = 1 } ^ { k } s _ { n } )$ is unique.Up to this point,proof of the theorem is ompleted.

From above-mentioned property of correlative measure,criterion by which lowest requirement of ideal partition is presented should be

i $) \mu ( s _ { i } ) > \mu ( s _ { i } , s _ { j } ) , \mu ( s _ { j } ) > \mu ( s _ { i } , s _ { j } )$ (ii) $\mu ( s _ { i } ) > \mu ( s _ { i _ { 1 } } ) + \mu ( s _ { i _ { 2 } } ) , \mu ( s _ { j } ) > \mu ( s _ { j _ { 1 } } ) + \mu ( s _ { j _ { 2 } } ) .$

for any $i , j ( i \neq j ) , s _ { i } \bigcap s _ { j } = \phi$ . Here $s _ { i _ { l } } , s _ { j _ { l } } \ ( l = 1 , 2 )$ denotes partition of $s _ { i }$ and $s _ { j }$ ,respectively. (i) denotes that correlative measure of any subsystem itself which is obtained by partition is larger than correlative measure between it and any subsystem. (ii) denotes that correlative measure of any subsystem which is obtained by partition possesses strictly superadditivity.

When number of characteristic variable of system $X$ is very large,comprehensive observation for data is impossible.Even though the observation is possible,as obtaining complete data spends very long time and for other reason such that obtained data loses its available value.At this time,statistical method and theory can be applied to obtaining data and to analyzing problem.

Suppose $\pmb { x } _ { a } = ( x _ { a _ { 1 } } , x _ { a _ { 2 } } , \cdots , x _ { a _ { N } } )$ be index of quantity of characteristic variable $X _ { a }$ of the complex system $X$ ，data obtained by mean of random sampling is $\overline { { \pmb { x } } } _ { a } = ( x _ { a _ { 1 } } , x _ { a _ { 2 } } , \cdots , x _ { a _ { q } } ) , \overline { { \pmb { x } } } _ { a }$ be a random sample from population $X _ { a }$ . Obviously any $x _ { a _ { i } }$ necessarily is equal to some $x _ { a _ { \theta _ { i } } }$ ，here $\theta _ { i }$ $( i = 1 , 2 , \cdots , q )$ is number of $i$ _ th individual of $\scriptstyle { \pmb { x } } _ { a }$ . For any $\theta _ { i }$ ,we have probability

$$
p \big \{ x _ { { a } _ { 1 } } = x _ { { a } _ { { a } _ { 1 } } } , x _ { { a } _ { 2 } } = x _ { { a } _ { { a } _ { 2 } } } , { \therefore } , x _ { { a } _ { q } } = x _ { { a } _ { { \theta } _ { q } } } \big \} = ( N - q ) ! / N !
$$

Corresponding to expressions $( 1 ^ { \prime } )$ and $( 2 ^ { \prime } )$ , we have

$$
H ( \overline { { \pmb { x } } } _ { a } ) = \frac { ( n - q ) ! } { n ! } \Bigg [ \log ( n ! q ) - \log ( n - q ) ! - \frac { 1 } { q } \sum _ { i } n _ { i } \log n _ { i } \Bigg ]
$$

$$
H ( \overline { { \pmb { x } } } _ { a } \cup \overline { { \pmb { x } } } _ { b } ) = \frac { ( N - q ) ! } { N ! } \bigg [ \log ( N ! q ) - \log ( N - q ) ! - \frac { 1 } { q } \sum _ { i } \sum _ { j } n _ { i j } \log n _ { i j } \bigg ]
$$

At this time,correlative measure of the system $\mathrm { \Delta } \mathrm { X }$ is defined by way similar to the above-mentioned method.Actually,in order to obtain the partition of the complex system $X$ we often introducecoefficient ofcorrelative measure $\mu _ { a b } = \mu ( X _ { a } , X _ { b } ) / H ( X _ { b } ) ( \mu _ { i j } = \mu ( s _ { i } , s _ { j } ) / H ( s _ { j } )$ ，obviously, $\mu ( a , b ) ( \mu _ { i j } )$ is between O and 1. Having computed $\mu _ { a b }$ for all $^ { a , b }$ ，those $X _ { a }$ ，

$X _ { b }$ whose correlative measure is larger than other $\mu ( a , l ) , l \neq b$ or $\mu ( \boldsymbol { r } , \boldsymbol { b } )$ K $r \neq a$ ，are combined so as to accord with the preceding criterion（1），（2）. Hence some corresponding subsystems $s _ { i }$ ， $i = 1 , \cdots , m$ ,i.e.some neural functional units are obtained.

From neurophysiological studies,elementary unit representing information in human brain is mutually coordinated clique of neuron; by combining mutual function, neurons with different properties carry out correlative activity,form various dynamic neural networks which correspond to various function of information.

Above-mentioned partition or aggregation of neurons is carried out in selforganizable and adaptive form; the partition or aggregation of neurons makes human brain emerge intelligence.

# 3.Emergence of consciousness

Suppose system $X$ self-organizably and adaptively is partitioned into $m$ subsystem. Correspondingly, there are $m$ subspace $\left( \varOmega _ { i } , \Im _ { i } , P _ { i } \right)$ ， $i = 1 , \cdots , m$ , of probabilityspace $( { \mathcal { Q } } , { \mathfrak { I } } , P )$ , where $\varOmega$ is space of configuration of system $X$ . Let $( { \mathcal { Q } } , { \mathfrak { I } } , P )$ be Radon measurable space,then for any $\Im _ { i }$ there exists regular conditional probability $P ( \cdot | \Im _ { i } )$ ，obviously, $P ( \cdot \mid \mathfrak { I } _ { i } ) = P ( \cdot \mid \Im ( s _ { i } ) ) = P ( \cdot \mid s _ { i } ) { = } \mu ( \cdot \mid s _ { i } ) .$ （20 Hence, for any $A \in { \mathfrak { I } }$ （204

$$
\mu ( A ) = \int _ { X \mid s } \ \mu ( \cdot \mid s _ { i } ) d \mu ( s _ { i } )
$$

and for any random variable $\eta$

$$
E \eta = \int _ { X \mid s } ~ E ( \eta \mid s _ { i } ) d \mu ( s _ { i } )
$$

where $X \mid s$ is quotient space.

Let us define following two functions,

$$
\begin{array} { c } { { f _ { \varepsilon } ^ { ( 1 ) } ( t ) = \left\{ \begin{array} { c c } { { \displaystyle \underline { { { \mu } } } ( A ) } } & { { , } } \\ { { \varepsilon } } & { { 0 \leq t \leq \varepsilon } } \\ { { 0 , } } & { { \varepsilon < t } } \end{array} \right. } } \\ { { f _ { \varepsilon } ^ { ( 2 ) } ( t ) = \left\{ \begin{array} { c c } { { \displaystyle { \underline { { { E } } } \eta } } } & { { , } } \\ { { \varepsilon } } & { { 0 \leq t \leq \varepsilon } } \\ { { 0 , } } & { { \varepsilon < t } } \end{array} \right. } } \end{array}
$$

where $\varepsilon$ is arbitrary positive number, $t$ is time.

To be this,we eventually have conscious experience,that is $\mu ( A )$ or $E ( \eta )$ under some situations or $\operatorname* { l i m } _ { \varepsilon \downarrow 0 } f _ { \varepsilon } ^ { ( i ) } ( t ) , i = 1 , 2$ , under other situations.

When $A$ or $\eta$ comes from outside of $X$ ，or when $A$ or $\eta$ is caused by“gene” or“experience”which is in inside of $X$ ，all will emerge consciousness，and is emerged by system $X$ self-organizably and adaptively to be partitioned (aggregated), then to be integrated in hundreds of milliseconds. $\mu ( A )$ or ${ E \eta }$ is the integrated process.

If there exists sequences of mutual information between any appointed two neurons and between any appointed two subsystems,and the sequences of the mutual information have nice asymptotical properties, that is,if there exists ergodic superadditive process for the sequence of mutual information in neural system $\mathrm { \Delta X }$ ， then the system can implement self-organization and self-adaptation.

# Remarks

(1） That the neural system $X$ is self-organizably and adaptively partitioned into some subsystems is implemented by means of the correlative measure,i.e.mutual information,which does not touch upon change of location of space of neurons or subsystems.This is a“Internet” in the neural System $X$ .The some subsystems form large-scale hierarchical intelligent system. On the hierarchical intelligent system, basic principle of hierarchical intelligent system IPDI (Increasing precision with decreasing intelligence) holds [4,5]. The layer with high intelligence is dynamic core of the neural system $X$

(2) Consciousness is some memory. (3)Unconsciousness is also consciousness. (4) Consciousness is conscious of consciousness. (5)Nearly 3OoO years ago, Chinese ancient learned men said consciousness is that Saint hears at silence, looks at immateriality,and firmly believed memory all is in brain. (6) Oneself-consciousness can only be experienced; it forever cannot be captured by oneself.

Acknowledgments.This work was partly supported by the National 973 Project No.2003CB517106, China and the NSFC Projects under Grant No. 60621001, China.

# References

1. Giulio Tonoi and Gerald M. Edelman,Consciousness and Complexity， Science 828,1846-1851 (1998) 2.Xiguangcheng， Entropy-method of Partition of Complex System， ACTA AUTOMATICA SINICA (in Chinese) 13(3),216-220,(1987). 3.Xi Guangcheng，The Entropy-method of Eologico-Economical Regionalization, ACTA AUTOMATICA SINICA (in Chinese) 16(2),170-173,(1990)

4.Xiguangcheng， Intelligent Control with Relative Entropy Minimizing，Control Theory and Applications (in Chinese) 16(1),27-31,(1999) 5. Saridis G N,Analytic formulation of the principle of increasing precision with decreasing intelligence for intelligent machine,Automatica 25(3), 461-467 (1989).