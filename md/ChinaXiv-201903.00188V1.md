# 融合语言特征的抽象式中文摘要模型

胡德敏，王荣荣

(上海理工大学 光电信息与计算机工程学院，上海 200093)

摘要：为了解决传统抽象式摘要模型生成的中文摘要难以保存原文本语义信息的问题，提出了一种融合语言特征的抽象式中文摘要模型。模型中添加了拼接层，将词性、命名实体、词汇位置、TF-IDF等特征拼接到词向量上，使输入模型的词向量包含更多的维度的语义信息来确定关键实体。结合指针机制有选择地复制原文中的关键词到摘要中，从而提高生成的摘要的语义相关性。使用LCSTS 新闻数据集进行实验，取得了高于基线模型的ROUGE 得分。分析表明本模型能够生成语义相关度较高的中文摘要。

关键词：抽象式摘要模型；语言特征；关键实体；词向量 中图分类号：TP391.1 doi:10.19734/j.issn.1001-3695.2018.07.0531

# Abstractive Chinese summarization model with linguistic features

Hu Demin, Wang Rongrong (SchoolofOptical-Electrical&ComputerEngineering,UniversityofShanghaiforScience&Technology,Shanghai200093, China)

Abstract:Inorderto solve the problem thatthe Chinesesummarization generated bytraditional abstractive modelscan hardly preserve the semantic information of the original text,this paper proposed an abstractive Chinese summarization model with linguistic features.Aconnection layerisadded tothe model,andfeaturessuch as partofspeech,named entity word position,and TF-IDF are spliced into the word vector,so that the word vector of the input model contains more semantic information to determine the keyentity.The pointer mechanism allows model selectivelycopy the keywords in source text into the summarization to improve the semantic relevance between source text and summarization.Evaluates this modelon LCSTS dataset,andobtains ahigher ROUGE score thanthe baseline model.The analysis shows that the model can generate Chinese summarization with higher semantic relevance.

Key words: abstractive summarization model; linguistic features; key entities; word vector

# 0 引言

生成简洁凝练，语义连贯，保留关键信息的总结是自动文本摘要的最终目标。根据对信息的抽取方式的不同，可将文本自动摘要技术主要分为两大类：抽取式文本摘要生成方式和抽象式文本摘要生成方式[1。目前的中文摘要研究大多使用抽取式方法，根据语言特征计算句子权重，复制比较重要的句子组成摘要，但这种方法没有考虑句子间的连贯性，不能完整的表达文章的含义；抽象式文本摘要生成方法应用神经网络模型，通过对大量的数据进行训练，生成深入理解原文的新句子。

与抽取式方法提取原文的句子作为摘要不同的是，抽象式摘要方法不是简单地从原文中提取的一些现有的段落或句子，而是对文档的主要内容进行了压缩解释，重新措辞，使用了原文档中未现的词汇来生成摘要。抽象式方法生成的摘要更接近于人工生成的摘要。Sutskever等人[2提出的sequence-to-sequence 模型（简称 seq2seq）和 Bahdanau 等人[3]提出的 Attention 机制，推动了抽象式自动摘要的发展。但抽象式摘要方法仍处于早期阶段，存在一定的局限性，比如，依赖大规模、高质量的训练集来训练模型；适用于短文本摘要生成，在长文本上的摘要效果较差；生成的摘要语义相关性较低，往往存在语法和语义错误。

为了提高抽象式摘要与原文本的相关度，本文提出了一种融合语言特征的抽象式摘要模型（简称LF_model)。本文认为抓住原文中的关键实体可以使摘要更加贴近文章的主题，考虑了输入模型的词汇的语言特征对摘要质量的影响，将原文本的词性标注，命名实体，词汇位置，TF-IDF等特征向量化后与原始词向量拼接在一起构建输入模型的词向量，使输入模型的向量有更多维度的含义来抓取原文中的关键实体。考虑到未登录词大多是原文本中的命名实体，解决OOV（outofvocabulary）问题有助于模型输出原文中的关键实体，本模型结合Gulcehre等人[5]提出的Pointer机制选择性地复制原文的词汇到摘要中，从而生成与原文本语义相关度高的摘要。使用LCSTS新闻数据集来训练模型，并将生成的摘要的评价得分同基线模型进行了对比，取得了比基线模型表现更好的实验结果。

# 1 相关工作

当前采用抽取式方法生成摘要的技术相对比较成熟，中文摘要的研究大多采用抽取式的方法，根据句子的各种文本特征，例如句子长度、句子位置、句子与文章标题的相似度、语言规则等来计算句子权重，根据句子的总权重给句子排序，选取权重高的句子作为摘要句。

Rush 等人[第一次使用Seq2Seq $^ +$ Attention 模型进行句子摘要任务，其中 Seq2Seq 模型也称为Encoder-Decoder 模型，使用一个循环神经网络作为编码器读取输入的句子，将整个句子的信息压缩到一个连续的中间语义向量中。再使用另一个循环神经网络作为解码器读取这个中间语义向量，将其解压为目标语言的一个句子[3]。Attention机制，使模型在输出端的某个节点将注意力集中在输入部分的某一个特定部分，而不是如以往的工作将输入部分作为一个整体均等的送入每一个输出端[3]，便于理解输入序列中的信息是如何影响最后生成的序列的。且作者提出了利用Gigaword 构建大量平行句对的方法，使得利用神经网络训练成为可能，但该模型更适用于为一个句子生成摘要。Lopyrev等人[7描述了一个使用LSTM（LongShort-TermMemory）作为循环神经网络计算单元，联合注意力机制来生成新闻摘要的应用，但未处理OOV（Outofvocabulary）问题。为了处理OOV问题，Gu等人[8提出了一种合并复制机制，允许一部分摘要复制原文中的内容。Nallapati 等人[]研究了关键词对于自动文摘所起到的关键作用，使用了Feature-richEncoder来尝试抓住句子的关键概念和关键实体。还提出了Generator-Pointer机制，使编码器能够生成原文中的句子。Romain等人[提出了内部注意力机制和新的训练方法，有效的提升了文本摘要生成的质量。Hu 等人[10]构建了一个大规模的中文语料库，并提供了基线，为研究中文摘要提供了便利。Ma 等人在提高抽象式摘要的质量上做了很多尝试，在文献[4中提出一种引入了相似性评估组件的模型来提高语义相关性，在文献[1]中，使用自动编码器作为辅助监控器，来改进中文新闻文本的文本表示。

本文借鉴抽取式方法，研究了语言特征对摘要的影响，提出了一种融合语言特征的抽象式中文摘要模型。使用引入注意力机制的encoder-decoder模型作为基础框架，在模型的输入端添加了拼接层，用于将原文词汇的词性、命名实体、词汇位置和TF-IDF等特征与原始词向量融合在一起，构成输入模型的词向量。使用Bi-LSTM(bi-directional longshort-termmemory)为编码器从正反两个方向编码生成中间语义向量，单向LSTM为解码器读取中间语义向量生成目标序列，模型结合pointer机制，在每个解码步骤中使用开关函数来决定是正常预测词表还是复制原文中的词，最终生成与原文语义相关度高的摘要。

# 2 模型

# 2.1基于语言特征的词汇表示

抽象式摘要方法，通过对大量的数据进行训练而预测生成的新的摘要句子，摘要句子中会出现在输入文档中未出现的句子。抽象式方法考虑了摘要句子的语法正确性和连贯性，而忽略了生成摘要与原文档的语义相关性，从而导致生成与原文无关的摘要[4]。如图1所示，RNN生成的摘要语句通顺但与输入的原文没有太大的关联。为了解决这个问题，本文抽取词汇的词性、命名实体、词汇位置和 TF-IDF 等特征来抓住原文本的关键实体。本文认为将词性、命名实体等语言特征融入词向量，可以改进模型避免语法错误并生成良好的摘要。词汇的 TF-IDF 特征值能够评估该词汇对原文的重要程度。除此之外，根据新闻的特点，将词在新闻文本中的位置也作为一项特征融入到了词向量中。

1)词性词性是词汇基本的语法属性，决定了词汇的语义倾向性[12]。提取词性特征有助于探究和识别相邻词之间的关系和化解自然语言中一词多义的问题，对语义理解具有重要的作用。研究发现，在摘要任务中名词和动词相对于其他词性的词汇往往更能体现原文的关键信息。因此本文对训练集中的词汇进行词性标注，将词性进行Embedding表示后与词向量拼接，使词汇的词向量包含词性特征。

原文：昨晚，中联航空成都飞北京一架航班被发现有多人吸烟。后因天气原因，飞机备降太原机场。几名乘客在舱门边吸烟被发现。有乘客要求重新安检，机长决定继续飞行，引起机组人员与未吸烟乘客冲突。目前中联航空正联系机组进行核实。  
人工摘要：成都飞北京航班多人吸烟机组人员与未吸烟乘客冲突。  
RNN：中联航空机场发生爆炸致多人死亡。

2)命名实体命名实体就是人名、地名、机构名、专有名词等具有特定意义的实体[12]。在摘要任务中，命名实体是文本信息的主要载体，识别出文章中的命名实体不仅有助于模型确定代表文章主题的关键实体还能帮助模型处理OOV问题，因此，本文对语料库进行命名实体识别，将命名实体Embedding后与词向量拼接，使词汇的词向量拥有命名实体特征。

3)词汇位置词汇在文本中所处的位置是新闻文本的另一个一个重要特征，新闻类文章的第一句或第一段往往会覆盖整片文章的主旨信息，距离文章开始位置越近的词汇越接近文章的主题，因此计算词汇的位置特征如式(1)所示来提高摘要的质量。

$$
L o c _ { i } = \left( 1 + n - l _ { i } \right) / n
$$

其中：Loc代表词汇的位置特征， $l _ { i }$ 代表新闻文本中第i个词汇的位置，n代表该新闻文本中总的词汇数目，Loc的值越大，证明该位置的词汇越重要，。

4)TF-IDF词频-逆文档频率(term frequency-inversedocumentfrequency，简称TF-IDF)是一种统计方法，用以评估一个特定词语对于一个语料库的其中一份文本的重要程度。TF为词频，用来统计词汇在该文本中出现的频率。IDF为逆文档频率，用于识别某一词汇在整个语料库中的重要性。TF-IDF为词频与逆文档频率的乘积，TF-IDF越大，则说明这个词对这篇文章的区分度就越高。TF-IDF的计算公式如下所示：

$$
t f - i d f _ { i , j } = { t f _ { i , j } } ^ { * } i d f _ { i }
$$

$$
t f _ { i , j } = \frac { n _ { i , j } } { \sum _ { k } n _ { k , j } }
$$

$$
i d f _ { i } = \log \left( { \frac { N } { D F _ { i } + 1 } } \right)
$$

在式(3)中， $n _ { i , j }$ 为词汇 $t _ { i }$ 在文章 $d _ { j }$ 中出现的次数， $\sum _ { k } n _ { k , j }$ 为文章 $d _ { j }$ 中所有的词汇数目。在式(4)中，N为语料库的文档总数， $D F _ { i }$ 为语料库中包含词汇 $t _ { i }$ 的文档数目。当词汇未出现在语料库中时 $D F _ { i }$ 为零，为了避免分母为零，将 $D F _ { i } + 1$ 作为分母来计算 IDF。

本文将词embedding成原始词向量，在原始词向量后添加经过embedding后的POS、NER和Los、TF-IDF等特征。

于是输入编码器的词汇被形象的表示为

$$
x _ { i } = \left\{ r _ { i } ^ { w } , r _ { i } ^ { p o s } , r _ { i } ^ { n e r } , r _ { i } ^ { l o c } , r _ { i } ^ { t f - i d f } \right\}
$$

其中: $r ^ { w }$ 代表词汇的原始词向量， $r ^ { p o s }$ 代表词的词性标注的embedding向量， $r ^ { n e r }$ 代表词的命名实体识别的embedding向量， $r ^ { l o c }$ 代表词的位置特征， $r ^ { t f - i d f }$ 是词的TF-IDF特征。拼接层将这五种向量拼接起来作为最终输入编码器的向量。

# 2.2LSTM循环神经网络

基于神经网络的 seq2seq模型由两部分组成，编码器和解码器，LSTM长短期记忆网络是一种特殊的循环体结构，LSTM计算单元添加了一种门机制来解决标准RNN模型的梯度消失问题。LSTM的计算单元的结构如图2所示。在很多任务上，采用LSTM结构的循环神经网络比标准的循环神经网络表现更好。本模型使用LSTM作为编码器和解码器，LSTM在t时刻的隐藏层状态 $h _ { { \scriptscriptstyle t } }$ 的计算公式如下所示：

$$
\left[ \begin{array} { l } { i _ { t } } \\ { f _ { t } } \\ { O _ { t } } \\ { c _ { t } } \end{array} \right] = \left[ \begin{array} { l } { \sigma } \\ { \sigma } \\ { \sigma } \\ { \tan { h } } \end{array} \right] \quad W \cdot \left[ h _ { t - 1 } , x _ { t } \right] \quad
$$

$$
C _ { t } = f _ { t } \odot C _ { t - 1 } + i _ { t } \odot c _ { t }
$$

$$
h _ { t } = O _ { t } \odot \tan h ( C _ { t } )
$$

其中： $i _ { \iota }$ 指输入门， $f _ { t }$ 指忘记门， $O _ { t }$ 指输出门， $\boldsymbol { c } _ { t }$ 更新候选向量， $W$ 代表被学习的权值矩阵， $\sigma$ 代表激励函数， $\odot$ 代表逐点运算操作。

![](images/b8800aa29e7f497dde1c54eafc700df6b394950e7db2ac43aeed09eb5fc5dcfa.jpg)  
图2LSTM计算单元结构图 Fig.2Illustration of LSTM

# 2.3融合语言特征的神经网络模型

融合语言特征的神经网络模型如图3所示。本模型在输入层添加了一个拼接层，用于将词汇的原始词向量与词性、命名实体、词汇位置、TF-IDF等语言特征拼接起来生成最终输入模型的词向量。原始词向量进入拼接层，拼接层根据式（1）计算该文章中词汇的位置信息，根据式（2）计算该文章中词汇的 TF-IDF 特征值，将每个词汇的词性标记和命名实体标记映射为POSembedding和NERembedding。将每个词汇的POS embedding、NERembedding、Loc、IF-IDF与原始词向量拼接在一起，最终构成一个512维的向量$x _ { i } = \left\{ r _ { i } ^ { w } , r _ { i } ^ { p o s } , r _ { i } ^ { n e r } , r _ { i } ^ { l o c } , r _ { i } ^ { t f - i d f } \right\} _ { \mathrm { ~ c ~ } }$

编码器将整个原文本压缩成一个连续的向量，学习原文本的每个单词的矢量表示。本文使用Bi-LSTM作为编码器，向前LSTM从左向右读取输入序列 $\mathbf { x } = \left( x _ { 1 } , . . . , x _ { \mathrm { m } } \right)$ ，生成隐藏状态序列 $\left( \overrightarrow { h _ { 1 } } , . . . , \overrightarrow { h _ { m } } \right)$ 。向后的LSTM反向读取输入序列，生成$\left( \overleftarrow { h } _ { 1 } , . . . , \overleftarrow { h } _ { m } \right)$ 。在每个时间步骤中连接向前和向后的LSTM的隐藏状态得到 $( h _ { 1 } , . . . , h _ { m } )$ ，其中 $\left( h _ { _ t } = \left[ \overrightarrow { h _ { t } } ; \overrightarrow { h _ { t } } \right] \right)$ ，隐藏状态 $h _ { \scriptscriptstyle t }$ 包含了词向量正反两个方向的信息。

本文使用单向LSTM作为解码器， $h _ { i } ^ { ' }$ 表示解码器在i时刻的隐藏状态，如式（9）所示。在每个解码步骤中，引入了注意力机制使注意力集中在输入序列的某一个特定部分，内容向量 $c _ { i }$ 用于对系统所关注的词进行编码以生成下一个摘要词，如式（10）～（12）所示。

$$
\ddot { h _ { i } } = f \left( \dot { h _ { i - 1 } } , y _ { i - 1 } , c _ { i } \right)
$$

$$
e _ { i j } = a \big ( h _ { i - 1 } ^ { ' } , h _ { j } \big )
$$

$$
a _ { i j } = \frac { e x p \left( e _ { i j } \right) } { \sum _ { k = 1 } ^ { m } e x p \left( e _ { i k } \right) }
$$

$$
c _ { i } = \sum _ { j = 1 } ^ { \mathrm { m } } a _ { i j } h _ { j }
$$

其中： $e _ { i j }$ 是输入隐藏状态 $h _ { j }$ 和输出隐藏状态 $h _ { i - 1 } ^ { ' }$ 的注意力得分， $a _ { i j }$ 为标准化后的注意力得分。

模型结合Pointer机制，在解码端使用了一个开关(switch)函数，决定在每个解码步骤是正常的预测词表生成摘要词 $y _ { t } ^ { g }$ ，还是复制原文的词 $y _ { t } ^ { p }$ 作为摘要词。如果 $u _ { i } = 1$ ，表示开关打开，正常预测词表。如果 $u _ { i } = 0$ ，表示开关关闭，指向原文的一个位置，将指针指向的词作为输出。开关打开的概率的计算公式如下：

$$
\mathrm { p } \left( u _ { i } = 1 \right) = \sigma \left( \nu ^ { s } \cdot \left( W _ { h } ^ { s } h _ { i } ^ { \prime } + W _ { e } ^ { s } E \left[ o _ { i - 1 } \right] + W _ { c } ^ { s } c _ { i } + b ^ { s } \right) \right)
$$

其中： $h _ { i } ^ { ' }$ 是隐藏状态， $\mathbb { E } \big [ o _ { i - 1 } \big ]$ 上一个时间步的词向量， $c _ { i }$ 是上下文的权重， $\boldsymbol { W } _ { h } ^ { s }$ ， $\boldsymbol { W } _ { e } ^ { s }$ ， $\boldsymbol { W } _ { c } ^ { s }$ ， $\boldsymbol { b } ^ { s }$ 和 $\boldsymbol { \nu } ^ { s }$ 是开关的参数。使用文档中单词的注意力分布作为采样指针的分布。

$$
p _ { i } ^ { a } \left( j \right) \propto e x p \left( \nu ^ { a } \cdot \left( W _ { h } ^ { a } \dot { h _ { i - 1 } } + W _ { e } ^ { a } E \left[ o _ { i - 1 } \right] + W _ { c } ^ { a } h _ { j } + b ^ { a } \right) \right) ,
$$

$$
p _ { i } = a r g m a x \left( p _ { i } ^ { a } \left( j \right) \right) f o r j \in \{ 1 , . . . , m \}
$$

$p _ { i } ^ { a } \left( j \right)$ 是解码的第i个时间步指向原文位置 $\mathrm { j }$ 的概率， $h _ { j }$ 是编码器在位置 $\mathrm { ~ j ~ }$ 处的隐藏状态， $p _ { i }$ 是摘要位置 $\mathbf { i }$ 处的指针值。联合式（13）（14）得到最终输出 $y _ { i }$ 的概率为

$$
p \left( y _ { i } \right) = p \left( u _ { i } = 1 \right) p \left( y _ { i } | u _ { i } = 1 \right) + p \left( u _ { i } = 0 \right) p \left( y _ { i } | u _ { i } = 0 \right)
$$

在训练时过程中，当摘要中出现未登录词时，为模型提供显式的指针信息，当生成摘要的第i个位置的单词是未登录词或命名实体时 $u _ { i }$ 被设置为0。优化似然函数如下所示：

$$
\log p ( y | x ) =
$$

$$
\sum _ { i } ( l o g \left\{ p \left( y _ { i } | y _ { - i } , x \right) p \left( u _ { i } \right) \right\} + \log p \left( p \left( i \right) | y _ { - i } , x \right) \left( 1 - p \left( u _ { i } \right) \right) )
$$

在测试时，模型在每个时间步，根据估计的开关函数的概率 $\mathsf { p } ( u _ { i } )$ 来决定是正常的预测词表还是指向原文中的一个位置。

# 2.4摘要生成流程

新闻摘要的生成流程如图4所示。

a）读取新闻文本text。

b）预处理：为新闻文本分词，分词后生成词汇表Vocab,为Vocab中的词生成所对应的词性标志和命名实体标志。

c）计算输入序列长度 $\scriptstyle { \mathrm { m } = 0 }$ count(Vocab)，创建输入模型的向量数组new Text_Matrix[m][]，将Vocab中的词汇、词性标志和命名实体标志向量化，获得原始词向量 $r ^ { \scriptscriptstyle { w } }$ 、词性标志向量 $r ^ { p o s }$ 和命名实体标志的向量 $r ^ { n e r }$ ，根据式(1)计算词的位置特征值 $r ^ { l o c }$ 、根据式(2)计算每个词的TF-IDF 值 $r ^ { t f - i d f }$ 。

d）拼接语言特征向量concatenate $( r ^ { w } , r ^ { p o s } , r ^ { n e r } , r ^ { l o c } , r ^ { t f - i d f } ) $   
Text_Matrix[i][]。e）计算编码层隐藏状态 $h _ { { \scriptscriptstyle t } }$ ，根据式(16)计算输出 $y _ { i }$ 的  
概率使用BeamSearch算法selecttop5score迭代预测摘要。

f）输出新闻摘要。

![](images/24494321568c54e92915b342179d57a947c8b77ec1c36c58a72f43d097bf8f4f.jpg)  
图3融合语言特征的神经网络模型

![](images/dc88823205eddf3ac6c94df4f770ac659ecdaf813fabd522a6eab2b03c01c586.jpg)  
Fig.3Abstractive model with linguistic features   
图4摘要生成流程图  
Fig.4Summarization generating process

# 3 实验

# 3.1数据集

数据集的质量、内容和规模都直接影响摘要的生成效果，LCSTS是当前最大规模的中文数据集，是从新浪微博上爬取过滤得到的，包含了超过240多万对的新闻文本以及摘要，该数据集质量高，涵盖领域广。数据集来源于具有较大影响力的官方微博，例如，“人民日报”“经济观察报”“国防部”等[10]，这些新闻内容书写规范，语句通顺，几乎不存在错别字，非常适合深度学习模型的研究。LCSTS的训练集有240万多对，验证集有1万多对，测试集有1千多对，而且验证集和测试集用人工标注了正文和标题之间的相关度，并且从1-5打分，分数越高越好。本文采用LCSTS中给出的数据集来训练模型，使用测试集中3分以上的数据来测试模型。

# 3.2预处理

本文首先对语料库中的文本进行预处理。词是最小的能够独立运用的语言单位，本文使用基于分词的词向量进行实验，使用StanfordCoreNLP工具包对语料进行分词、词性标注和命名实体识别。表1给出了一篇新闻内容经过预处理的示例。统计数据集内每个词汇的出现频率，按照词频的顺序对单词进行排序，从中选取使用频率高的词汇来构建词汇表，词汇表大小为50000。词汇表中的<unk>符号被用来代替测试数据集中未出现在训练词汇表中的词。使用Gensim工具包，对包含动词、形容词、名词、代词等40种中文词性标记进行embedding处理生成POSembedding，对包含有人名、机构名、地名、时间、日期、货币、百分比和非命名实体等8种类型命名实体标记进行embedding 处理生成NERembedding，使用CBOW模型来生成每个词所对应的原始词向量。将每个词的原始词向量与所对应的POS embedding 和NERembedding一一对应。

表1文本预处理示例  

<html><body><table><tr><td>Table 1Example of pre-process</td><td>总理18日在美药典公司餐厅与10家进驻自贸</td></tr><tr><td></td><td>区的中外企业家座谈，请他们给自贸区各项改革“打 新闻内容分”。他对10位参会企业家说：“希望我们在留有饭菜 余香中进行的座谈会，不仅 friendly(友好)，而且 frankly(坦率)，有什么问题直来直去讲出来。</td></tr><tr><td>分词结果</td><td>总理18日在美药典公司餐厅与10 家 进驻自贸区的中外企业家 座谈 请 他们给 自贸区各项改革打分他对10位参会 企业家 说希望我们在留有饭菜余香中进行的座谈 会不仅 friendly 友好而且 frankly 坦率 有什么 问题 直来直去 讲出来</td></tr><tr><td>词性标注结果 命名实体识别 000LocationOOOOOOOOOOOOOOO0O 结果</td><td>nr n mqpan n n p m n v nz uj jnv v rvnzr vn vrp unqvnvvrpvnnfvujncen adcenanvrnlvv PersonOOOOOOOOOOOOLoctionOOOOO</td></tr></table></body></html>

# 3.3评价指标

如何有效合理地评价文本摘要的生成效果是一个很难的问题，当前的文本摘要评价方法分为两类，一种是内部评价方法，将获得的摘要与参考摘要进行对比，根据两者的相似性进行评价。与参考摘要越吻合，说明摘要的质量就越高;另一种是外部评价方法，将摘要应用于特定的任务，根据摘要提高这项任务的效果来评价生成文摘的效果。本文使用了

目前流行的内部评价方法ROUGE。ROUGE评价方法是Lin 等人[13]提出的一种自动文本摘要评价方法。其通过统计自动生成的摘要与参考摘要之间的重叠基本单元的数目来评价文摘的质量。本文使用的参考摘要为数据集中的人工摘要。ROUGE 常用的评价标准有ROUGE-N和ROUGE-L。其中ROUGE-N表示系统生成摘要的 $\mathfrak { n }$ -gram召回率，ROUGR-L

表示系统摘要和参考摘要的最大公共序列。ROUGE-N的计算方法如式（17）所示。

$$
{ \mathrm { R O U G E - N } } = { \frac { \sum _ { s \in R } \sum _ { g r a m _ { n } \in S } C o u n t _ { m a t c h } \left( g r a m _ { n } \right) } { \sum _ { s \in R } \sum _ { g r a m _ { n } \in S } C o u n t \left( g r a m _ { n } \right) } }
$$

其中： $C o u n t _ { \scriptscriptstyle m a t c h } \left( g r a m _ { \scriptscriptstyle n } \right)$ 表示系统输出的摘要与参考摘要重叠的 $\mathbf { n }$ -gram的个数，R表示参考摘要。ROUGE-L考虑了摘要中词汇的顺序，评价更合理。本文使用了Lin提供的标准工具包，选用了ROUGE-1，ROUGE-2，和ROUGE-L来评价本模型生成的摘要质量。

# 3.4实验设置

本文使用TensorFlow框架进行实验，原始词向量的维度是350，经过拼接层后输入编码器的词向量的维度是512，隐藏层的大小为512，批次大小为64。使用Adam优化器，默认设置为 $\mathbf { \alpha } \mathbf { { a = } } 0 . 0 0 1$ ， $\beta _ { 1 } = 0 . 9$ ， $\beta _ { { } _ { 2 } } = 0 . 9 9 9$ ， $\epsilon = 1 \times 1 0 ^ { - 8 }$ 。在测试时，为了得到最符合语言模型的摘要，本文选择使用集束搜索(BeamSearch)算法，集束大小设置为5来生成摘要。

# 3.5实验结果及分析

本文使用LCSTS中文数据集来验证模型的生成效果，将实验结果分别与 $\mathrm { H u }$ 等人[10]提出的RNNcontext模型、Gu等人[8提出的COPYNET模型和Ma等人[1I提出的SRB模型的实验结果进行对比。取得了比上述模型更高的ROUGE 得分，如表2所示。

表2在lcstc数据集上的ROUGE得分表Table 2 ROUGE score on LCSTS dataset  

<html><body><table><tr><td>model</td><td>R-1</td><td>R-2</td><td>R-L</td></tr><tr><td>RNN context(W)</td><td>26.8</td><td>16.1</td><td>24.1</td></tr><tr><td>RNN context(C)</td><td>29.9</td><td>17.4</td><td>27.2</td></tr><tr><td>COPYNET(W)</td><td>35.0</td><td>22.3</td><td>32.0</td></tr><tr><td>COPYNET(C)</td><td>34.4</td><td>21.6</td><td>31.3</td></tr><tr><td>SRB(C)</td><td>33.3</td><td>20.0</td><td>30.1</td></tr><tr><td>LF_model</td><td>36.2</td><td>23.6</td><td>32.9</td></tr></table></body></html>

RNNcontext 模型是Hu等人[o使用的引入上下文的摘要生成模型。作者对LCSTS数据集分别进行词语级别分词处理和字符级别分词处理后进行对比，实验结果显示使用

字符级别分词处理优于基于词语级别分词处理结果。这是因为，根据字符分词生成的词典远远小于根据词语分词生成的词典。字符词典能覆盖更多的原文内容，有效减少了OOV问题。RNNcontext模型的结果成为后来使用LCSTS数据进行中文摘要研究的基线。

COPYNET模型是Gu等人[8提出的一种能够解决OOV问题的模型。COPYNET在 Seq2seq $+$ Attention 模型的基础上引入了拷贝机制，允许部分摘要复制原文中的内容。在基于分词表示的摘要任务上取得了更高的ROUGE 得分。

SRB模型是Ma等人[11]提出了一种基于语义相关性的神经模型，用来鼓励文本和摘要之间的语义相似性。SRB在基于字符表示的摘要任务中能够生成与原文语义相关度较高的摘要，但未考虑OOV问题获得了低于COPYNET模型的ROUGE 得分。

由于汉字通常有多重语义，使用基于字符的向量可能会误解句子的意义。本文认为基于词语的表示可以更准确的捕捉文章的语义，因此本模型使用基于分词表示的词向量输入模型。除此之外，本模型结合Pointer机制选择性地输出原文中的词汇，不仅能够有效地解决OOV问题还能输出原文中的关键词。

本文使用融合了词性、命名实体、IF-IDF值和Loc 等语言特征的LF_model生成的摘要与未融合语言特征的lackfeaturemodel生成的摘要进行了对比，结果如表3所示。

实验结果表明，使用融合语言特征的模型获得了更高的ROUGE得分。证明了语言特征对生成摘要的质量的影响，融入词性、命名实体、位置特征、TF-IDF等语言特征扩展了词向量的维度，使输入模型的词向量包含了更多的语义信息。词性信息使模型关注原文中动名词，识别出命名实体有助于模型输出OOV词，IF-IDF帮助模型识别语料中的重要词汇，位置信息使模型关注词汇在文章中的位置对摘要的影响。最终生成更贴近原文主题的摘要。

表3融合语言特征的rouge 得分表  
Table3 ROUGE score with linguistic features   

<html><body><table><tr><td>model</td><td>R-1</td><td>R-2</td><td>R-L</td></tr><tr><td>lack feature model</td><td>35.4</td><td>21.1</td><td>30.5</td></tr><tr><td>LF_model</td><td>36.2</td><td>23.6</td><td>32.9</td></tr></table></body></html>

图5是使用本模型生成的摘要实例，通过观察可以发现传统的RNN模型生成的摘要可读性差，语义相关性较低，存在OOV问题。本模型生成的摘要抓住了该条新闻的关键词汇“ 总理”“企业家和自贸区”，可读性更强，并且缓解了OOV问题。结果表明本文提出的模型生成了更接近人工生成的摘要，生成的摘要与原文内容的语义相关度较高。

原文：李克强总理18日在美药典公司餐厅与10家进驻自贸区的中外企业家座谈，请他们自贸区各项改革"打分”。他对10位参会企业家说：“希望我们在留有饭菜余香中进行的座谈会，不仅 friendly(友好)，而且 frankly(坦率)，有什么问题直来直去讲出来。”  
人工摘要：李克强邀10企业给上海自贸区打分。  
RNNcontext(W)：李克强在每UNK公司给自贸区打分：有什么问题UNK 讲出来。  
Ourmodel：李克强总理与中外企业家座谈自贸区改革。

# 4 结束语

考虑到输入模型的向量对摘要的影响，本文提出了一种融合语言特征的神经网络摘要模型来解决语义相关度低的问题。模型中的拼接层，用于将语言特征融入词向量，使模型能够抓取原文本中的关键实体，pointer 机制用来解决OOV问题和输出关键实体。在LCATS数据集上的实验结果表明，本文提出的模型不仅生成高于基线模型的ROUGE 得分，还能够抓住原文本的关键实体缓解OOV问题，生成与原文本语义相关度较高的摘要。

# 参考文献：

[1]Paulus R,Xiong Caiming,Socher R.A deep reinforced model for abstractive summarization.[EB/OL].(2o15).http://cn.arxiv.org/abs/   
1705.04304. [2]Sutskever I, Vinyals O,Le Quoc V. Sequence to sequence learning with neuralnetworks[EB/OL].(2014-12-14).https://arxiv.org/pdf/   
1409.3215v3.pdf. [3]Bahdanau D,Cho K,Bengio Y.Neural machine translation by jointly learning to align and translate[EB/OL].(2014).http://cn.arXiv.org/abs/   
1409.0473. [4]Ma Shuming,Sun Xu,Xu Jinging.Improving semantic relevance for sequence-to-sequence learning of Chinesesocial mediatext summarization [C]//Proc of Meeting of Association for Computational Linguistics. 2017: 635-640.   
[5]Gulcehre C,Ahn S,Nallapati R.Pointing the unknown Words [EB/OL]. (2016).http//cn.arxiv.org/abs/1603.08148.   
[6]Rush A M, Chopra S,Weston J.A neural model for abstractive sentence summarization [C]//Empirical Methods in Natural Language Processing. 2015:379-389.   
[7]Lopyrev K.Generating new headlines with recurrent neural networks [J].Computer Science,2015.   
[8]Gu Jiatao,Lu Zhengdong,Li Huang. Incorporating copying mechanism in sequence-to-sequence learning [Cl// Proc of the 54 th Annual Meeting of the Association for Computational Linguistics.ACL.2016: 1631-1640.   
[9]Nallapati R， Zhou Bowen， dos SantosC. Abstractivetext summarization using sequence-to-sequence RNNs and beyond [C]// Proc of the 2Oth SIGNLL Conference on Computational Natural Language Learning.2016:280-290.   
[10] Hu Baotian,Chen Qingcai,Zhu Fangze.LCSTS:a large scale Chinese short text summarization dataset [J].Computer Science，2015, 2667-2671.   
[11] Ma Shuming,Sun Xu,Lin Junyang.Autoencoder as assistant supervisor: improvingtext representationforchinesesocial mediatext summarization [EB/OL].(2018).http//cn.arxiv.org/abs/1805.04869.   
[12]宗成庆.统计自然语言处理[M].2版．北京：清华大学出版社，2013: 150-175.(Zong Chengqing, Statistical natural language processing [M]. 2nd ed.Beijing: Tsinghua University,2013:150-175.)   
[13]Lin C Y,Hovy E.Automatic evaluation of summaries using $n$ -gram co-occurrence statistics [C]//Proc of Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. Association for Computational Linguistics,2003: 71-78.   
[14]Tan Jiwei，Wan Xiaojun，Xiao Jianguo．Abstractive document summarization with a graph-based attentional neural model [C]// Proc of Meeting of the Association for Computational Linguistics.2017: 1171-1181.   
[15] See A,Liu PJ,Manning C D.Get to the point: summarization with pointer-generator networks[EB/OL]．(2017-04-25）．http://arxiv. org/abs/1704.04368.   
[16]Li Piji,Lam Wai,Bing Lidong.Deep recurrent generative decoder for abstractive text summarization [C]//Proc of Conference on Empirical Methods in Natural Language Processing.2O17:2091-2100.