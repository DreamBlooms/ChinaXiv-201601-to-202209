# 外显和内隐情绪韵律加工的脑机制：近红外成像研究1

雷震」　毕蓉² 莫李澄²于文汶²张丹丹1,2(西南财经大学中国行为经济与行为金融研究中心，成都611130,深圳大学心理学院，深圳 518060)

摘 要准确识别言语中的情绪韵律信息对社会交往非常重要。本研究采用功能近红外成像技术，探索外显和内隐情绪加工条件下愤怒、恐惧、快乐三种情绪韵律加工过程中的大脑皮层神经活动。结果表明，对愤怒、恐惧、快乐韵律进行特异性加工的脑区分别为左侧额极/眶额叶、右侧缘上回、左侧额下回，其中右侧缘上回脑区同时受到情绪和任务的调控。此外，右侧颞中回、颞下回和颞极在情绪外显任务中的激活明显强于内隐任务。本研究的结果部分支持了情绪韵律的层次模型，也对该模型的第三层次，即“额区对语音情绪信息的精细加工需要外显性情绪加工任务参与”提出了质疑。

关键词情绪；语音韵律；颞上回；眶额叶；额下回；缘上回

# 1引言

准确解码情绪信息不但对人类的生存至关重要，而且还能帮助我们在社会交往中感知他人的情绪状态、推测他人的意图。在日常交流中常见的传递情绪的载体包括面孔表情、身体姿态、语音等。对面孔和身体姿势情绪信息的加工通过视觉通道完成，目前其神经机制已经得到了较充分的揭示（参见综述 Calvo& Nummenmaa,2016; Enea& Iancu, 2016; Hinojosa,Mercado,&Carretie,2015)。相比于视觉通道，我们对语音中情绪信息的解码机制还不是非常清楚（Liebenthal,Silbersweig,& Stern,2016)。语音的情绪信息可以通过语义和韵律表达。本研究拟考察情绪韵律加工的神经机制。情绪性韵律（affective prosody）是指说话人不依赖语义和语法结构，借助语音中的音高、音长、音强、重音和语调等多种声学线索的动态变化来表达和传递情绪的方式（Bruk,Kreifelts,&Wildgruber,2011)。语音的情绪韵律线索不但帮助我们理解说话人的语义和情绪状态，当情绪韵律与语义信息冲突时，我们通常依赖前者去推测说话人的意图（Ben-David,Multani,Shakuf,Rudzicz,& van Lieshout,2016)。例如，当某人用愤怒的情绪韵律说“我真幸运”时，我们会认为说话人想表达愤怒情绪。因此，对语音中情绪韵律的加工与我们的生活和社会交往密切相关（Bruik et al.，2011；Früholz,Trost，&Kotz,2016)。揭示和阐明情绪韵律加工的神经机制，不但帮助我们认识大脑的情绪语音加工过程，也有利于对具有社会功能障碍的患者（自闭症、精神分裂症、抑郁症等）进行早期诊断和康复评估（Knight＆ Baune,2019;Lin,Ding,& Zhang,2018)。此外，揭示大脑对情绪韵律的解码规则是人工合成情绪性语音语调的基础，这将极大促进人工智能和人-机交互界面的发展（Mitchell& Xu,2015）。

早期关于情绪韵律加工的神经机制模型（Ross,1981）认为，情绪韵律完全由右半脑完成，其中右侧颞上皮层（superior temporal cortex,STC²）中与左脑Wernicke 区对应的部位负责情绪韵律的感知和理解，右背外侧前额叶中与左脑 Broca区对应的部位负责产生情绪性语音韵律。随着研究的深入，学者们逐渐发现大脑皮层的其他部分和皮下结构也参与了情绪韵律的加工，逐渐形成了情绪韵律加工的层次模型（hierarchical model）（Bruk et al.，2011;Ethofer, et al. 2006; Schirmer & Kotz, 2006; Witteman, Van Heuven,& Schiller,2012)。该模型认为大脑对情绪韵律的加工分为三个层次：1）对声音敏感的 STC 的中部（mid-STC）接收初级听觉皮层的神经信号，负责提取语音中的声学参数；2）右侧 STC 的后部（p-STC）通过多通道整合的方式识别语音中的情绪信息；3）以双侧额上回（inferior frontalgyrus,IFG）和眶额皮层（orbitofrontal cortex,OFC）为代表的额区负责对语音中的情绪信息进行评估和精细加工。情绪韵律加工的层次模型认为，听觉信息从初级听皮层传至mid-STC 的过程为自底向上的刺激驱动加工，而在 p-STC 和额区进行的语音信息整合和情绪精细加工需要依赖于注意和外显性情绪评估（Bruk et al.,2011)。进一步的，Fruholz 等（2016）在对情绪声音（包括非人类发出的声音、人类的非言语声音、韵律语音、音乐）的加工脑机制研究结果进行元分析后指出，情绪声音加工依赖于广泛分布的神经网络，其核心脑区包括杏仁核（Fruholz,Hofstetter, Cristinzio,Saj,Seeck,& Vuilleumier,2015）、初级和次级听觉皮层、右侧STC（Fruholz & Grandjean,2013a）、IFG（Früholz & Grandjean,2013b）、OFC（Kotz,Kalberlah,

Bahlmann,Friederici,& Haynes,2O13）、脑岛(Mothes-Lasch,Mentzel,Miltner,& Straube,2011)。

我们认为在情绪韵律加工的脑机制方面，目前有三个问题需要解决。第一，不同情绪种类的语音韵律的大脑表征。已知人类对愤怒、厌恶、恐惧、快乐和悲伤等基本情绪的加工依赖不完全重叠的脑区，即大脑对不同种类情绪的加工具有相对特异性（Lindquist,Wager,Kober,Bliss-Moreau,& Barrett,2012），但这一结论主要是依据视觉通道的研究证据得出的（例如面孔表情、身体姿势、情绪词加工等任务）。在情绪的听觉加工领域，特别是情绪韵律加工方面，目前已发现有两项相关研究，他们利用机器学习算法，采用数据驱动的方式对被试在收听不同种类情绪韵律的功能性核磁共振成像（functional magnetic resonance imaging,fMRI）激活模式进行分类（Ethofer,Van De Ville,Scherer,& Vuilleumier,2009a; Kotz et al.,2013）。结果发现，对多分类（区分不同种类的情绪）有贡献的脑区包括：颞上回（superiortemporal gyrus,STG）、颞上沟（superior temporal sulcus,STS）、颞中回（middle temporal gyrus,MTG）、IFG、额中回（middle frontal gyrus,MFG）、前脑岛。这两项研究表明，上述脑区对不同种类情绪韵律的加工具有一定的区分度（即特异性)。另外Kotz 等人（2013）还对比了多种情绪韵律与中性韵律激活的脑区，发现每种情绪都激活了不同的脑区，例如，愤怒韵律比中性韵律更强地激活了双侧 MFG，快乐韵律比中性韵律更强地激活了左侧 IFG。然而，除了上述三项研究，目前在情绪韵律加工方面已有的研究，大多只关注了某一种情绪韵律与中性韵律的差异性大脑表征，很少考察大脑对不同情绪种类韵律的特异性加工（Fruholz &Grandjean,2013a)。我们认为，仅比较某种情绪与中性条件的差异性脑区，并不能获得情绪韵律加工的“特异性”脑区。本研究将考察愤怒、恐惧、快乐这三类辨识度较高（Liu&Pell,2012）且具有代表性的情绪韵律的特异性加工脑区。具体的，我们将对三种情绪韵律条件进行两两比较，尝试找寻仅参与单种情绪韵律加工的脑区。依据相关文献(Fruholz& Grandjean,2013b)，我们预期在IFG 等额区可能发现能区分三种情绪韵律的特异性脑区。

第二，“情绪相关”任务与“情绪无关”任务（task relevant/irrelevant）激活脑区的异同。虽然已有一些研究采用外显和内隐任务比较了两种条件下情绪韵律加工参与脑区的差异，但目前矛盾的结果还很多。例如，有研究发现内隐任务激活了p-STG，而外显任务反而激活了mid-STG（Früholz,Ceravolo,& Grandjean,2012），这与前文所述的层次模型的预期相反。此外，不少研究发现杏仁核、脑岛等情绪脑区在外显任务中的激活比内隐任务更强（Fruholzet al.，2012）；而另一些研究则同时在外显和内隐任务中观察到了情绪主效应，即愤怒等情绪韵律比中性韵律在这些脑区均诱发了更强的神经活动（Bach etal.，2008；Ethofer et al,

2009b;Quadfieg,Mohr,Mentzel,Miltner,& Straube,2008）。更重要的是，已有研究在额区的发现也不一致：有研究报道外显和内隐情绪任务均会激活双侧或单侧IFG(Fruholz et al.,2012;Steber,Konig，Stephan,& Rossi,2020），但也有研究观察到了OFG（Ethofer et al.,2009b;Quadfieg et al.,2008）和 IFG（Bach et al.,2008; Beaucousin et al.,2011）等脑区在外显任务中有更强的激活。我们认为，现有的矛盾首先可能是由于任务设置的不合理造成的。上述研究几乎均采用“情绪辨别”作为外显任务，“性别辨别”作为内隐任务。相比于情绪辨别，辨别说话人的性别更简单，因此被试在完成性别辨别任务时可能还不自主的进行了情绪加工、语义加工等，从而在结果中引入了混淆变量。本研究将“性别辨别”替换为“身份辨别”，即语音材料均为女性声音，被试的任务为辨别不同身份的女性，加大了内隐任务的难度，降低了被试进行情绪加工的可能性。其次，已有研究绝大多数采用词语作为刺激材料（例如Bach et al., 2008; Ethofer et al.,2009b; Früholz et al.,2012; Quadfieg et al.,2008; Steber et al.,2020）。考虑到情绪韵律为语音的长时程特征，而单个词的呈现时间非常有限（550\~750 ms，多为双音节词），我们认为在情绪韵律的研究中应该采用更长的语音单元（例如句子）作为实验材料。此外，已有的研究有的使用有语义的材料（Beaucousin et al.,2011; Ethofer et al.,2009b;Mitchell,2007;Quadfieg et al.,2008），有的使用无语义的材料（Bach et al.,2008;Fruholz et al.,2012; Steber et al.,2020），这也可能造成对实验结果的干扰，因此在本研究中我们使用了无语义的“伪句”。在提高了内隐任务的认知负荷、延长了情绪韵律材料的呈现时间、排除了语义加工的干扰后，本研究希望能更准确地揭示外显和内隐情绪韵律加工的脑机制差异。由于已有文献存在大量矛盾的结果，我们对此问题无法提出具体的假设（探索性问题）。

第三，基于无噪声脑成像技术的实验证据需进一步累积。已有的绝大部分情绪韵律加工脑机制研究使用了fMRI技术(Fruholz etal.,2016)。fMRI是目前脑影像研究中的主流技术，能对全脑的神经活动进行较准确的空间定位。但fMRI在扫描过程中由于梯度线圈定位等环节会产生高强度的噪声，这对情绪韵律加工等需要关注各种声学参数细节的任务会造成一定的影响（Dieler,Tupak,&Fallgatter,2012)。因此我们建议基于声学感知通道的 fMRI研究结果最好能通过其他脑成像技术加以验证。功能近红外光谱成像技术（functional near-infraredspectroscopy，fNIRS）具有无噪声、对头动相对不敏感等特点，很适合用于语音加工研究。本研究希望利用 fNIRS 技术得到与已有的 fMRI研究较一致的情绪韵律加工脑区。

# 2方法

# 2.1被试

采用 G\*Power3.1.7 软件预估样本量，参考已有类似文献的效应量（partial $\begin{array} { r } { \eta ^ { 2 } > 0 . 1 0 \rangle } \end{array}$ （Bach etal.,2008；Früholz etal.,2012)，计算得到16 名被试即可达到0.95 的统计检验力（power)。同时，参考课题组前一项研究的样本量（ $n = 2 2$ ）（Zhang, Zhou,& Yuan,2018),本研究招募了25名（其中13名女生）大学在校学生（年龄： $2 0 . 4 2 \pm 2 . 1 3 \mathrm { \ y }$ ）参与实验。所有被试均听力正常、右利手。被试排除标准：1）有精神病史；2）有头部创伤史或癫痫病史;3)有严重躯体疾病；4)酒精或药物依赖。实验前告知被试实验仪器及实验内容的相关信息，被试签署了书面知情同意书，实验结束后支付被试一定的报酬。实验方案经深圳大学伦理委员会批准。

# 2.2实验材料和实验过程

情绪韵律材料选自中国语音情绪库（Liu&Pell,2012)。该库的情绪韵律语音为符合汉语语法规则但无意义的“伪句"：每个句子符合主、谓、宾结构，但主谓宾成分均采用无意义的“伪词”，从而消除了句意。每个句子时长约1\~2s。本研究选取愤怒、恐惧、快乐、中性韵律的材料（每种情绪条件含30个伪句)。每种情绪分别由 5\~9个句子合成10 s整的语音段。每个10 s的情绪语音片段分别由 5\~8句情绪语音（为同种情绪）和1\~4句中性语音（填充刺激）组成，伪句之间无时间间隔。用120个伪句制作愤怒、恐惧和快乐韵律各 5段10 s材料，伪句不重复使用。每个10s的语音片段均含两名女性发声的句子，两个身份对应的句子数量在15段材料间不等。

本实验为3（情绪：恐惧、愤怒、高兴） $\times 2$ （任务：外显、内隐）被试内设计。实验中语音刺激通过两个音箱播放（EDIFIER-R26T，中国东莞)，音响置于距离被试左右耳前方约 $5 0 \mathrm { c m }$ 处，语音材料播放声强为60 至 $7 0 ~ \mathrm { d B }$ ，平均背景噪声值（无语音材料呈现时）为30 dB。

实验分为两个任务，任务间的顺序在被试之间进行平衡。“情绪外显加工任务”要求被试对每段10s语音内包含的情绪性句子和中性句子进行计数，每段语音结束后按键作答：“情绪性句子比中性句子的数量多3个或以上吗？”（答案“是”、“否”各占 $50 \%$ )。该任务包含恐惧、愤怒、快乐3个block，block 的顺序在被试间随机排列。每个block 包含10 段 $1 0 \mathrm { ~ s ~ }$ 的语音（每个语音材料播放两遍)，语音材料以随机顺序呈现，每两段语音材料间的静音时长为15s减去反应时。“情绪内隐加工任务”要求被试对每段10s语音中的两名女性身份发音的句子进行计数，每段语音播放后按键作答：“两名女性播放的句子的数量相差大于2或以上吗？”（答案“是”、“否”各占 $50 \%$ )。该任务同样包含3个block，每个block含10 段$1 0 \mathrm { ~ s ~ }$ 的语音。在两个任务中，每个block 持续约 $4 \mathrm { m i n }$ ，block间被试自主掌握休息时间（可选范围： $2 { \sim } 1 0 \mathrm { m i n }$ )。为控制语音和语义等无关变量引入的混淆，两个任务使用的30段10 s语音材料完全相同。

# 2.3近红外数据记录

使用多通道fNIRS系统以连续波形式记录大脑活动（NirScan,HuiChuang,China）。光学探头的定位采用国际 10/20 系统的 NIRS-EEG 兼容帽（EASYCAP,Herrsching,Germany）。根据已有文献，本研究观测的脑区为双侧额叶和颞叶。我们使用了13个发射器和15个探测器组成了37个有效观测通道（图1)，发射器和探测器之间的平均距离为 $3 . 2 \mathrm { c m }$ （范围为$2 . 8 { \sim } 3 . 6 ~ \mathrm { c m }$ )。在记录过程中没有发生探测器信号过饱和的现象。我们定义每个通道的中间点为该通道探测的主要脑区，并以此点为圆心进行每个通道的脑区标定。首先采用 NFRI工具包（http:/brain.job.affrc.go.jp/tools/）计算每个通道的MNI坐标，然后在成人BrodmannTalairach 脑模中（Lancaster etal.,2000）查找对应的脑区。各通道的坐标及脑区标定信息见表1。

![](images/856363e85e322e3d0236660823dc5ca581bc18c838103ca71e7c5e40d8f7ee90.jpg)  
图1 NIRS通道排布图

表1实验中37个NIRS通道的空间配准信息  

<html><body><table><tr><td rowspan="2">通道</td><td rowspan="2">发射器-</td><td rowspan="2"></td><td colspan="3">MNI坐标</td><td rowspan="2">Brodmann分区及脑区重合度*</td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td>探测器</td><td>X -10</td><td>y 68</td><td>Z</td><td></td><td>10 - Frontopolar area (0.62)</td></tr><tr><td>1 2</td><td>Fp1-Fpz Fp1-AF3</td><td>-25</td><td>66</td><td>-5 4</td><td></td><td>10 - Frontopolar area (1.00)</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>10 - Frontopolar area (0.58)</td></tr><tr><td>3</td><td>Fp1-AF7</td><td>-32</td><td>62</td><td>-8</td><td></td><td>11 - Orbitofrontal area (0.42)</td></tr><tr><td>4</td><td> AFz-Fpz</td><td>3</td><td>66</td><td>11</td><td></td><td>10 - Frontopolar area (1.00)</td></tr><tr><td>5</td><td>AFz-AF3</td><td>-12</td><td>65</td><td>20</td><td></td><td>10 - Frontopolar area (1.00)</td></tr><tr><td>6</td><td>AFz-AF4</td><td>16</td><td>65</td><td>20</td><td></td><td>10 - Frontopolar area (1.00) 10 - Frontopolar area (0.46)</td></tr><tr><td>7</td><td>F5-AF7</td><td>-46</td><td>48</td><td>0</td><td></td><td> 47 - Inferior prefrontal gyrus (0.34)</td></tr><tr><td>8</td><td>F5-F7</td><td>-52</td><td>39</td><td>0</td><td></td><td>47 - Inferior prefrontal gyrus (0.62)</td></tr><tr><td>9</td><td>F5-FC5</td><td>-56</td><td>27</td><td>16</td><td></td><td>45 - pars triangularis, part of Broca's area (0.64)</td></tr><tr><td>10</td><td>FT7-F7</td><td>-57</td><td>21</td><td>-13</td><td></td><td>38 - Temporopolar area (0.68)</td></tr><tr><td>11</td><td>FT7-FC5</td><td>-61</td><td>8</td><td>2</td><td></td><td>22 - Superior temporal gyrus (0.61)</td></tr><tr><td>12</td><td>FT7-T7</td><td>-66</td><td>-7</td><td>-14</td><td></td><td> 21 - Middle temporal gyrus (1.00)</td></tr><tr><td>13</td><td>C5-FC5</td><td>-64</td><td>-2</td><td>24</td><td></td><td>6 - Pre-motor and supplementary motor cortex (0.67)</td></tr><tr><td>14</td><td>C5-T7</td><td>-68</td><td>-17</td><td>8</td><td></td><td> 42 - Primary and auditory association cortex (0.51)</td></tr><tr><td>15</td><td>C5-CP5</td><td>-66</td><td>-30</td><td>28</td><td></td><td> 40 - Supramarginal gyrus, part of Wernicke's area (0.73)</td></tr><tr><td>16</td><td>TP7-T7</td><td>-69</td><td>-31</td><td>-9</td><td></td><td> 21 - Middle temporal gyrus (1.00)</td></tr><tr><td>17</td><td>TP7-CP5</td><td>-67</td><td>-44</td><td>11</td><td></td><td> 22 - Superior temporal gyrus (0.92)</td></tr><tr><td>18</td><td>TP7-P7</td><td>-64</td><td>-55</td><td>-4</td><td></td><td>21 - Middle temporal gyrus (0.58) 37 - Fusiform gyrus (0.42)</td></tr><tr><td>19</td><td>P5-CP5</td><td>-60</td><td>-56</td><td>28</td><td></td><td> 40 - Supramarginal gyrus, part of Wernicke's area (0.58)</td></tr><tr><td>20</td><td>P5-P7</td><td>-58</td><td>-68</td><td>13</td><td></td><td> 39 - Angular gyrus, part of Wernicke's area (0.42)</td></tr><tr><td>21</td><td>Fp2-Fpz</td><td>14</td><td>68</td><td>-5</td><td></td><td>10 - Frontopolar area (0.66)</td></tr><tr><td>22</td><td> Fp2-AF4</td><td>28</td><td>66</td><td>4</td><td></td><td>10 - Frontopolar area (1.00)</td></tr><tr><td>23</td><td>Fp2-AF8</td><td>35</td><td>63</td><td>-8</td><td></td><td>10 - Frontopolar area (0.63)</td></tr><tr><td>24</td><td>F6-AF8</td><td>49</td><td>48</td><td>1</td><td></td><td>10 - Frontopolar area (0.45)</td></tr><tr><td>25</td><td>F6-F8</td><td>54</td><td>39</td><td>1</td><td></td><td>47 - Inferior prefrontal gyrus (0.56)</td></tr><tr><td>26</td><td>F6-FC6</td><td>58</td><td>25</td><td>16</td><td></td><td> 45 - pars triangularis, part of Broca's area (0.69)</td></tr><tr><td>27</td><td>FT8-F8</td><td>59</td><td>21</td><td>-12</td><td></td><td> 38 - Temporopolar area (0.62)</td></tr><tr><td>28</td><td>FT8-FC6</td><td>63</td><td>7</td><td>3</td><td></td><td>22 - Superior temporal gyrus (0.63)</td></tr><tr><td>29</td><td>FT8-T8</td><td>67</td><td>-7</td><td>-12</td><td></td><td>21 - Middle temporal gyrus (1.00)</td></tr><tr><td>30</td><td>C6-FC6</td><td>66</td><td>-3</td><td>24</td><td></td><td> 6 - Pre-motor and supplementary motor cortex (0.66)</td></tr><tr><td>31</td><td>C6-T8</td><td>70</td><td>-17</td><td>8</td><td></td><td>42 - Primary and auditory association cortex (0.50)</td></tr><tr><td>32</td><td>C6-CP6</td><td>67</td><td>-30</td><td>28</td><td></td><td> 40 - Supramarginal gyrus, part of Wernicke's area (0.78)</td></tr><tr><td>33</td><td>TP8-T8</td><td>70</td><td>-30</td><td>-9</td><td></td><td>21 - Middle temporal gyrus (0.98)</td></tr><tr><td>34</td><td>TP8-CP6</td><td>68</td><td>-43</td><td></td><td>11</td><td> 22 - Superior temporal gyrus (0.92)</td></tr></table></body></html>

<html><body><table><tr><td rowspan="2">35</td><td rowspan="2">TP8-P8</td><td rowspan="2">64</td><td rowspan="2">-54</td><td rowspan="2">-4</td><td colspan="2">37 - Fusiform gyrus (0.54)</td></tr><tr><td></td><td>21 - Middle temporal gyrus (0.46)</td></tr><tr><td>36</td><td>P6-CP6</td><td>61</td><td>-56</td><td>28</td><td></td><td>40 - Supramarginal gyrus, part of Wernicke's area (0.61)</td></tr><tr><td>37</td><td>P6-P8</td><td>57</td><td>-67</td><td>13</td><td></td><td>39 - Angular gyrus, part of Wernicke's area (0.54)</td></tr></table></body></html>

\* 部分近红外观测通道可能覆盖了多个脑区，为节省论文空间，本表仅列出重合度高于0.4的脑区。

# 2.4近红外数据分析

采用 NirSpark 软件（HuiChuang，China）进行数据预处理。1）使用基于小波的运动去伪迹方法（Molavi&Dumont,2012）对原始光密度数据进行运动伪迹校正；2）使用 $0 . 0 1 { \sim } 0 . 2 0$ Hz 的滤波器对数据进行带通滤波；3）基于修正的 Beers-Lambert 定律将滤波后的光密度数据转换为 HbO 和 HbR 的浓度变化数据 $\Delta [ \mathrm { H b O } ]$ 和 $\Delta [ \mathrm { H b R } ]$ 。在本研究中， $\Delta [ \mathrm { H b O } ]$ 比 △[HbR]信噪比高，对脑血流的变化更敏感（Tong,Hocke,&Frederick,2011; Zhang,Chen,Hou,& Wu,2019)，故后续统计分析均采用△[HbO]数据。

使用一般线性模型（general linear model,GLM）解算不同条件下任务相关的β值，将 β值作为衡量相应脑区激活的指标。采用block设计，以每段语音10s的时间作为block 的长度，以此为时间线索与HRF 进行卷积。统计分析采用 SPSS 20.0 软件（IBM,Somers,USA）。显著性水平设定为0.05。描述性统计量报告为均值 $\pm$ 标准差。对每个通道的β值进行3（情绪：恐惧、愤怒、高兴) $\times 2$ （任务：外显、内隐）的重复测量方差分析，采用Greenhouse-Geisser进行球形矫正，采用 Bonferroni 进行事后多重比较矫正。最后，在通道间采用 FDR方法对$p$ 值进行多重比较矫正，进一步降低假阳性率。

# 3结果

反应时（ $6 2 8 . 3 5 \pm 1 5 1 . 1 1 ~ \mathrm { m s }$ ）和正确率（ $\textcircled { 9 1 . 4 2 \% } \pm 6 . 3 0 \%$ ）在各实验条件间无显著差异$( F < 1 )$ ）°

脑激活数据分析结果如下。首先，我们在通道3,9,32发现了情绪主效应（表2，图2及图3A）。事后多重比较发现，左侧的额极和眶额皮层（frontopolar/orbitofrontalarea；即额中回和额上回的前部，通道3)对愤怒语音最敏感 $\left( F ( 2 , 4 8 ) = 1 2 . 5 1 , p < 0 . 0 0 1 \right.$ , partial $\eta ^ { 2 } = 0 . 3 4 3$ corrected $p = 0 . 0 0 1$ ；愤怒vs.恐惧 $p = 0 . 0 0 7$ ；愤怒vs.快乐 $p = 0 . 0 0 1$ ）。左侧的额下回三角部(pars triangularis; Broca 区的一部分；通道9)对快乐语音最敏感 $( F ( 2 , 4 8 ) = 2 4 . 2 6 , p < 0 . 0 0 1$ partial $\eta ^ { 2 } \ : = \ : 0 . 5 0 2$ ；corrected $p ~ < ~ 0 . 0 0 1$ ；快乐 vs.愤怒/恐惧 $p \mathbf { s } ~ < ~ 0 . 0 0 1$ ）。右侧缘上回（supramarginal gyrus,SMG；通道32）对恐惧语音最敏感 $( F ( 2 , 4 8 ) = 1 2 . 5 3 , p < 0 . 0 0 1$ , partial$\mathfrak { n } ^ { 2 } = 0 . 3 4 3$ ; corrected $p = 0 . 0 0 1$ ；恐惧vs.愤怒 $p = 0 . 0 0 1$ ；恐惧vs.快乐 $p = 0 . 0 0 3 { \mathrm { ~ , ~ } }$ ）

其次，我们在通道27\~29发现了任务主效应（表3，图2及图3B)。在右侧颞叶的三个通道内，外显比内隐情绪加工任务引起了更强的皮层活动。其中通道 27 对应于颞极(temporopolar; $F ( 1 , 2 4 ) = 1 1 . 6 3 \$ $p = 0 . 0 0 2$ , partial $\mathfrak { n } ^ { 2 } = 0 . 3 2 5$ ; corrected $p = 0 . 0 0 4 )$ ，通道28对应于颞上回（STG; $F ( 1 , 2 4 ) = 2 6 . 1 0 , p < 0 . 0 0 1$ , partial $\mathfrak { n } ^ { 2 } = 0 . 5 2 1$ ; corrected $p < 0 . 0 0 1 \$ ，通道29对应于颞中回（MTG; $F ( 1 , 2 4 ) = 1 5 . 8 1$ ， $p = 0 . 0 0 1$ , partial $\mathfrak { n } ^ { 2 } = 0 . 3 9 7$ ; corrected $p = 0 . 0 0 3 \$ ）°

最后，我们在通道32发现了任务类型和情绪的交互作用（ $F ( 2 , 4 8 ) = 1 4 . 2 4 \$ ， $p < 0 . 0 0 1$ partial $\eta ^ { 2 } = 0 . 3 7 2$ ; corrected $p = 0 . 0 0 2$ ；图2及图3C)，该通道对应于右侧 SMG。简单效应分析表明，在外显任务中，右侧 SMG 脑区对恐惧（β值： $0 . 6 3 \pm 0 . 4 3 \$ ）比对愤怒 $\left( 0 . 0 6 \pm 0 . 4 4 \right)$ 和快乐韵律（ $0 . 1 0 \pm 0 . 3 4 \check { . }$ ）更敏感 $( F ( 2 , 2 3 ) = 1 0 2 . 2 7 , p < 0 . 0 0 1$ , partial $\boldsymbol { \eta } ^ { 2 } = 0 . 8 9 8 \mathrm { \ ' }$ ，而该情绪效应在内隐任务中不显著（ $F < 1$ ；恐惧/愤怒/快乐β值 $= 0 . 1 0 \pm 0 . 4 4 / 0 . 1 4 \pm 0 . 6 6 / 0 . 1 2 \pm$ 0.55)。

表2情绪主效应结果  

<html><body><table><tr><td>通道</td><td>脑区</td><td>F</td><td>p*</td><td>愤怒β值</td><td>恐惧β值</td><td>快乐β值</td></tr><tr><td>3</td><td>L Frontopolar/orbitofrontal area</td><td>12.51</td><td>.001</td><td>0.21 ± 0.20</td><td>0.12 ± 0.23</td><td>0.06 ±0.20</td></tr><tr><td>9</td><td>L pars triangularis/Broca's area</td><td>24.24</td><td><.001</td><td>0.10 ±0.16</td><td>0.10±0.15</td><td>0.21 ± 0.15</td></tr><tr><td>32</td><td>R Supramarginal gyrus</td><td>12.48</td><td>.001</td><td>0.10 ± 0.56</td><td>0.36 ± 0.51</td><td>0.11 ± 0.45</td></tr></table></body></html>

\*The $p$ was corrected using FDR method across fNIRS channels.

表3任务主效应结果  

<html><body><table><tr><td>通道</td><td>脑区</td><td>F</td><td>p*</td><td>内隐β值</td><td>外显β值</td></tr><tr><td>27</td><td>R Temporopolar area</td><td>11.62</td><td>0.004</td><td>0.04 ± 0.36</td><td>0.32 ± 0.42</td></tr><tr><td>28</td><td>R Superior temporal gyrus</td><td>26.17</td><td><0.001</td><td>0.05 ± 0.45</td><td>0.37 ± 0.43</td></tr><tr><td>29</td><td>R Middle temporal gyrus</td><td>15.84</td><td>0.003</td><td>-0.03 ± 0.49</td><td>0.34 ± 0.53</td></tr></table></body></html>

\*The $p$ was corrected using FDR method across fNIRS channels.

![](images/c4dcce21f71b043ee25aebed6a733e8b31b1a769fc55cac2e6a7a3d107a22057.jpg)  
图2不同脑区在情绪和任务条件中的激活（仅显示出现显著效应的通道)。图中的errorbar表示均值的标准误。

![](images/23223fe306613cfec78a8a491cfc420b86e22533cf4f3bf6a3227b6cb785b686.jpg)  
图3脑区激活的成像图。

# 4讨论

本研究利用 fNIRS 技术，探讨了外显和内隐情绪任务下，情绪韵律加工的大脑皮层神经活动。结果表明，外显情绪任务比内隐任务更强地激活了右侧颞叶的 STG、MTG 和颞极，说明右侧颞叶在情绪任务相关的语音韵律感知中发挥了重要作用。我们还发现，不同种类情绪的辨别性解码依赖于额区的 IFG、OFC，且无论在外显还是内隐情绪任务中这些脑区均会激活。而顶叶的SMG 脑区仅在外显任务中可辨别出恐惧韵律。

情绪主效应表明，大脑在情绪任务相关和无关条件下均可分辨不同种类的情绪韵律。相较于恐惧和快乐韵律，愤怒韵律更强的激活了左侧的额极和OFC 脑区。这一发现与此前在情绪韵律辨别任务（Kotz et al.,2013）和被动收听任务（Zhang et al.,2018）中得到的结果非常相似。OFC 不但在愤怒情绪的加工中具有重要作用（Lindquist et al.,2012），同时也负责冲突解决和抑制不恰当的行为（例如攻击行为）（Beyer,Munte,Gottlich,&Krämer,2015）。OFC 损伤的病人更具有攻击倾向，且在主观情绪状态评估、情绪信息整合、情绪韵律辨识方面存在显著缺陷（Fox et al.,2018; Herpertz et al.,2017; Paulmann, Seifert,& Kotz,2010）。与愤怒韵律对应OFC 的结果类似，我们还发现左侧额区的 IFG可以在外显和内隐任务条件下、从恐惧和愤怒韵律中特异性地区分出快乐韵律，这一结果与此前在成人（Zhang et al.,2018）和新生儿（Zhanget al.,2019）情绪韵律被动收听任务中的发现相符。具体的，我们在通道9发现的这一结果，其脑区定位于IFG的三角部（pars triangularis），此区域在语义理解和语义-情绪韵律等语言信息整合中具有关键作用（Goucha&Friederici,2015;Kirby&Robinson,2017; Schirmer& Kotz,2006）。一项关注讽刺理解的研究发现，左侧 IFG 在语境、语义和韵律整合加工中发挥了重要作用，当出现赞扬性的语义和负性情绪的韵律时(即讽刺)，该脑区激活增加（Matsui etal.,2016）。在另一项听觉情绪和社会判断的研究中，左侧 IFG被认为同时参与了社会特质评估（信任度评估）和说话者快乐程度评估（Hensel，Bzdok.Muller,Zilles,& Eickhoff,2015）。Kotz等人的两项研究发现，与中性韵律相比，快乐韵律激活了左侧 IFG（Kotz et al.,2003;2013），而 Johnstone 等人的研究则观察到快乐韵律使右侧 IFG 激活增强（Johnstone,Van Reekum,Oakes,& Davidson,2006）。IFG 激活的左右半球不一致结果可能是实验设置或情绪材料的不同造成的：在Kotz等人（2003;2013)的实验中，被试仅加工语音韵律，而在Johnstone 等人（2006）的实验中，被试在收听语音韵律的同时还需观察情绪信息一致或不一致的面部表情。本研究的情绪材料与Kotz等人（2003;2013）的研究相似，因此发现快乐韵律特异性的激活了左侧IFG。由于对情绪条件进行了两两对比分析，本文发现了愤怒和快乐情绪韵律对应的“特异性脑区”，这一结果在之前那些仅对比某种情绪与中性条件的研究中是未曾得到的。

根据情绪韵律加工的层次模型，在情绪韵律加工的第三层次，OFC、IFG 等额区负责对语音的情绪信息进行精细加工，这一层次需要依赖于注意或外显性情绪评估（Brük et al.,2011)。而本文在OFC 和 IFG脑区的发现与此结论不符。回溯已有的采用外显和内隐情绪任务考察情绪韵律加工的研究发现，那些观察到OFC/IFG 在外显任务中的激活强于内隐任务的研究大多选用了有语义的词（Ethofer et al.,2009b; Quadfieg et al.,2008)或句子（Beaucousinet al.,2011）作为实验材料，而那些观察到额区在外显和内隐任务中均有明显激活的研究往往选用了不具有语义的情绪韵律材料（例如Fruholz et al.,2012; Steber et al.,2020）。因此我们推断语义加工对情绪韵律加工可能造成了干扰或影响。目前我们发现的唯一的一项例外研究为Bach等人（2008），他们虽然在实验中采用伪词作为语音材料，但仍然观察到左侧 IFG在外显比在内隐条件下的激活更强。然而该研究选用性别辨别作为内隐任务，正如引言所述，性别辨别的难度远小于情绪辨别(该研究中，正确率和反应时在两种任务中存在显著差异)，这有可能对结果造成影响；此外 Bach 等人（2008）仅招募了16 名被试进行实验，样本量稍小。本研究使用“身份辨别”作为内隐任务，匹配了内隐和外显条件间的任务难度，而且我们采用无语义的伪句作为实验材料。在此基础上我们发现，额区OFC/IFG 在外显和内隐任务中均可区分不同种类的情绪韵律，因此本研究能比较有信心地对层次模型第三层次的加工理论提出质疑。

我们的结果还表明，右侧 SMG 脑区对恐惧韵律有相对特异性的加工，这一发现是未曾预期到的。SMG 是参与语言感知和加工的脑区之一，该脑区损伤会导致感觉性失语症。SMG也参与身体姿势的加工，被认为是镜像神经元系统的一部分（Carlson，2012）。右侧 SMG更是与语音韵律加工（Hartwigsen,Baumgaertner,Price,Koehnke,Ulmer,& Siebner,2010）和情绪感知相关（Adolphs,R., Damasio,H., Tranel,D., Cooper, G.,& Damasio,20oo; Aryani, Hsu,& Jacobs,2018）。Köchel等人认为 SMG与注意、警觉功能密切相关（Köchel, Schongassner,& Schienle,2013）。他们的研究采用恐惧、厌恶、中性的非言语声音，发现当被试加工恐惧声音时（例如恐惧或痛苦引发的尖叫），右侧 STG 和双侧 SMG 的激活显著增加。此外，Patel等人（2018）发现，情绪韵律的表达功能受损与 SMG受损有关。更值得关注的是，SMG是本研究中观察到的唯一一个同时受情绪和任务调控的脑区：右侧 SMG仅在外显情绪任务中表现出对恐惧韵律的特异性加工。一方面，在此前的成人情绪韵律被动收听任务中（Zhang

Zhou,&Yuan,2018）我们并未观察到右侧 SMG 对恐惧韵律的敏感性，这也说明该脑区对恐惧情绪的加工需要相当程度的情绪相关任务的参与。另一方面，右侧 SMG 对恐惧韵律的特异性加工与课题组此前在新生儿研究中的发现一致（Zhang,Zhou,Hou,Cui,& Zhou,2017;Zhang etal.,2019），这说明人类在刚出生时其大脑即可对恐惧情绪韵律进行快速反应，此过程与是否有情绪任务参与无关。结合我们在成人和新生儿实验中对恐惧情绪韵律的发现，我们认为这可能反映了人类在进化过程中发展出的对恐惧情绪的先天性加工偏向，即出生时可以自动地对恐惧情绪产生敏感的脑反应，而随着年龄的增长，对恐惧韵律的加工（而非恐惧面孔的加工）则需要主动的外显情绪任务参与。

本研究的任务主效应的表明，相较于内隐任务，右侧 MTG、STG、颞极在情绪外显任务下激活显著增强，这一结果与情绪韵律加工的层次模型（Bach et al.,2008;Bruk et al.,2011;Ethofer,T.et al.2006; Schirmer& Kotz,2006; Witteman et al.,2012）完全吻合，也符合情绪语音韵律加工的右侧化理论（见综述 Belyk& Brown,2014)。右侧 STC 是“情绪语音脑区”（emotional voice area; Ethofer et al.,2012; Liebenthal et al.,2016）的最主要结构，是情绪性语音解码的关键脑区（见综述Fruholz&Grandjean,2013a)。STC 的低级结构（初级听觉皮层和 mid-STC）负责解析情绪声音（包括语音、人类非语音、自然界声音）的听觉特征，而STC 的高级结构将这些解析出的音频特征整合并构建出对情绪语音的感知（Fruholz et al.,2016; Schirmer&Kotz,2006)。已有研究表明 STC 对情绪性语音比对中性语音有更强的激活(Bach et al.,2008; Brück et al., 2011; Ethofer et al.,2009b; Fruholz et al., 2012; Kotz et al.,2003;Mothes-Lasch et al.,2011; Witteman et al.,2012）。本研究发现，STC 对情绪韵律的外显性加工有明显的右侧化优势，与已有文献一致（Früholz et al.,2016;Kotz et al.,2013），也说明右脑对语音中的慢变信号和超音段特征敏感（Witteman et al.,2012）。考虑到右侧 STG是经典的音高（pitch）加工脑区（Patterson,Uppenkamp,Johnsrude,& Griffiths,2002），本文观察到的外显情绪任务效应也可能反应了右侧 STG 在情绪辨别任务中对音高进行主动地辨别加工(愤怒、恐惧、快乐韵律的音高均高于中性韵律;音频材料信息见Liu&Pell,2012)。本研究发现的情绪韵律外显性加工的颞区右侧化优势可帮助临床对脑疾病进行诊断，例如判断癫痫的病灶、评估癫痫及大脑器质性病变之后的脑功能可塑性（Alba-Ferrara,Kochen，&Hausmann, 2018）。

综上所述，本研究发现愤怒、恐惧、快乐韵律加工的特异性脑区分别位于左侧OFC、右侧 SMG 和左侧 IFG。我们还发现右侧 STC（包含MTG、STG等）以及右侧 SMG 在外显性情绪韵律任务中的重要作用。本研究的结果部分支持了情绪韵律加工的层次模型，也对该模型的第三层次，即“额区对语音情绪信息的精细加工需要外显性情绪任务参与”提出了质疑。此外，由于fNIRS 技术本身的局限性，本研究无法探测到大脑较深部的、与情绪韵律加工密切相关的脑区（例如颞上沟和杏仁核）。我们建议本领域的探索除了依赖 fMRI、fNIRS、脑磁图等多种影像技术，还需结合脑损伤研究、经颅磁刺激技术（特别是最新发展起来的深部经颅磁刺激）等进一步展开。

# 参考文献

Adolphs,R.masioael,oras.).olefotosyoissalof emotion as revealed by three-dimensional lesion mapping.The Journal of Neuroscience,2O(7),2683-2690.   
Alba-Ferrara,,Kochn,S.,&Husman,M.(l).Emotioalprosodyprocessinginilepsy:Some sightsobogatio Frontiers in Human Neuroscience,12,92.   
Aryani,A.,Hsu,C.T.,&Jacobs,A.M.(2l8).Thesoundofwords evokesaffectivebrainresponses.Brain Siences,8(6),94.   
Bach,D.RjedrdrikKizE).osaveg emotional prosody in meaningless speech. Neuroimage,42(2),919-927.   
Beaucousin,V.,Zag,L，Herve,P.Y.，Strelnikov,K.，CriveloF.Mazoyer,B，&Tzourio-Mazoyer,N.(20ll).Sex-depedt modulationofactivityintheneuralnetworksengagedduringemotionalspeechcomprehension.Brain Research,l39,0817.   
Ben-David,B..ultani,N.akuf,Vudzicz,F.,&vanLieshout,P.H.(216).Prosodyandemantisareseparatebutot separablechaels intheperceptionofemotional speech:testforatingofemotions inspeech.JournalofSpeechLanguageand Hearing Research,59(1),72-89.   
Beyer,F.,Munte,T.FGotlich,,&Kramr,U..(4).Orbitofrontalcorexreactivitytoangryfcialexpressoninoial interaction correlates with aggressive behavior. Cerebral Cortex,25(9),3057-3063.   
Belyk,M.,&Brown,S.(014).Perceptionofaffectiveandlinguisticprosody:AnALEmeta-analsisofeuromagingstudies.Social Cognitive andAffective Neuroscience,9,1395-1403.   
Brik,C.KreifeltsB.&Wildgruber,D.(0l).Emotionalvoicesicontext:Aneurobiologicalmodelofmultimodalffectie information processing.Physics of Life Reviews,8,383-403.   
Calvo,M.G&ume,L.(6).rcealdctiehssincialexresecogtioAtegatievie. Cognition and Emotion,30,1081-1106.   
Dieler,.C.upk,V,Fle..()ioalarfedecfesstofeadss. Brain and Language,121(2),90-109.   
Enea,V.,&cu.(6).rocssgmioaleresistef-tatocialroence,(),490   
Ethofer,T,rehrJind,reifels,idgrubrD,uler,P).otioalicerea:atc location,functionalproperties,ndstructuralcoectiosrevealedbycombinedfID.CerebralCortex,2,900.   
Ethofer,T.,etal.(o6).Cerebralpathwaysinprocesingoffectiveprosody:adnamiccausalmodelingsudyNeuromage0, 580-587.   
Ethofer,T.,VaneilehreKVile.a)coingofotaliftioiniceesiic Current Biology,19(12),1028-1033.   
Ether,T.oolf,b) task,andnoeltyinreiosderingteproceingofspecheldy.Joualofgitiveeurosiene,68.   
Fox,K.C.Rihaaedekanti.Limbach.EdaD.viJ.(1).agebee experience elicitedbydirect stimulationof thehumanorbitofrontal cortex.Neurology,91(16),e1519-e1527.   
Fruholz,S.,ost,W,&Kotz,S.A.(l6).esoundofmotios-owardsuiingeraletworkprspecieofctiesoud processing. Neuroscience and Biobehavioral Reviews,68,96-110.   
Frholz,S.,&danD.(a.ultileubegisinpeompoalrteediereallysitietoalepa quantitative meta-analysis.Neuroscience and Biobehavioral Reviews,37,24-35.   
Fruolz,S.，&djeanD(3b).rocessingofotioalocalzatiosinlateralieriorfrotalorte.Neurosiced Biobehavioral Reviews,37(10),2847-2855.   
Fruolz,S.,ej,,.)s amygdaladamageonauditorycorticalprocessingofvocalemotions.Proceeingsof theNationalAcademyofSciencesofthe United StatesofAmerica,112(5),1583-1588.   
Fruholz,S.,Cervolo,L&Gandean,D(0l2).Specificbaetworksdringexplicitandmplicitdecodingofmotioalpod Cerebral Cortex, 22,107-1117.   
Goucha,T.,&Friederici,A.D.(215).Thelanguageskeletonafterdissectingmeanng:Afunctionalsegregation withinBroca’sArea. Neuroimage,114,294-302.   
Hartwigsen,Gugerter,.ricC.J,oe,,Ue,S,br,H.().oogicaisiosqeothe leftandrightsupramarginal gyri.Proceedingsof the NationalAcademyofSciencesof the United StatesofAmerica107(38) 16494-16499.   
Hensel,LlsKof..)tli Cerebral Cortex,25(5),1152-1162.   
Herpertz,S.C,gyK,Uer,K.it,ce,,al,Crtsc,K().inhsdl reactive aggression inborderline personalitydisorder-sex maters.Biological Psychiatry,82(4),257-266.   
Hinojosa,J.A.，MercadoF.，&Carrtie,L.(2O15).N17sensitivitytofacialexpression:Ameta-analysis.Neuroscienceand Biobehavioral Reviews,55,498-509.   
Johnstone,T.,VanRekum,C.M.,Oakes,T.R,&Dvidson,R.J.(2o6).Thevoiceofemotion:anFMRIstudyofneuralresposesto angry and happy vocal expressions.Social CognitiveandAfective Neuroscience,1(3),242-249.   
Kirby,L.A.J.,&Robinson,J..(ol7).Afectivemapping:Anactivationlikelioodestimation(ALE)meta-analysis.rainnd Cognition,118,137-148.   
Knight,M.J.,,B.T.9).ocialoiieabitsprdictpsosoialciiajoepriedodee and Anxiety,36(1),54-62.   
Kotz,S.A,Kbea,C,a,Jdeici,&s,.(3).dictingoclotiepsifro brain.Human Brain Mapping,34,1971-1981.   
Kotz,S.A.,etal.).Ontelateralzatioofmotioalprosdyevet-relatedfuncialivestigationBainandLagge 86,366–376.   
Kochel,A.,hngassrF,&enle,.(l3).Corticalctiaidringioryicitatioofeasgst:arfrad spectroscopy (NIRS) study. Neuroscience Leters,9(549),197-200.   
LancasterJ.od,siFraac for functional brain mapping. Human Brain Mapping,l0(3),120-131.   
Liebenthal,E,berseigD.A&e,E.(6).TheLngage,Toeadprosodyofmotios:eualsubstratsanddaicsf spoken-word emotion perception.Frontiers inAging Neuroscience,10,506.   
Lin,Y.,Ding,H,&Zhang,Y.(18).Emotialprosodyprocsinginszoenicpaints:Aselectiveeviewandmetaaly Journal of Clinical Medicine,7(10),363.   
Liu,P,&Pel,.D.(o2).ecogngvocalmotioiandarinesealidateddatabaseofnesocalemotioalili BehaviorResearchMethods,44,1042-1051.   
Linduist,K.Ag.ob,s-ouE,e,LF).iisoali. Behavioral and Brain Sciences,35,121-143.   
Matsui,TNaamuaUuisai.Kkeshida,Y,ada,Tt6).eoeprosoddoe sarcasm comprehension: Behavioral and fMRI evidence.Neuropsychologia,87,74-84.   
Mitchell,R.L.，&Xu,Y.(215).Whatisthevalueofembedingartficialemotionalprosodyinhuman-computerinteractions? Implications for theory and design in psychological science.Frontiers in Psychology, 6,1750.   
MitchellR.L.(o.dneatioofokingemoryfrotioalprosodyinainommonliwithteexicoatic emotion network.Neuroimage,36(3),1015-1025.   
Mothes-Lasch,entzel,H.J,ilrW.H.,&raube,T.(ll).Visualtetionmodulatesbainactivationtgry. Journal of Neuroscience,31,9594-9598.   
Ros,E.D.(981).Theaprosodias.Functionalaatomicrgazatioofthafectivecomponentsoflanguageinthightheisphere. Archives of Neurology,38(9),561-569.   
Patel,S.OisKhttFad,.&s..)ighs critical for expression of emotion through prosody.Frontiers in neurology,9,224.   
Paterson,R.DUppeap,Ssde,.&fsTD.).eproessingoftmopideldyftio auditory cortex. Neuron,36,767-776.   
Paulmann,S.,eifert,S.&Kotz,S.A.(0).Orbio-frontallsionscauseimpaimenturinglatebutntarlymotionalprosodic processing. Social Neuroscience,5(1),59-75.   
Quadfieg,S.,ohAntzel,HJerW.H,&trabe,T.().duatiooftheeurlnetwokivoleditheocin of anger prosody: the role of task-relevance and social phobia.Biological Psychology,78,129-137.   
Schirmer,A,&Kotz,S.A.(20o6).Beyondtherighthemisphere:brainmchanismsmediatingvocalemotional procesing.Trendin Cognitive Sciences,10,24-30.   
Steber,S.,Kig,ea,osi).Uovnglectroiloicaldsarigturfpitl prosody. Scientific Reports,10(1),5807.   
Tong,Y.,Hocke,L.M,&Frederick,B.deB.(2011).Isolatingthesourcesofwidespreadphysiologicalfluctuationsinfuctional near-infrared spectroscopy signals.Journal of Biomedical Optics,l6(10),106005.   
Witteman,JVaeV.,.O.().gs:aiieeasglf emotional prosody perception. Neuropsychologia,50,2752-2763.   
Zhang,D.,Chen,Y.,Hou,X,&Wu,Y..(l9).Near-infraredspectroscopyrevealseuralpreptionofvocalemotiosan neonates.Human Brain Mapping,40(8),2434-2448.   
Zhang,D.,Zho,Y,Hou,X,Cu,Y.,&ou,C().isciiationoemotioalprosdiesinmaeoates:Apilotstdy Neuroscience Letters,658,62-66.   
Zhang,D.,Zo,Y,a,8).hossoftoalagoiateditiioil an fNIRS study. Scientific Reports,8(1),218.

# The brain mechanism of explicit and implicit processing of emotional prosodies: An fNIRS study

LEI Zhen1; BI Rong²; MO Licheng²; YU Wenwen²; ZHANG Dandan1,2

1China CenterforBehavioral EconomicsandFinance& SchoolofEconomics,SouthwesterUniversityofFinanceandEconomics, Chengdu 611130,China,² College of Psychology,Shenzhen University, Shenzhen 518060,China

Abstract Emotional expressions of others embedded in speech prosodies are important for social interactions.Affective prosody refers to a way to express and convey emotions through the dynamic changes of various acoustic cues such as pitch, intensity, stress,and intonation in speech, without relying on vocabulary and grammatical structure. Previous studies have shown that STC, IFG, OFC,and other cerebral cortex and subcortical structures are involved in emotional prosody processing,and gradually formed a hierarchical model. However, existing studies on the neural mechanism of emotional prosody processing mostly focus on the difference between non-neutral emotional prosody and neutral prosody， while the comparison between various non-neutral emotional prosody is less investigated. Besides,the differences involved in brain regions of emotional prosody processing under explicit and implicit tasks are stil not clear. Furthermore, it is necessary to further accumulate experimental evidence based on noise-free brain imaging technology, such as the noise-free features of fNIRS are especially suitable for speech processing research.

This study used functional near-infrared spectroscopy to investigate how speech prosodies of different emotional categories are processed in the cortex under diferent task conditions. A group of 25 college students participated in this study with a 3 (emotion: anger vs. fearful vs. happy) by 2 (task focus: explicit vs. implicit) within-participant factorial design. We manipulated task focus by adopting two different tasks, with emotional discrimination task as explicit condition and sex discrimination task as implicit condition. Ten phonological materials for each of anger, fearful, and happy prosody were selected from the Chinese Speech Emotion Database and consisted of the corresponding emotional prosodies and neutral prosodies. The emotional explicit task was to count the emotional and neutral sentences contained in each 10-second speech,and the emotional implicit task was to count the sentences played by two women in each l0-second speech. A multi-channel fNIRS system was used to record brain activity in a continuous waveform. According to existing literature,the brain regions observed in this study are the bilateral frontal and temporal lobes.Therefore,we used 13 emitters and 15 detectors to form 37 effective observation channels.

We first adopted NirSpark-2442 software to preprocess the data, and then conducted general linear model analyses to calculate the cortical activation related to the task.The results showed that the brain activation was significantly higher when anger was contrasted to fearful and happy prosody in left frontal pole /orbitofrontal cortex,and when happy was contrasted to fearful and anger prosody in left inferior frontal gyrus, and when fearful was contrasted to anger and happy prosody in right supramarginal gyrus.Importantly, there was an interaction between emotion and task. In the explicit task, cortex activity in the right supramarginal gyrus was more sensitive to fearful than to anger and happy prosodies. But no similar results were found under anger and happy prosody. In addition, the brain activation in temporopolar, superior temporal gyrus,and middle temporal gyrus with the explicit task was greater than that in the implicit task.

The present study demonstrated the specific brain regions for processing angry, fearful and happy prosody were left frontal pole /orbitofrontal cortex,right supramarginal gyrus,and left inferior frontal gyrus respectively,and the important role of right superior temporal gyrus and right supramarginal gyrus in emotional explicit task. These findings partially support the hierarchical model of emotional prosody and question the third level of the model.

Key wordsemotion; phonetic prosody; superior temporal gyrus; orbitofrontal cortex; inferior frontal gyrus; supramarginal gyrus